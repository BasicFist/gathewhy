# AI Backend Provider Registry
# Master catalog of all LLM inference providers in LAB ecosystem

providers:
  # ============================================================================
  # ACTIVE PROVIDERS
  # ============================================================================

  ollama:
    type: ollama
    base_url: http://127.0.0.1:11434
    status: active
    description: Local Ollama server for general-purpose models
    features:
      - Easy model management (ollama pull/list)
      - CPU and GPU support
      - QuantizedGGUF format
      - Fast model switching
    models:
      - name: llama3.1:latest
        size: "8B"
        quantization: Q4_K_M
        pulled_at: "2025-10-19"
        specialty: general_chat
      - name: qwen2.5-coder:7b
        size: "7.6B"
        quantization: Q4_K_M
        pulled_at: "2025-10-19"
        specialty: code_generation
      - name: mythomax-l2-13b-q5_k_m
        size: "13B"
        quantization: Q5_K_M
        pulled_at: "2025-10-24"
        specialty: creative_writing
        options:
          num_gpu_layers: -1
    health_endpoint: /api/tags
    docs: https://ollama.ai/docs
    location: ../openwebui (integrated)

  llama_cpp_python:
    type: llama_cpp
    base_url: http://127.0.0.1:8000
    status: active
    description: llama.cpp Python bindings server with CUDA support
    features:
      - GPU offloading (full or partial)
      - Context window up to 8192 tokens
      - Parallel requests (4 concurrent)
      - OpenAI-compatible API
    configuration:
      n_gpu_layers: -1  # Full GPU offload
      n_ctx: 8192
      n_parallel: 4
      threads: 6
    health_endpoint: /v1/models
    service_file: ~/.config/systemd/user/llamacpp-python.service
    location: ../openwebui/backends/llama.cpp

  llama_cpp_native:
    type: llama_cpp
    base_url: http://127.0.0.1:8080
    status: active
    description: llama.cpp native C++ server (maximum performance)
    features:
      - Pure C/C++ implementation
      - No Python overhead
      - Maximum throughput
      - CUDA-optimized
    health_endpoint: /v1/models
    location: ../openwebui/backends/llama.cpp

  vllm-qwen:
    type: vllm
    base_url: http://127.0.0.1:8001
    status: active
    description: vLLM high-throughput inference with AWQ quantization (Qwen model)
    features:
      - Continuous batching
      - PagedAttention for KV cache
      - AWQ quantization (4-bit)
      - Optimized for code generation
      - Low memory footprint
    models:
      - name: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
        size: "7B"
        quantization: AWQ
        context_length: 4096
        specialty: code_generation
        model_size_gb: 5.2
        kv_cache_available_gb: 7.36
    health_endpoint: /v1/models
    api_format: openai_compatible
    gpu_memory_utilization: 0.45
    max_model_len: 4096
    location: ~/venvs/vllm
    hardware_requirements:
      gpu_vram_gb: 16
      gpu_model: "Quadro RTX 5000"
      compute_capability: "7.5"

  vllm-dolphin:
    type: vllm
    base_url: http://127.0.0.1:8002
    status: disabled  # vLLM runs single instance - use vllm-model-switch.sh to enable
    description: vLLM high-throughput inference with AWQ quantization (Dolphin model)
    features:
      - Continuous batching
      - PagedAttention for KV cache
      - AWQ quantization (4-bit)
      - Optimized for conversational responses
      - Low memory footprint
    models:
      - name: solidrust/dolphin-2.8-mistral-7b-v02-AWQ
        size: "7B"
        quantization: AWQ
        context_length: 4096
        specialty: conversational
    health_endpoint: /v1/models
    api_format: openai_compatible
    gpu_memory_utilization: 0.45
    max_model_len: 4096
    location: ~/venvs/vllm
    hardware_requirements:
      gpu_vram_gb: 16
      gpu_model: "Quadro RTX 5000"
      compute_capability: "7.5"

  ollama_cloud:
    type: ollama
    base_url: https://ollama.com
    status: active
    description: Ollama Cloud API for large models without local GPU
    requires_api_key: true
    env_var: OLLAMA_API_KEY
    features:
      - Cloud-hosted large models (no local GPU required)
      - Data-center-grade hardware
      - Same API as local Ollama
      - Automatic offloading for cloud models
    models:
      - name: deepseek-v3.1:671b-cloud
        size: "671B"
        specialty: advanced_reasoning
      - name: qwen3-coder:480b-cloud
        size: "480B"
        specialty: code_generation
      - name: kimi-k2:1t-cloud
        size: "1T"
        specialty: advanced_reasoning
      - name: gpt-oss:120b-cloud
        size: "120B"
        specialty: general_chat
      - name: gpt-oss:20b-cloud
        size: "20B"
        specialty: general_chat
      - name: glm-4.6:cloud
        size: "4.6B"
        specialty: general_chat
    health_endpoint: /api/tags
    docs: https://docs.ollama.com/cloud
    rate_limits:
      hourly: true
      daily: true
      details: "Free preview with generous limits"

  # ============================================================================
  # TEMPLATE PROVIDERS (Disabled, for reference)
  # ============================================================================

  openai:
    type: openai
    base_url: https://api.openai.com/v1
    status: active
    description: OpenAI cloud API (requires API key)
    requires_api_key: true
    env_var: OPENAI_API_KEY
    features:
      - Industry-leading language models
      - Function calling and JSON mode
      - Vision capabilities (GPT-4V)
      - Advanced reasoning (o1 models)
    models:
      - name: gpt-4o
        size: "Unknown"
        specialty: advanced_reasoning
        context_length: 128000
        supports_vision: true
        supports_function_calling: true
      - name: gpt-4o-mini
        size: "Unknown"
        specialty: general_chat
        context_length: 128000
        supports_vision: true
        supports_function_calling: true
      - name: gpt-4-turbo
        size: "Unknown"
        specialty: advanced_reasoning
        context_length: 128000
        supports_vision: true
        supports_function_calling: true
      - name: gpt-4
        size: "Unknown"
        specialty: advanced_reasoning
        context_length: 8192
        supports_function_calling: true
      - name: gpt-3.5-turbo
        size: "Unknown"
        specialty: general_chat
        context_length: 16385
        supports_function_calling: true
      - name: o1
        size: "Unknown"
        specialty: advanced_reasoning
        context_length: 200000
        notes: "Advanced reasoning model with extended thinking"
      - name: o1-mini
        size: "Unknown"
        specialty: code_generation
        context_length: 128000
        notes: "Fast reasoning model optimized for code and STEM"
    rate_limits:
      requests_per_minute: 3500
      tokens_per_minute: 90000
    cost_per_1k_tokens:
      gpt-4o: 0.005
      gpt-4o-mini: 0.00015
      gpt-4-turbo: 0.01
      gpt-4: 0.03
      gpt-3.5-turbo: 0.0015
      o1: 0.015
      o1-mini: 0.003
    health_endpoint: /v1/models
    docs: https://platform.openai.com/docs

  anthropic:
    type: anthropic
    base_url: https://api.anthropic.com/v1
    status: active
    description: Anthropic Claude API (requires API key)
    requires_api_key: true
    env_var: ANTHROPIC_API_KEY
    features:
      - Extended context windows (200K tokens)
      - Superior analysis and reasoning
      - Strong safety and helpfulness
      - Tool use and computer use capabilities
    models:
      - name: claude-3-5-sonnet-20241022
        size: "Unknown"
        specialty: advanced_reasoning
        context_length: 200000
        supports_vision: true
        supports_tool_use: true
        notes: "Latest Claude 3.5 Sonnet with improved coding"
      - name: claude-3-5-haiku-20241022
        size: "Unknown"
        specialty: general_chat
        context_length: 200000
        supports_vision: true
        notes: "Fast, cost-effective Claude model"
      - name: claude-3-opus-20240229
        size: "Unknown"
        specialty: advanced_reasoning
        context_length: 200000
        supports_vision: true
        supports_tool_use: true
        notes: "Most capable Claude 3 model"
      - name: claude-3-sonnet-20240229
        size: "Unknown"
        specialty: general_chat
        context_length: 200000
        supports_vision: true
        supports_tool_use: true
      - name: claude-3-haiku-20240307
        size: "Unknown"
        specialty: general_chat
        context_length: 200000
        supports_vision: true
    rate_limits:
      requests_per_minute: 4000
      tokens_per_minute: 400000
    cost_per_1k_tokens:
      claude-3-5-sonnet-20241022: 0.003
      claude-3-5-haiku-20241022: 0.0008
      claude-3-opus-20240229: 0.015
      claude-3-sonnet-20240229: 0.003
      claude-3-haiku-20240307: 0.00025
    health_endpoint: /v1/messages
    docs: https://docs.anthropic.com

  custom_openai_compatible:
    type: openai_compatible
    base_url: http://localhost:CUSTOM_PORT
    status: template
    description: Template for any OpenAI-compatible API server
    notes: |
      Use this template to add:
      - LocalAI servers
      - Text Generation WebUI
      - FastChat servers
      - Any server implementing OpenAI API spec

# ============================================================================
# PROVIDER METADATA
# ============================================================================

metadata:
  version: "1.5"
  last_updated: "2025-11-09"
  total_providers: 7  # ollama, llama_cpp_python, llama_cpp_native, vllm-qwen, ollama_cloud, openai, anthropic
  total_models_available: 24  # 3 Ollama + 1 vLLM + 1 llama.cpp + 6 Ollama Cloud + 7 OpenAI + 5 Anthropic + 1 llama.cpp
  notes: "Added OpenAI and Anthropic cloud providers; vllm-dolphin disabled - vLLM runs single instance on port 8001"

  provider_types:
    - ollama: Simple local server
    - ollama_cloud: Managed cloud API (Ollama)
    - llama_cpp: High-performance C++ inference
    - vllm: Production-grade batched inference
    - openai: OpenAI cloud API (GPT-4, GPT-3.5, o1 models)
    - anthropic: Anthropic cloud API (Claude 3 family)
    - openai_compatible: Generic OpenAI-compatible servers

  selection_criteria:
    ollama:
      best_for: Quick prototyping, easy model management
      performance: Medium
      resource_usage: Low to Medium

    llama_cpp:
      best_for: Maximum single-request performance
      performance: High
      resource_usage: Medium

    vllm:
      best_for: High concurrency, production workloads
      performance: Very High (batched)
      resource_usage: High (requires GPU)

    ollama_cloud:
      best_for: Massive models without local GPU
      performance: Data-center grade
      resource_usage: Remote (billable)

    openai:
      best_for: Industry-leading capabilities, vision, function calling
      performance: Very High (cloud infrastructure)
      resource_usage: Remote (billable per token)
      cost: Medium to High
      context_window: Up to 200K tokens (o1)

    anthropic:
      best_for: Extended context (200K), analysis, safety-critical applications
      performance: Very High (cloud infrastructure)
      resource_usage: Remote (billable per token)
      cost: Medium to High
      context_window: 200K tokens (all models)

# ============================================================================
# HEALTH CHECK CONFIGURATION
# ============================================================================

health_checks:
  enabled: true
  interval_seconds: 60
  timeout_seconds: 5
  retry_attempts: 3

  endpoints:
    ollama: http://127.0.0.1:11434/api/tags
    llama_cpp_python: http://127.0.0.1:8000/v1/models
    llama_cpp_native: http://127.0.0.1:8080/v1/models
    vllm-qwen: http://127.0.0.1:8001/v1/models
    vllm-dolphin: http://127.0.0.1:8002/v1/models
