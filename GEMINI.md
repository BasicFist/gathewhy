# Gemini Code-aware Auto-generated Context

This document is automatically generated by Gemini to provide context for future interactions.

## Project Overview

This project is a unified backend infrastructure that provides a single, consistent API endpoint for various Large Language Model (LLM) providers. It acts as a central routing layer, abstracting the underlying complexity of different LLM inference engines.

**Key Features:**

*   **Single Entry Point:** All requests are routed through a single LiteLLM instance, providing a unified OpenAI-compatible API.
*   **Multi-Provider Support:** It supports multiple LLM backends, including Ollama, llama.cpp, and vLLM, with the flexibility to add more.
*   **Configuration-Driven:** The entire system is configured through YAML files, allowing for easy management of providers, models, and routing rules.
*   **Robust Automation:** The project heavily relies on scripts for validation, testing, deployment, and monitoring.
*   **Comprehensive Observability:** It includes a full monitoring stack with Prometheus and Grafana for real-time metrics and dashboards.

**Architecture:**

The architecture consists of a central LiteLLM gateway that receives requests and routes them to the appropriate backend provider based on the model requested. The supported providers run as separate services.

```
┌─────────────────────────────────────────────────────────┐
│            LAB AI Backend Infrastructure                │
│                                                         │
│  ┌──────────────────────────────────────────────────┐  │
│  │     Single Entry Point: LiteLLM :4000            │  │
│  │  OpenAI-Compatible API + MCP Protocol            │  │
│  └────────────────┬─────────────────────────────────┘  │
│                   │                                     │
│         ┌─────────┼──────────┬──────────┐              │
│         │         │          │          │              │
│         ▼         ▼          ▼          ▼              │
│    ┌────────┐ ┌──────┐ ┌────────┐ ┌─────────┐         │
│    │ Ollama │ │llama │ │ vLLM   │ │ Future  │         │
│    │ :11434 │ │.cpp  │ │ :8001  │ │Providers│         │
│    └────────┘ └──────┘ └────────┘ └─────────┘         │
└─────────────────────────────────────────────────────────┘
```

## Building and Running

### Running the System

To start the unified backend and its associated services, you can use the provided scripts.

```bash
# Start the monitoring stack (Prometheus, Grafana)
cd monitoring
docker compose up -d
cd ..

# Check the status of all providers
./scripts/validate-unified-backend.sh
```



### Validation

The project has a strong emphasis on validation.

```bash
# Validate all configuration files
./scripts/validate-all-configs.sh

# Check for port conflicts
./scripts/check-port-conflicts.sh

# Hot-reload the LiteLLM configuration with validation
./scripts/reload-litellm-config.sh
```

## Development Conventions

### Configuration Management

*   **Source of Truth:** `config/providers.yaml` and `config/model-mappings.yaml` are the primary sources of truth.
*   **Generated Config:** `config/litellm-unified.yaml` is auto-generated from the source files. Do not edit it directly.
*   **Validation:** All configuration changes are validated using Pydantic schemas and consistency checks. Pre-commit hooks are in place to enforce this.

### Contribution Workflow

1.  **Update Configuration:** Modify `config/providers.yaml` and/or `config/model-mappings.yaml`.
2.  **Generate & Validate:**
    ```bash
    python3 scripts/generate-litellm-config.py
    python3 scripts/validate-config-schema.py
    ```

4.  **Update Documentation:** Update any relevant documentation, including the Serena memories.
5.  **Commit & Push:** Pre-commit hooks will run validation automatically.

### Quality Standards

*   All configuration changes must pass Pydantic validation.
*   Unit test coverage must remain above 90%.
*   Provider contracts must comply with the OpenAI API format.
*   Integration tests must pass for all active providers.
