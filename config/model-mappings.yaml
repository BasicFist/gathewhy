# Model Routing Configuration
# Defines how LiteLLM routes model requests to providers

# ============================================================================
# ARCHITECTURAL CONSTRAINTS AND PATTERNS
# ============================================================================
#
# vLLM Single-Instance Pattern:
#   vLLM can only run ONE model at a time per GPU. Current setup:
#   - Port 8001: vllm-qwen (Qwen/Qwen2.5-Coder-7B-Instruct-AWQ) [ACTIVE]
#   - Port 8002: vllm-dolphin (solidrust/dolphin-2.8-mistral-7b-v02-AWQ) [DISABLED]
#
#   To switch models:
#     ./scripts/vllm-model-switch.sh dolphin  # Switch to Dolphin
#     ./scripts/vllm-model-switch.sh qwen     # Switch back to Qwen
#
#   Design Implications:
#   - Only vllm-qwen routes are active in this configuration
#   - vllm-dolphin references are commented out throughout
#   - Load balancing cannot distribute across both vLLM models
#   - High-throughput routing uses only the active vLLM model
#
# Cloud Model Fallback Strategy (NEW - v1.7):
#   Cloud models now fall back to OTHER CLOUD MODELS first to preserve quality:
#     Tier 1 (Premium):   671B/1T → 120B cloud → local (only as last resort)
#     Tier 2 (Standard):  480B/120B → 20B cloud → local code specialists
#     Tier 3 (Entry):     20B/4.6B → local
#
#   This prevents quality cliffs (e.g., 671B → 8B was 98.8% param reduction)
#
# Capability Consolidation (NEW - v1.7):
#   Reduced from 10 overlapping capabilities to 8 clear categories:
#     Primary: code, analytical, reasoning, creative, chat
#     Performance: high_throughput, low_latency, large_context
#
# Load Balancing Expansion (NEW - v1.7):
#   Added intelligent routing for code generation and creative tasks:
#     - Complexity-based: Route by estimated task complexity
#     - Saturation-aware: Overflow to cloud when local saturated
#     - Quality-based: Route by requested quality tier
#
# ============================================================================
# EXACT MODEL NAME ROUTING
# ============================================================================

exact_matches:
  # Ollama models (exact name)
  "llama3.1:latest":
    provider: ollama
    priority: primary
    fallback: null
    description: "General-purpose chat model"

  "qwen2.5-coder:7b":
    provider: ollama
    priority: primary
    fallback: null
    description: "Code generation specialist"
  "mythomax-l2-13b-q5_k_m":
    provider: ollama
    priority: primary
    fallback: llama_cpp_python
    description: "Creative writing and roleplay tuned MythoMax L2"
  # vLLM models (AWQ quantized)
  "qwen-coder-vllm":
    provider: vllm-qwen
    priority: primary
    fallback: ollama
    description: "AWQ-quantized 7B code generation model via vLLM"
    backend_model: "Qwen/Qwen2.5-Coder-7B-Instruct-AWQ"

  # Dolphin Uncensored (Mistral-based) - vLLM Dolphin disabled (single instance mode)
  # "dolphin-uncensored-vllm":
  #   provider: vllm-dolphin
  #   priority: primary
  #   fallback: ollama
  #   description: "7B uncensored Mistral-based model via vLLM (AWQ quantized) - DISABLED: use vllm-model-switch.sh to enable"
  #   backend_model: "solidrust/dolphin-2.8-mistral-7b-v02-AWQ"
  # Note: vLLM runs single instance. Enable Dolphin with: ./scripts/vllm-model-switch.sh dolphin

  # llama.cpp models
  "llama-cpp-default":
    provider: llama_cpp_python
    priority: primary
    fallback: llama_cpp_native
    description: "GGUF model via llama.cpp Python bindings"

  # Ollama Cloud models (large models not available locally)
  "deepseek-v3.1:671b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: null
    description: "671B advanced reasoning model via Ollama Cloud"

  "qwen3-coder:480b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "480B code generation model via Ollama Cloud"

  "kimi-k2:1t-cloud":
    provider: ollama_cloud
    priority: secondary
    fallback: null
    description: "1 trillion parameter model via Ollama Cloud (requires valid cloud API key; falls back via default chain)"

  "gpt-oss:120b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "120B open-source GPT via Ollama Cloud"

  "gpt-oss:20b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "20B general chat model via Ollama Cloud"

  "glm-4.6:cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "4.6B general chat model via Ollama Cloud"

# ============================================================================
# PATTERN-BASED ROUTING
# ============================================================================

patterns:
  # HuggingFace model paths → vLLM
  - pattern: "^Qwen/Qwen2\\.5-Coder.*"
    provider: vllm-qwen
    fallback: ollama
    description: "Qwen Coder models via vLLM (AWQ quantized)"

  - pattern: "^solidrust/dolphin.*AWQ$"
    provider: vllm-dolphin
    fallback: ollama
    description: "Dolphin uncensored models via vLLM (AWQ quantized)"

  - pattern: "^meta-llama/.*"
    provider: vllm-qwen
    fallback: ollama
    description: "HuggingFace Llama models prefer vLLM for performance"

  - pattern: "^mistralai/.*"
    provider: vllm-dolphin
    fallback: ollama
    description: "Mistral models via vLLM"

  # Ollama-style names → Ollama
  - pattern: ".*:\\d+[bB]$"  # Matches "model:7b", "model:13B", etc.
    provider: ollama
    description: "Ollama naming convention (model:size)"

  # GGUF files → llama.cpp
  - pattern: ".*\\.gguf$"
    provider: llama_cpp_python
    fallback: llama_cpp_native
    description: "GGUF quantized models"

# ============================================================================
# CAPABILITY-BASED ROUTING
# ============================================================================
# Consolidated capabilities with clear separation of concerns
# Priority: Semantic routing by use case, not implementation details

capabilities:
  # --------------------------------------------------------------------------
  # PRIMARY CAPABILITIES (Use Case Driven)
  # --------------------------------------------------------------------------

  code:
    description: "Code generation, analysis, and debugging"
    preferred_models:
      - qwen3-coder:480b-cloud      # Cloud: High complexity tasks
      - qwen-coder-vllm             # Local: Medium complexity, high throughput
      - qwen2.5-coder:7b            # Local: Simple tasks, fast response
    routing_strategy: complexity_based
    complexity_thresholds:
      high: qwen3-coder:480b-cloud   # > 100 tokens, multiple files
      medium: qwen-coder-vllm        # 50-100 tokens, single file
      low: qwen2.5-coder:7b          # < 50 tokens, snippet generation

  analytical:
    description: "Technical analysis, structured problem-solving, planning"
    preferred_models:
      - deepseek-v3.1:671b-cloud    # Cloud: Complex reasoning chains
      - kimi-k2:1t-cloud            # Cloud: Large context analysis
      - qwen2.5-coder:7b            # Local: Quick analysis
    routing_strategy: context_based
    context_thresholds:
      large: kimi-k2:1t-cloud        # > 4K tokens context
      complex: deepseek-v3.1:671b-cloud  # Multi-step reasoning
      standard: qwen2.5-coder:7b     # < 4K tokens

  reasoning:
    description: "Multi-step logical reasoning and inference"
    preferred_models:
      - deepseek-v3.1:671b-cloud    # Cloud: Advanced reasoning
      - kimi-k2:1t-cloud            # Cloud: Long-form reasoning
      - llama3.1:latest             # Local: Standard reasoning
    routing_strategy: direct

  creative:
    description: "Creative writing, storytelling, roleplay"
    preferred_models:
      - mythomax-l2-13b-q5_k_m      # Local: Creative writing specialist
      - llama3.1:latest             # Local: General creative tasks
      - gpt-oss:120b-cloud          # Cloud: Advanced creative tasks
    routing_strategy: usage_based

  chat:
    description: "Conversational AI and general assistance"
    preferred_models:
      - llama3.1:latest             # Local: Primary conversational
      - gpt-oss:120b-cloud          # Cloud: Enhanced conversation
      - gpt-oss:20b-cloud           # Cloud: Standard conversation
      - glm-4.6:cloud               # Cloud: Lightweight conversation
    routing_strategy: usage_based

  # --------------------------------------------------------------------------
  # PERFORMANCE-ORIENTED CAPABILITIES
  # --------------------------------------------------------------------------

  high_throughput:
    description: "Batch processing and concurrent request handling"
    min_model_size: "13B"
    provider: vllm-qwen
    routing_strategy: least_loaded
    notes: "vLLM single-instance: only Qwen model available for throughput optimization"

  low_latency:
    description: "Single-request speed priority (TTFB optimization)"
    provider: llama_cpp_native
    fallback: llama_cpp_python
    routing_strategy: fastest_response

  large_context:
    description: "Large context window processing (> 8K tokens)"
    min_context: 8192
    preferred_models:
      - kimi-k2:1t-cloud            # Cloud: 1T param, massive context
      - llama_cpp_python            # Local: 8192 context configured
    routing_strategy: most_capacity

# ============================================================================
# LOAD BALANCING CONFIGURATION
# ============================================================================
# Intelligent distribution across providers with complexity-aware routing

load_balancing:
  # --------------------------------------------------------------------------
  # CONVERSATIONAL LOAD BALANCING
  # --------------------------------------------------------------------------

  "llama3.1:latest":
    providers:
      - provider: ollama
        weight: 0.7  # Primary: Best for streaming and general chat
      - provider: llama_cpp_python
        weight: 0.3  # Secondary: CUDA-optimized fallback
    strategy: weighted_round_robin
    description: "Distribute general chat: 70% Ollama, 30% llama.cpp"

  "general-chat":
    providers:
      - provider: ollama
        weight: 0.6
      - provider: vllm-qwen
        weight: 0.4
    strategy: least_loaded
    description: "Dynamic load balancing based on current provider load"

  # --------------------------------------------------------------------------
  # CODE GENERATION LOAD BALANCING (NEW)
  # --------------------------------------------------------------------------

  "code-generation":
    providers:
      - provider: vllm-qwen          # High throughput for batched requests
        weight: 0.5
        condition: "request_count > 5 OR batch_mode"
      - provider: ollama             # Fast single requests
        weight: 0.3
        condition: "request_count <= 5"
      - provider: ollama_cloud       # Overflow to cloud for complex tasks
        weight: 0.2
        condition: "local_saturated OR complexity == 'high'"
    strategy: adaptive_weighted
    description: "Intelligent code routing: local for simple/medium, cloud for complex/overflow"
    parameters:
      saturation_threshold: 0.8  # 80% capacity triggers cloud routing
      complexity_detection: token_count  # Estimate complexity from prompt length

  # --------------------------------------------------------------------------
  # CREATIVE WRITING LOAD BALANCING (NEW)
  # --------------------------------------------------------------------------

  "creative-tasks":
    providers:
      - provider: ollama
        weight: 0.7
        preferred_model: mythomax-l2-13b-q5_k_m
      - provider: ollama_cloud
        weight: 0.3
        preferred_model: gpt-oss:120b-cloud
        condition: "length > 1000 OR quality_tier == 'high'"
    strategy: quality_based
    description: "Creative writing: local for standard, cloud for high-quality or long-form"

# ============================================================================
# FALLBACK CHAINS
# ============================================================================
# Quality-preserving fallback chains with tier-based degradation
# Design principle: Maintain capability tier, degrade model size gradually

fallback_chains:
  # --------------------------------------------------------------------------
  # LOCAL MODEL FALLBACK CHAINS
  # --------------------------------------------------------------------------

  "qwen2.5-coder:7b":
    description: "Local code model fallback: same capability, alternative providers"
    chain:
      - qwen-coder-vllm              # Same model, higher throughput provider
      - llama3.1:latest              # General fallback
    tier: local

  "llama3.1:latest":
    description: "Primary local chat model fallback"
    chain:
      - mythomax-l2-13b-q5_k_m       # Alternative local model
      - gpt-oss:20b-cloud            # Cloud fallback if local exhausted
    tier: local

  "mythomax-l2-13b-q5_k_m":
    description: "Creative model fallback chain"
    chain:
      - llama3.1:latest              # Local general fallback
      - gpt-oss:120b-cloud           # Cloud creative fallback
    tier: local

  "qwen-coder-vllm":
    description: "vLLM code model fallback"
    chain:
      - qwen2.5-coder:7b             # Same capability, different provider
      - qwen3-coder:480b-cloud       # Upgrade to cloud if local fails
    tier: local

  # --------------------------------------------------------------------------
  # CLOUD MODEL FALLBACK CHAINS (FIXED: Quality-Preserving)
  # --------------------------------------------------------------------------

  "deepseek-v3.1:671b-cloud":
    description: "Advanced reasoning: cloud-first, maintain quality tier"
    chain:
      - kimi-k2:1t-cloud             # Same tier: 1T param reasoning model
      - gpt-oss:120b-cloud           # Step down: 120B general model
      - llama3.1:latest              # Local fallback only as last resort
    tier: cloud_premium
    notes: "Preserves reasoning capability through cloud alternatives before local degradation"

  "kimi-k2:1t-cloud":
    description: "1T param model: largest available, gradual degradation"
    chain:
      - deepseek-v3.1:671b-cloud     # Same tier: 671B reasoning specialist
      - gpt-oss:120b-cloud           # Step down: 120B general model
      - llama3.1:latest              # Local fallback (extreme degradation)
    tier: cloud_premium
    notes: "Maintains cloud-grade quality before falling back to 8B local"

  "qwen3-coder:480b-cloud":
    description: "Advanced code model: cloud-first code specialists"
    chain:
      - gpt-oss:120b-cloud           # Same tier: 120B cloud model (general coding)
      - qwen-coder-vllm              # Step down: 7B vLLM code specialist
      - qwen2.5-coder:7b             # Final: 7B Ollama code model
    tier: cloud_standard
    notes: "Preserves code capability through cloud then local code specialists"

  "gpt-oss:120b-cloud":
    description: "120B cloud model: mid-tier cloud fallback"
    chain:
      - gpt-oss:20b-cloud            # Step down: 20B cloud model
      - llama3.1:latest              # Local fallback
      - mythomax-l2-13b-q5_k_m       # Alternative local model
    tier: cloud_standard

  "gpt-oss:20b-cloud":
    description: "20B cloud model: entry-level cloud fallback"
    chain:
      - glm-4.6:cloud                # Alternative cloud model
      - llama3.1:latest              # Local 8B fallback
    tier: cloud_entry

  "glm-4.6:cloud":
    description: "Smallest cloud model: direct to local fallback"
    chain:
      - llama3.1:latest              # Local 8B
      - mythomax-l2-13b-q5_k_m       # Alternative local 13B
    tier: cloud_entry

  # --------------------------------------------------------------------------
  # DEFAULT FALLBACK CHAIN
  # --------------------------------------------------------------------------

  default:
    description: "Universal fallback: cloud capability preservation then local"
    chain:
      - gpt-oss:120b-cloud           # Cloud: 120B general capability
      - gpt-oss:20b-cloud            # Cloud: 20B standard capability
      - llama3.1:latest              # Local: 8B general fallback
      - qwen2.5-coder:7b             # Local: 7B code fallback
      - qwen-coder-vllm              # Local: vLLM high-throughput fallback
    tier: universal
    notes: "Cloud-first for unknown models, gradual degradation to local"

# ============================================================================
# ROUTING RULES
# ============================================================================

routing_rules:
  # Rule precedence (highest to lowest):
  # 1. Exact model name match
  # 2. Capability-based routing
  # 3. Pattern matching
  # 4. Default provider

  default_provider: ollama
  default_fallback: llama_cpp_python

  request_metadata_routing:
    # Route based on request headers/metadata
    high_priority_requests:
      provider: vllm-qwen
      condition: header.x-priority == "high"

    batch_requests:
      provider: vllm-qwen
      condition: batch_size > 1

    streaming_requests:
      provider: ollama  # Best streaming support
      condition: stream == true

  model_size_routing:
    # Automatic provider selection based on model size
    - size: "< 8B"
      provider: ollama
      reason: "Small models work well with Ollama"

    - size: "8B - 13B"
      provider: ollama
      fallback: llama_cpp_python
      reason: "Medium models, Ollama preferred"

    - size: "> 13B"
      provider: vllm-qwen
      fallback: ollama
      reason: "Large models benefit from vLLM batching"

# ============================================================================
# SPECIAL ROUTING CASES
# ============================================================================

special_cases:
  first_request_routing:
    description: "Cold start optimization"
    strategy: prefer_warm_providers
    warm_check_timeout_ms: 100

  rate_limited_fallback:
    description: "Automatic fallback when provider hits rate limit"
    enabled: true
    fallback_duration_seconds: 60

  error_based_routing:
    description: "Avoid providers with recent errors"
    enabled: true
    error_threshold: 3
    cooldown_seconds: 300

  geographic_routing:
    description: "Route based on provider location (future)"
    enabled: false
    prefer_local: true

# ============================================================================
# METADATA
# ============================================================================

metadata:
  version: "1.7"
  last_updated: "2025-11-11"
  total_routing_rules: 30+
  architecture_version: "quality-preserving-fallbacks"

  supported_routing_strategies:
    - exact_match
    - pattern_match
    - capability_based
    - load_balanced
    - fallback_chain
    - complexity_based       # NEW: Route by task complexity
    - quality_based          # NEW: Route by quality tier
    - adaptive_weighted      # NEW: Dynamic weight adjustment

  capability_categories:
    primary_use_cases: 5     # code, analytical, reasoning, creative, chat
    performance_oriented: 3  # high_throughput, low_latency, large_context
    deprecated: 2            # code_generation, analysis (merged into 'code' and 'analytical')

  load_balancing_configs:
    conversational: 2        # llama3.1:latest, general-chat
    code_generation: 1       # NEW: code-generation (adaptive)
    creative_writing: 1      # NEW: creative-tasks (quality-based)

  fallback_chain_architecture:
    local_models: 4          # Fallback within local infrastructure
    cloud_models: 6          # Cloud-first quality preservation
    quality_tiers:
      cloud_premium: 2       # 671B, 1T (deepseek, kimi)
      cloud_standard: 2      # 480B, 120B (qwen3-coder, gpt-oss:120b)
      cloud_entry: 2         # 20B, 4.6B (gpt-oss:20b, glm)
      local: 4               # 7-13B (qwen, llama3.1, mythomax, vllm)

  changes:
    v1.7:
      - "FIXED: Cloud model fallback chains now preserve quality tiers"
      - "Example: deepseek-v3.1:671b → kimi-k2:1t → gpt-oss:120b → local"
      - "Prevents quality cliffs (671B → 8B was 98.8% param reduction)"
      - "CONSOLIDATED: Merged 10 overlapping capabilities into 8 clear categories"
      - "ADDED: Complexity-based code routing (simple→local, complex→cloud)"
      - "ADDED: Quality-based creative routing with saturation overflow"
      - "DOCUMENTED: vLLM single-instance pattern with model switching instructions"
    v1.6:
      - "Added Ollama Cloud provider with DeepSeek, Qwen3, Kimi K2, GPT-OSS, and GLM models"
      - "Updated capability routing and fallback chains to leverage cloud capacity"
      - "Configured local vs cloud routing strategy for optimal performance"

notes: |
  This configuration is consumed by LiteLLM's router.

  Architecture Principles (v1.7):
  - Graceful Degradation: Fallbacks preserve capability tier before degrading size
  - Quality Preservation: Cloud models stay in cloud tier before falling to local
  - Complexity Awareness: Route by estimated task complexity (token count, batch size)
  - Explicit Constraints: vLLM single-instance limitation documented prominently

  When adding new models:
  1. Add exact match entry if it has a canonical name
  2. Assign to appropriate capability category (5 primary + 3 performance)
  3. Define fallback chain with quality tier preservation
  4. Consider load balancing if multiple providers offer same capability
  5. Test routing with validation script

  When adding new providers:
  1. Update providers.yaml first (provider registry)
  2. Add routing rules here (model-mappings.yaml)
  3. Regenerate litellm-unified.yaml (scripts/generate-litellm-config.py)
  4. Validate all configs (scripts/validate-all-configs.sh)
  5. Test end-to-end (scripts/validate-unified-backend.sh)
