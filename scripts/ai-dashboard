#!/usr/bin/env python3
"""Interactive AI backend command center built with Textual."""

from __future__ import annotations

import os
import subprocess
import time
import warnings
from collections.abc import Iterable
from dataclasses import dataclass
from urllib.parse import urlparse

import psutil
import requests  # type: ignore[import-untyped]
from textual import on
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Container, Horizontal, Vertical
from textual.coordinate import Coordinate
from textual.message import Message
from textual.reactive import reactive
from textual.widgets import Button, DataTable, Footer, Header, Label, Log, Static

# ---------------------------------------------------------------------------
# Data models


@dataclass
class ServiceMetrics:
    """Snapshot of a provider's health and resource usage."""

    key: str
    display: str
    required: bool
    status: str
    port: int | None
    models: int | None
    cpu_percent: float
    memory_mb: float
    memory_percent: float
    vram_mb: float
    vram_percent: float
    response_ms: float
    pid: int | None
    notes: list[str]


@dataclass
class GPUOverview:
    """Aggregated GPU utilisation information."""

    detected: bool
    per_gpu: list[dict[str, float]]
    total_used_mb: float
    total_capacity_mb: float
    peak_util_percent: float


# ---------------------------------------------------------------------------
# Monitoring helpers


class GPUMonitor:
    """Utility helpers around NVIDIA's NVML for VRAM insight."""

    def __init__(self) -> None:
        self.initialized = False
        try:
            with warnings.catch_warnings():
                warnings.filterwarnings(
                    "ignore",
                    message="The pynvml package is deprecated",
                    category=FutureWarning,
                )
                import pynvml

            pynvml.nvmlInit()
            self._pynvml = pynvml
            self.initialized = True
            self.device_count = pynvml.nvmlDeviceGetCount()
        except Exception:  # pragma: no cover - driver availability
            self.initialized = False
            self.device_count = 0
            self._pynvml = None

    def get_gpu_info(self) -> list[dict[str, float]]:
        if not self.initialized:
            return []

        assert self._pynvml is not None
        pynvml = self._pynvml
        info: list[dict[str, float]] = []

        for index in range(self.device_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(index)
                name = pynvml.nvmlDeviceGetName(handle).decode("utf-8")
                memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
            except Exception:
                continue

            total_mb = memory.total / 1024**2
            used_mb = memory.used / 1024**2

            info.append(
                {
                    "id": float(index),
                    "name": name,
                    "memory_total_mb": total_mb,
                    "memory_used_mb": used_mb,
                    "memory_util_percent": (used_mb / total_mb * 100) if total_mb else 0.0,
                    "gpu_util_percent": float(util.gpu),
                }
            )

        return info

    def get_process_vram(self) -> dict[int, tuple[float, float]]:
        """Return VRAM usage per PID as (used_mb, percentage of associated GPUs)."""

        if not self.initialized:
            return {}

        assert self._pynvml is not None
        pynvml = self._pynvml
        usage: dict[int, dict[str, float]] = {}

        for index in range(self.device_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(index)
                memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
                total_mb = memory.total / 1024**2 if memory.total else 0.0
                processes = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)
            except Exception:
                continue

            for record in processes or []:
                pid = getattr(record, "pid", None)
                used_bytes = getattr(record, "usedGpuMemory", 0)
                if pid is None or used_bytes is None or used_bytes < 0:
                    continue

                used_mb = used_bytes / 1024**2
                bucket = usage.setdefault(pid, {"used": 0.0, "total": 0.0})
                bucket["used"] += used_mb
                if total_mb > 0:
                    bucket["total"] += total_mb

        per_pid: dict[int, tuple[float, float]] = {}
        for pid, values in usage.items():
            total = values["total"] or 0.0
            used = values["used"]
            per_pid[pid] = (used, (used / total * 100) if total else 0.0)

        return per_pid


class ProviderMonitor:
    """Collects live diagnostics for each backend provider."""

    PROVIDERS: dict[str, dict[str, object]] = {
        "ollama": {
            "endpoint": "http://127.0.0.1:11434/api/tags",
            "display": "Ollama",
            "service": "ollama.service",
            "required": False,
            "type": "ollama",
        },
        "vllm": {
            "endpoint": "http://127.0.0.1:8001/v1/models",
            "display": "vLLM",
            "service": "vllm.service",
            "required": True,
            "type": "litellm",
        },
        "llama_cpp_python": {
            "endpoint": "http://127.0.0.1:8000/v1/models",
            "display": "llama.cpp (Python)",
            "service": "llamacpp-python.service",
            "required": False,
            "type": "litellm",
        },
        "llama_cpp_native": {
            "endpoint": "http://127.0.0.1:8080/v1/models",
            "display": "llama.cpp (Native)",
            "service": "llama-cpp-native.service",
            "required": False,
            "type": "litellm",
        },
        "litellm_gateway": {
            "endpoint": "http://127.0.0.1:4000/v1/models",
            "display": "LiteLLM Gateway",
            "service": "litellm.service",
            "required": True,
            "type": "litellm",
        },
    }

    def __init__(self) -> None:
        self.gpu_monitor = GPUMonitor()

    # --------------------------- system helpers ---------------------------------

    def _service_name(self, key: str) -> str | None:
        record = self.PROVIDERS.get(key)
        return None if record is None else str(record["service"])

    def _get_service_pid(self, key: str) -> int | None:
        service = self._service_name(key)
        if not service:
            return None

        try:
            result = subprocess.run(
                ["systemctl", "--user", "show", service, "--property=MainPID", "--value"],
                capture_output=True,
                text=True,
                check=False,
                timeout=2,
            )
        except (subprocess.SubprocessError, FileNotFoundError):  # pragma: no cover - OS specific
            return None

        value = result.stdout.strip()
        if not value:
            return None
        try:
            pid = int(value)
        except ValueError:
            return None
        return pid if pid > 0 else None

    def _collect_process_metrics(self, key: str) -> tuple[int | None, float, float]:
        pid = self._get_service_pid(key)
        if pid is None:
            return None, 0.0, 0.0

        try:
            proc = psutil.Process(pid)
            cpu = proc.cpu_percent(interval=0.0)
            memory_mb = proc.memory_info().rss / 1024**2
            return pid, cpu, memory_mb
        except psutil.Error:
            return None, 0.0, 0.0

    def _parse_models(self, key: str, response_json: dict) -> int:
        provider_type = str(self.PROVIDERS[key]["type"])
        try:
            if provider_type == "ollama":
                return len(response_json.get("models", []))
            return len(response_json.get("data", []))
        except Exception:
            return 0

    # ----------------------------- public API ----------------------------------

    def collect_snapshot(self) -> tuple[list[ServiceMetrics], GPUOverview]:
        gpu_info = self.gpu_monitor.get_gpu_info()
        per_pid_vram = self.gpu_monitor.get_process_vram()
        system_memory_total = psutil.virtual_memory().total / 1024**2

        metrics: list[ServiceMetrics] = []

        for key, cfg in self.PROVIDERS.items():
            notes: list[str] = []
            endpoint = str(cfg["endpoint"])
            required = bool(cfg["required"])
            display = str(cfg["display"])

            start = time.perf_counter()
            status = "inactive"
            models = 0

            try:
                response = requests.get(endpoint, timeout=float(os.getenv("AI_DASH_HTTP_TIMEOUT", "3")))
                elapsed_ms = (time.perf_counter() - start) * 1000

                if response.ok:
                    payload = response.json()
                    status = "active"
                    models = self._parse_models(key, payload)
                else:
                    notes.append(f"HTTP {response.status_code} {response.reason}")
                    status = "degraded" if required else "inactive"
            except requests.exceptions.Timeout:
                notes.append("Timed out")
                elapsed_ms = (time.perf_counter() - start) * 1000
                status = "degraded" if required else "inactive"
            except requests.exceptions.ConnectionError:
                notes.append("Connection error")
                elapsed_ms = (time.perf_counter() - start) * 1000
                status = "degraded" if required else "inactive"
            except ValueError:
                notes.append("Invalid JSON payload")
                elapsed_ms = (time.perf_counter() - start) * 1000
                status = "degraded" if required else "inactive"

            pid, cpu_percent, memory_mb = self._collect_process_metrics(key)
            memory_percent = (memory_mb / system_memory_total * 100) if system_memory_total else 0.0
            vram_mb, vram_percent = per_pid_vram.get(pid, (0.0, 0.0)) if pid else (0.0, 0.0)

            metrics.append(
                ServiceMetrics(
                    key=key,
                    display=display,
                    required=required,
                    status=status,
                    port=urlparse(endpoint).port,
                    models=models,
                    cpu_percent=cpu_percent,
                    memory_mb=memory_mb,
                    memory_percent=memory_percent,
                    vram_mb=vram_mb,
                    vram_percent=vram_percent,
                    response_ms=elapsed_ms,
                    pid=pid,
                    notes=notes,
                )
            )

        if gpu_info:
            total_used = sum(entry["memory_used_mb"] for entry in gpu_info)
            total_capacity = sum(entry["memory_total_mb"] for entry in gpu_info)
            peak_util = max(entry["gpu_util_percent"] for entry in gpu_info)
        else:
            total_used = total_capacity = peak_util = 0.0

        overview = GPUOverview(
            detected=bool(gpu_info),
            per_gpu=gpu_info,
            total_used_mb=total_used,
            total_capacity_mb=total_capacity,
            peak_util_percent=peak_util,
        )

        return metrics, overview

    def systemctl(self, key: str, action: str) -> bool:
        service = self._service_name(key)
        if not service:
            return False

        try:
            subprocess.run(
                ["systemctl", "--user", action, service],
                check=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                timeout=8,
            )
        except subprocess.SubprocessError:
            return False

        return True


# ---------------------------------------------------------------------------
# UI components


class OverviewPanel(Static):
    """Condensed service summary."""

    def update_overview(self, metrics: Iterable[ServiceMetrics]) -> None:
        metrics = list(metrics)
        total = len(metrics)
        active = sum(1 for m in metrics if m.status == "active")
        degraded = [m for m in metrics if m.status != "active"]
        avg_cpu = sum(m.cpu_percent for m in metrics) / total if total else 0.0
        avg_mem = sum(m.memory_percent for m in metrics) / total if total else 0.0

        warn_lines = ", ".join(f"{m.display}" for m in degraded) if degraded else "None"

        self.update(
            f"[b]Services:[/b] {active}/{total} active\n"
            f"[b]Average CPU:[/b] {avg_cpu:.1f}%   [b]Average Memory:[/b] {avg_mem:.1f}%\n"
            f"[b]Attention:[/b] {warn_lines}"
        )


class ServiceTable(DataTable):
    """Tabular view of all providers."""

    def __init__(self) -> None:
        super().__init__(zebra_stripes=True)
        self.cursor_type = "row"
        self.show_header = True
        self.add_columns(
            "Provider",
            "Status",
            "CPU",
            "Mem",
            "VRAM",
            "Resp",
            "Models",
        )

    def populate(self, metrics: Iterable[ServiceMetrics], selected: str | None) -> None:
        self.clear()
        metrics = list(metrics)
        selected_index: int | None = None

        for index, metric in enumerate(metrics):
            status_style = {
                "active": "green",
                "degraded": "yellow",
                "inactive": "red",
            }.get(metric.status, "white")

            cpu_text = f"{metric.cpu_percent:.1f}%"
            mem_text = f"{metric.memory_mb:.0f}MB"
            vram_text = "-" if not metric.vram_mb else f"{metric.vram_mb:.0f}MB"
            resp_text = f"{metric.response_ms:.0f}ms"

            self.add_row(
                metric.display,
                f"[{status_style}]{metric.status.title()}[/]",
                cpu_text,
                mem_text,
                vram_text,
                resp_text,
                str(metric.models),
                key=metric.key,
            )

            if selected and metric.key == selected:
                selected_index = index

        if selected_index is not None:
            self.cursor_coordinate = Coordinate(row=selected_index, column=0)
        elif self.row_count:
            self.cursor_coordinate = Coordinate(row=0, column=0)


class GPUCard(Static):
    """Summarises GPU utilisation."""

    def update_overview(self, overview: GPUOverview) -> None:
        if not overview.detected:
            self.update("No NVIDIA GPU detected or NVML unavailable.")
            return

        lines = [
            f"Total VRAM: {overview.total_used_mb:.0f}/{overview.total_capacity_mb:.0f}MB ("
            f"{(overview.total_used_mb / overview.total_capacity_mb * 100) if overview.total_capacity_mb else 0:.1f}% )",
            f"Peak utilisation: {overview.peak_util_percent:.1f}%",
        ]

        for entry in overview.per_gpu:
            lines.append(
                f"GPU {int(entry['id'])}: {entry['memory_used_mb']:.0f}/"
                f"{entry['memory_total_mb']:.0f}MB ({entry['memory_util_percent']:.1f}%)"
            )

        self.update("\n".join(lines))


class DetailPanel(Vertical):
    """Shows focused service details and exposes control buttons."""

    class ServiceAction(Message):
        def __init__(self, action: str, service_key: str) -> None:
            super().__init__()
            self.action = action
            self.service_key = service_key

    def __init__(self) -> None:
        super().__init__(id="detail-panel")
        self._current: ServiceMetrics | None = None

    def compose(self) -> ComposeResult:
        yield Label("Select a provider to inspect", id="detail-title")
        yield Label(id="detail-status")
        yield Label(id="detail-resources")
        yield Label(id="detail-metadata")
        with Horizontal(id="detail-actions"):
            yield Button("Start", id="action-start", variant="success")
            yield Button("Stop", id="action-stop", variant="error")
            yield Button("Restart", id="action-restart", variant="warning")
            yield Button("Enable", id="action-enable", variant="primary")
            yield Button("Disable", id="action-disable", variant="primary")
        yield Log(id="detail-notes", highlight=True)

    def update_details(self, metrics: ServiceMetrics | None) -> None:
        self._current = metrics
        title = self.query_one("#detail-title", Label)
        status_label = self.query_one("#detail-status", Label)
        resources_label = self.query_one("#detail-resources", Label)
        metadata_label = self.query_one("#detail-metadata", Label)
        notes_log = self.query_one(Log)

        if not metrics:
            title.update("Select a provider to inspect")
            status_label.update("")
            resources_label.update("")
            metadata_label.update("")
            notes_log.clear()
            return

        status_colour = {
            "active": "green",
            "degraded": "yellow",
            "inactive": "red",
        }.get(metrics.status, "white")

        title.update(f"[b]{metrics.display}[/b] ({'required' if metrics.required else 'optional'})")
        status_label.update(
            f"Status: [{status_colour}]{metrics.status.upper()}[/]  |  Response: {metrics.response_ms:.0f} ms"
        )
        resources_label.update(
            "CPU: {:.1f}%   MEM: {:.1f} MB ({:.1f}%)   VRAM: {}".format(
                metrics.cpu_percent,
                metrics.memory_mb,
                metrics.memory_percent,
                "-" if not metrics.vram_mb else f"{metrics.vram_mb:.1f} MB ({metrics.vram_percent:.1f}%)",
            )
        )
        metadata_label.update(
            "Port: {}   Models: {}   PID: {}".format(
                metrics.port or "n/a",
                metrics.models,
                metrics.pid or "n/a",
            )
        )

        notes_log.clear()
        if metrics.notes:
            for line in metrics.notes:
                notes_log.write(line)
        else:
            notes_log.write("No warnings recorded.")

    @on(Button.Pressed)
    def handle_button(self, event: Button.Pressed) -> None:
        if not self._current:
            return
        action = event.button.id or ""
        if not action.startswith("action-"):
            return
        self.post_message(self.ServiceAction(action.replace("action-", ""), self._current.key))


# ---------------------------------------------------------------------------
# Main dashboard application


class DashboardApp(App[None]):
    """Interactive command center for the AI backend."""

    CSS = """
    Screen {
        layout: vertical;
    }

    /* Body grid splits the view in two columns
       Left: overview + table, Right: GPU + detail + event log

       ┌───────────────┬────────────┐
       │ overview      │ gpu        │
       │ service table │ detail/log │
       └───────────────┴────────────┘

       Fine-tune spacing with padding and gaps so layout stays compact. */

    Container#body {
        layout: grid;
        grid-size: 2;
        grid-columns: 3fr 2fr;
        grid-rows: auto 1fr;
        height: 1fr;
        padding: 1 2;
    }

    Vertical#left-column,
    Vertical#right-column {
        layout: vertical;
    }

    OverviewPanel {
        border: solid #3a3a3a;
        padding: 1;
    }

    ServiceTable {
        height: 1fr;
        border: solid #3a3a3a;
    }

    GPUCard {
        border: solid #3a3a3a;
        padding: 1;
    }

    /* Detail panel and log share the right column vertically.
       Detail gets the remaining height with the log pinned at the bottom.
       Provide some contrast for the action row. */

    #detail-panel {
        border: solid #3a3a3a;
        padding: 1;
        height: 1fr;
    }

    #detail-actions {
        padding-top: 1;
    }

    Log {
        height: 12;
        border: solid #333333;
        padding: 1;
    }
    """

    BINDINGS = [
        Binding("r", "refresh", "Refresh"),
        Binding("q", "quit", "Quit"),
        Binding("a", "toggle_auto", "Toggle auto-refresh"),
        Binding("ctrl+l", "clear_log", "Clear log"),
    ]

    auto_refresh = reactive(True)

    def __init__(self) -> None:
        super().__init__()
        self.monitor = ProviderMonitor()
        self.metrics: list[ServiceMetrics] = []
        self.gpu_overview = GPUOverview(False, [], 0.0, 0.0, 0.0)
        self.selected_key: str | None = None

    # ------------------------------ layout ---------------------------------

    def compose(self) -> ComposeResult:
        yield Header(show_clock=True)
        with Container(id="body"):
            with Vertical(id="left-column"):
                yield OverviewPanel(id="overview")
                yield ServiceTable()
            with Vertical(id="right-column"):
                yield GPUCard(id="gpu")
                yield DetailPanel()
                yield Log(id="event-log", highlight=True)
        yield Footer()

    # ----------------------------- lifecycle --------------------------------

    def on_mount(self) -> None:
        self._refresh_table()
        self.refresh_timer = self.set_interval(5, self._refresh_table, pause=not self.auto_refresh)

    def action_toggle_auto(self) -> None:
        self.auto_refresh = not self.auto_refresh
        if self.auto_refresh:
            self.refresh_timer.resume()
            self.log_event("Auto-refresh enabled (5s cadence).")
        else:
            self.refresh_timer.pause()
            self.log_event("Auto-refresh paused.")

    def action_refresh(self) -> None:
        self._refresh_table()
        self.log_event("Manual refresh completed.")

    def action_clear_log(self) -> None:
        self.query_one("#event-log", Log).clear()

    # ---------------------------- refresh engine ----------------------------

    def _refresh_table(self) -> None:
        self.metrics, self.gpu_overview = self.monitor.collect_snapshot()

        table = self.query_one(ServiceTable)
        table.populate(self.metrics, self.selected_key)

        self.query_one(OverviewPanel).update_overview(self.metrics)
        self.query_one(GPUCard).update_overview(self.gpu_overview)

        detail = self.query_one(DetailPanel)
        detail.update_details(self._find_metric(self.selected_key))

    def _find_metric(self, key: str | None) -> ServiceMetrics | None:
        if not key:
            return None
        for metric in self.metrics:
            if metric.key == key:
                return metric
        return None

    # ------------------------------- helpers --------------------------------

    def log_event(self, message: str) -> None:
        self.query_one("#event-log", Log).write(message)

    # --------------------------- event handlers -----------------------------

    @on(DataTable.RowSelected)
    def handle_row_selected(self, event: DataTable.RowSelected) -> None:
        row_key = getattr(event.row_key, "value", event.row_key)
        key = str(row_key)
        self.selected_key = key
        self.query_one(DetailPanel).update_details(self._find_metric(key))

    @on(DetailPanel.ServiceAction)
    def handle_service_action(self, event: DetailPanel.ServiceAction) -> None:
        success = self.monitor.systemctl(event.service_key, event.action)
        if success:
            self.log_event(f"{event.action.title()} command sent to {event.service_key}.")
            # Refresh immediately to reflect new status
            self._refresh_table()
        else:
            self.log_event(f"[red]Failed[/] to {event.action} {event.service_key}.")


if __name__ == "__main__":
    DashboardApp().run()
