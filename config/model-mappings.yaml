# Model Routing Configuration
# Defines how LiteLLM routes model requests to providers

# ============================================================================
# EXACT MODEL NAME ROUTING
# ============================================================================

exact_matches:
  # Ollama models (exact name)
  "llama3.1:latest":
    provider: ollama
    priority: primary
    fallback: null
    description: "General-purpose chat model"

  "qwen2.5-coder:7b":
    provider: ollama
    priority: primary
    fallback: null
    description: "Code generation specialist"
  "mythomax-l2-13b-q5_k_m":
    provider: ollama
    priority: primary
    fallback: llama_cpp_python
    description: "Creative writing and roleplay tuned MythoMax L2"
  # vLLM models (AWQ quantized)
  "qwen-coder-vllm":
    provider: vllm-qwen
    priority: primary
    fallback: ollama
    description: "AWQ-quantized 7B code generation model via vLLM"
    backend_model: "Qwen/Qwen2.5-Coder-7B-Instruct-AWQ"

  # Dolphin Uncensored (Mistral-based)
  "dolphin-uncensored-vllm":
    provider: vllm-dolphin
    priority: primary
    fallback: ollama
    description: "7B uncensored Mistral-based model via vLLM (AWQ quantized)"
    backend_model: "solidrust/dolphin-2.8-mistral-7b-v02-AWQ"

  # llama.cpp models
  "llama-cpp-default":
    provider: llama_cpp_python
    priority: primary
    fallback: null
    description: "GGUF model via llama.cpp Python bindings"
  "llama-cpp-llama3.1":
    provider: llama_cpp_python
    priority: primary
    fallback: ollama
    description: "Meta Llama 3.1 8B Instruct Q4_K_M served via llama.cpp Python server"
    backend_model: "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
  "llama-cpp-mistral-nemo":
    provider: llama_cpp_python
    priority: primary
    fallback: ollama
    description: "Mistral Nemo Instruct 12B Q4_K_M via llama.cpp for fast analytical chat"
    backend_model: "Mistral-Nemo-Instruct-2407.Q4_K_M.gguf"
  "llama-cpp-deepseek-coder-lite":
    provider: llama_cpp_python
    priority: primary
    fallback: vllm-qwen
    description: "DeepSeek Coder v2 Lite Q4_K_M for deterministic coding via llama.cpp"
    backend_model: "DeepSeek-Coder-V2-Lite-Instruct.Q4_K_M.gguf"
  "llama-cpp-openhermes-2.5":
    provider: llama_cpp_python
    priority: primary
    fallback: ollama
    description: "OpenHermes 2.5 (Mistral) Q5_K_M served via llama.cpp for rich conversation"
    backend_model: "openhermes-2.5-mistral-7b.Q5_K_M.gguf"
  "llama-cpp-phi3-medium":
    provider: llama_cpp_python
    priority: primary
    fallback: ollama
    description: "Phi-3 Medium 4K Instruct Q4_K_M via llama.cpp for factual Q&A"
    backend_model: "phi-3-medium-4k-instruct.Q4_K_M.gguf"
  "llama-cpp-mixtral-8x7b":
    provider: llama_cpp_python
    priority: secondary
    fallback: ollama
    description: "Mixtral 8x7B Instruct v0.1 Q4_K_M via llama.cpp for heavier reasoning (partial GPU offload)"
    backend_model: "mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf"

  # Ollama Cloud models (remote)
  "deepseek-v3.1:671b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: null
    description: "DeepSeek V3.1 671B hosted on Ollama Cloud"

  "qwen3-coder:480b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: null
    description: "Qwen3 Coder 480B cloud variant"

  "kimi-k2:1t-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: null
    description: "Kimi K2 1T ultra-long context model"

  "gpt-oss:120b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: null
    description: "GPT-OSS 120B open-source aligned model"

  "gpt-oss:20b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: null
    description: "GPT-OSS 20B open-source aligned model"

  "glm-4.6:cloud":
    provider: ollama_cloud
    priority: primary
    fallback: null
    description: "Zhipu GLM 4.6 hosted on Ollama Cloud"

# ============================================================================
# PATTERN-BASED ROUTING
# ============================================================================

patterns:
  # HuggingFace model paths → vLLM
  - pattern: "^Qwen/Qwen2\\.5-Coder.*"
    provider: vllm-qwen
    fallback: ollama
    description: "Qwen Coder models via vLLM (AWQ quantized)"

  - pattern: "^solidrust/dolphin.*AWQ$"
    provider: vllm-dolphin
    fallback: ollama
    description: "Dolphin uncensored models via vLLM (AWQ quantized)"

  - pattern: "^meta-llama/.*"
    provider: vllm-qwen
    fallback: ollama
    description: "HuggingFace Llama models prefer vLLM for performance"

  - pattern: "^mistralai/.*"
    provider: vllm-dolphin
    fallback: ollama
    description: "Mistral models via vLLM"

  # Ollama-style names → Ollama
  - pattern: ".*:\\d+[bB]$"  # Matches "model:7b", "model:13B", etc.
    provider: ollama
    description: "Ollama naming convention (model:size)"

  # GGUF files → llama.cpp
  - pattern: ".*\\.gguf$"
    provider: llama_cpp_python
    fallback: ollama
    description: "GGUF quantized models"

# ============================================================================
# CAPABILITY-BASED ROUTING
# ============================================================================

capabilities:
  code_generation:
    description: "Models specialized for code"
    preferred_models:
      - qwen2.5-coder:7b
      - qwen-coder-vllm
      - llama-cpp-deepseek-coder-lite
      - qwen3-coder:480b-cloud
    provider: ollama
    routing_strategy: load_balance

  analysis:
    description: "Models tuned for structured analysis and planning"
    preferred_models:
      - qwen2.5-coder:7b
      - llama3.1:latest
      - llama-cpp-llama3.1
      - llama-cpp-mistral-nemo
      - llama-cpp-phi3-medium
      - llama-cpp-mixtral-8x7b
      - deepseek-v3.1:671b-cloud
    provider: ollama
    routing_strategy: usage_based

  reasoning:
    description: "Models optimized for multi-step reasoning and problem solving"
    preferred_models:
      - llama3.1:latest
      - llama-cpp-llama3.1
      - llama-cpp-mistral-nemo
      - llama-cpp-mixtral-8x7b
      - qwen-coder-vllm
      - deepseek-v3.1:671b-cloud
    provider: ollama
    routing_strategy: direct

  creative_writing:
    description: "Storytelling and roleplay optimized models"
    preferred_models:
      - mythomax-l2-13b-q5_k_m
      - llama3.1:latest
      - llama-cpp-llama3.1
      - llama-cpp-openhermes-2.5
      - kimi-k2:1t-cloud
    provider: ollama
    routing_strategy: usage_based

  uncensored:
    description: "Uncensored models without content filters"
    preferred_models:
      - dolphin-uncensored-vllm
    provider: vllm-dolphin
    routing_strategy: direct

  conversational:
    description: "Natural conversation and general assistance"
    preferred_models:
      - dolphin-uncensored-vllm
      - llama3.1:latest
      - llama-cpp-llama3.1
      - llama-cpp-mistral-nemo
      - llama-cpp-openhermes-2.5
      - mythomax-l2-13b-q5_k_m
      - gpt-oss:20b-cloud
      - gpt-oss:120b-cloud
      - glm-4.6:cloud
    routing_strategy: usage_based

  high_throughput:
    description: "When you need to handle many concurrent requests"
    min_model_size: "13B"
    provider: vllm-qwen
    routing_strategy: least_loaded

  low_latency:
    description: "Single-request speed priority"
    provider: llama_cpp_python
    fallback: ollama
    routing_strategy: fastest_response

  general_chat:
    description: "General conversational AI"
    preferred_models:
      - llama3.1:latest
      - llama-cpp-llama3.1
      - llama-cpp-mistral-nemo
      - llama-cpp-openhermes-2.5
      - mythomax-l2-13b-q5_k_m
      - llama-cpp-phi3-medium
      - llama-cpp-mixtral-8x7b
      - gpt-oss:20b-cloud
      - gpt-oss:120b-cloud
      - glm-4.6:cloud
    routing_strategy: usage_based

  large_context:
    description: "Models that can handle large context windows"
    min_context: 8192
    providers:
      - llama_cpp_python  # Configured for 8192 context
    preferred_models:
      - llama-cpp-llama3.1
      - llama-cpp-mistral-nemo
      - llama-cpp-mixtral-8x7b
    routing_strategy: most_capacity

# ============================================================================
# LOAD BALANCING CONFIGURATION
# ============================================================================
# Same model available on multiple providers with weighted distribution

load_balancing:
  "llama3.1:latest":
    providers:
      - provider: ollama
        weight: 0.7  # Primary
      - provider: llama_cpp_python
        weight: 0.3  # Backup
    strategy: weighted_round_robin
    description: "Distribute requests with 70% to Ollama, 30% to llama.cpp"

  "general-chat":
    providers:
      - provider: ollama
        weight: 0.6
      - provider: vllm-qwen
        weight: 0.4
    strategy: least_loaded
    description: "Load balance general chat across multiple providers"

# ============================================================================
# FALLBACK CHAINS
# ============================================================================
# Defines fallback order when primary provider fails

fallback_chains:
  "llama3.1:latest":
    # Prefer other capable models if llama3.1 is unavailable
    chain:
      - qwen2.5-coder:7b
      - qwen-coder-vllm
      - dolphin-uncensored-vllm

  "qwen2.5-coder:7b":
    # Ollama code model can fall back to higher throughput or creative options
    chain:
      - qwen-coder-vllm
      - dolphin-uncensored-vllm

  "mythomax-l2-13b-q5_k_m":
    chain:
      - llama3.1:latest
      - qwen-coder-vllm
      - dolphin-uncensored-vllm
  "llama-cpp-llama3.1":
    chain:
      - llama3.1:latest
      - qwen2.5-coder:7b
      - dolphin-uncensored-vllm
  "llama-cpp-mistral-nemo":
    chain:
      - llama3.1:latest
      - llama-cpp-llama3.1
      - dolphin-uncensored-vllm
  "llama-cpp-deepseek-coder-lite":
    chain:
      - qwen-coder-vllm
      - qwen2.5-coder:7b
      - dolphin-uncensored-vllm
  "llama-cpp-openhermes-2.5":
    chain:
      - llama3.1:latest
      - mythomax-l2-13b-q5_k_m
      - llama-cpp-llama3.1
  "llama-cpp-phi3-medium":
    chain:
      - llama3.1:latest
      - llama-cpp-mistral-nemo
      - llama-cpp-llama3.1
  "llama-cpp-mixtral-8x7b":
    chain:
      - llama-cpp-mistral-nemo
      - llama-cpp-llama3.1
      - deepseek-v3.1:671b-cloud

  "qwen-coder-vllm":
    chain:
      - dolphin-uncensored-vllm

  default:
    # Default fallback chain for all models
    chain:
      - llama3.1:latest
      - qwen2.5-coder:7b
      - qwen-coder-vllm
      - dolphin-uncensored-vllm

# ============================================================================
# ROUTING RULES
# ============================================================================

routing_rules:
  # Rule precedence (highest to lowest):
  # 1. Exact model name match
  # 2. Capability-based routing
  # 3. Pattern matching
  # 4. Default provider

  default_provider: ollama
  default_fallback: ollama

  request_metadata_routing:
    # Route based on request headers/metadata
    high_priority_requests:
      provider: vllm-qwen
      condition: header.x-priority == "high"

    batch_requests:
      provider: vllm-qwen
      condition: batch_size > 1

    streaming_requests:
      provider: ollama  # Best streaming support
      condition: stream == true

  model_size_routing:
    # Automatic provider selection based on model size
    - size: "< 8B"
      provider: ollama
      reason: "Small models work well with Ollama"

    - size: "8B - 13B"
      provider: ollama
      fallback: vllm-qwen
      reason: "Medium models, Ollama preferred"

    - size: "> 13B"
      provider: vllm-qwen
      fallback: ollama
      reason: "Large models benefit from vLLM batching"

# ============================================================================
# SPECIAL ROUTING CASES
# ============================================================================

special_cases:
  first_request_routing:
    description: "Cold start optimization"
    strategy: prefer_warm_providers
    warm_check_timeout_ms: 100

  rate_limited_fallback:
    description: "Automatic fallback when provider hits rate limit"
    enabled: true
    fallback_duration_seconds: 60

  error_based_routing:
    description: "Avoid providers with recent errors"
    enabled: true
    error_threshold: 3
    cooldown_seconds: 300

  geographic_routing:
    description: "Route based on provider location (future)"
    enabled: false
    prefer_local: true

# ============================================================================
# METADATA
# ============================================================================

metadata:
  version: "1.4"
  last_updated: "2025-10-23"
  total_routing_rules: 18+
  supported_routing_strategies:
    - exact_match
    - pattern_match
    - capability_based
    - load_balanced
    - fallback_chain
  changes:
    - "Replaced DeepSeek-R1 with Dolphin 2.8 Mistral AWQ (uncensored, fits 16GB VRAM)"
    - "Added solidrust/dolphin-2.8-mistral-7b-v02-AWQ model"
    - "Updated uncensored and conversational capability routing"
    - "Added Dolphin AWQ pattern matching"
    - "Optimized for 16GB VRAM constraint with AWQ quantization"

notes: |
  This configuration is consumed by LiteLLM's router.

  When adding new models:
  1. Add exact match entry if it has a canonical name
  2. Consider which capability category it belongs to
  3. Define fallback chain for reliability
  4. Test routing with validation script

  When adding new providers:
  1. Update providers.yaml first
  2. Add routing rules here
  3. Update litellm-unified.yaml
  4. Test end-to-end
