# AI Backend Provider Registry
# Master catalog of all LLM inference providers in LAB ecosystem

providers:
  # ============================================================================
  # ACTIVE PROVIDERS
  # ============================================================================

  ollama:
    type: ollama
    base_url: http://127.0.0.1:11434
    status: active
    description: Local Ollama server for general-purpose models
    features:
      - Easy model management (ollama pull/list)
      - CPU and GPU support
      - QuantizedGGUF format
      - Fast model switching
    models:
      - name: llama3.1:8b
        size: "8B"
        quantization: Q4_K_M
        pulled_at: "2025-10-19"
      - name: qwen2.5-coder:7b
        size: "7.6B"
        quantization: Q4_K_M
        pulled_at: "2025-10-19"
        specialty: code_generation
    health_endpoint: /api/tags
    docs: https://ollama.ai/docs
    location: ../openwebui (integrated)

  llama_cpp_python:
    type: llama_cpp
    base_url: http://127.0.0.1:8000
    status: active
    description: llama.cpp Python bindings server with CUDA support
    features:
      - GPU offloading (full or partial)
      - Context window up to 8192 tokens
      - Parallel requests (4 concurrent)
      - OpenAI-compatible API
    configuration:
      n_gpu_layers: -1  # Full GPU offload
      n_ctx: 8192
      n_parallel: 4
      threads: 6
    health_endpoint: /v1/models
    service_file: ~/.config/systemd/user/llamacpp-python.service
    location: ../openwebui/backends/llama.cpp

  llama_cpp_native:
    type: llama_cpp
    base_url: http://127.0.0.1:8080
    status: active
    description: llama.cpp native C++ server (maximum performance)
    features:
      - Pure C/C++ implementation
      - No Python overhead
      - Maximum throughput
      - CUDA-optimized
    health_endpoint: /v1/models
    location: ../openwebui/backends/llama.cpp

  vllm:
    type: vllm
    base_url: http://127.0.0.1:8001
    status: active  # Deployed with AWQ quantized Qwen model
    description: vLLM high-throughput inference with AWQ quantization
    features:
      - Continuous batching
      - PagedAttention for KV cache
      - AWQ quantization (4-bit)
      - Optimized for code generation
      - Low memory footprint
    models:
      - name: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
        size: "7B"
        quantization: AWQ
        context_length: 4096
        specialty: code_generation
        model_size_gb: 5.2
        kv_cache_available_gb: 7.36
    health_endpoint: /v1/models
    api_format: openai_compatible
    gpu_memory_utilization: 0.9
    max_model_len: 4096
    location: ~/venvs/vllm
    hardware_requirements:
      gpu_vram_gb: 16
      gpu_model: "Quadro RTX 5000"
      compute_capability: "7.5"
    mcp_integration: true
    mcp_tools:
      - generate_text
      - chat_completion
      - stream_completion
      - server_status
      - server_metrics

  # ============================================================================
  # OPENWEBUI SERVICES (INTEGRATED)
  # ============================================================================

  openwebui_pipelines:
    type: openai_compatible
    base_url: http://127.0.0.1:9099/v1
    status: active
    description: OpenWebUI custom pipeline server with 6 pipelines
    features:
      - Academic paper search (arXiv, PubMed)
      - Market data snapshots
      - Smart query routing
      - Hybrid search (keyword + semantic)
      - Multi-model consensus
      - Code analysis pipeline
    pipelines:
      - academic_search
      - market_snapshot
      - smart_router
      - hybrid_search
      - consensus
      - code_analyzer
    health_endpoint: /health
    api_format: openai_compatible
    service_file: ~/.config/systemd/user/openwebui-pipelines.service
    location: ../openwebui/pipelines
    dependencies:
      - toolsrv  # For academic_search pipeline
      - ollama   # For smart_router pipeline

  openwebui_toolsrv:
    type: tool_server
    base_url: http://127.0.0.1:8600
    status: active
    description: OpenWebUI FastAPI tool server (arXiv, PubMed, market data)
    features:
      - arXiv paper search
      - PubMed biomedical search
      - Market data retrieval (mock fallback)
      - Rate limiting (60 req/min per IP)
      - Request queueing
    tools:
      - arxiv_search
      - pubmed_search
      - market_snapshot
    health_endpoint: /health
    service_file: ~/.config/systemd/user/toolsrv.service
    location: ../openwebui/toolsrv
    rate_limit:
      requests_per_minute: 60
      per_ip: true

  openwebui_frontend:
    type: web_ui
    base_url: http://127.0.0.1:5000
    status: active
    description: OpenWebUI web interface (GPU-accelerated, RAG-enabled)
    features:
      - Chat interface
      - Model selection (Ollama, llama.cpp, pipelines)
      - RAG knowledge base
      - Tool integration
      - Admin panel
    health_endpoint: /
    service_file: ~/.config/systemd/user/openwebui-native.service
    location: ../openwebui
    data_directory: ~/.local/share/openwebui
    rag_config:
      chunk_size: 768
      chunk_overlap: 128
      top_k: 10
      min_score: 0.35
      embed_model: snowflake-arctic-embed:33m

  # ============================================================================
  # TEMPLATE PROVIDERS (Disabled, for reference)
  # ============================================================================

  openai:
    type: openai
    base_url: https://api.openai.com/v1
    status: disabled
    description: OpenAI cloud API (requires API key)
    requires_api_key: true
    env_var: OPENAI_API_KEY
    models:
      - gpt-4
      - gpt-3.5-turbo
    rate_limits:
      requests_per_minute: 3500
      tokens_per_minute: 90000
    cost_per_1k_tokens:
      gpt-4: 0.03
      gpt-3.5-turbo: 0.0015

  anthropic:
    type: anthropic
    base_url: https://api.anthropic.com/v1
    status: disabled
    description: Anthropic Claude API (requires API key)
    requires_api_key: true
    env_var: ANTHROPIC_API_KEY
    models:
      - claude-3-opus-20240229
      - claude-3-sonnet-20240229

  custom_openai_compatible:
    type: openai_compatible
    base_url: http://localhost:CUSTOM_PORT
    status: template
    description: Template for any OpenAI-compatible API server
    notes: |
      Use this template to add:
      - LocalAI servers
      - Text Generation WebUI
      - FastChat servers
      - Any server implementing OpenAI API spec

# ============================================================================
# PROVIDER METADATA
# ============================================================================

metadata:
  version: "1.1"
  last_updated: "2025-10-21"
  total_providers: 4  # active providers (ollama, llama_cpp_python, llama_cpp_native, vllm)
  total_models_available: 6  # Including vLLM models

  provider_types:
    - ollama: Simple local server
    - llama_cpp: High-performance C++ inference
    - vllm: Production-grade batched inference
    - openai: Cloud API providers
    - openai_compatible: Generic OpenAI-compatible servers

  selection_criteria:
    ollama:
      best_for: Quick prototyping, easy model management
      performance: Medium
      resource_usage: Low to Medium

    llama_cpp:
      best_for: Maximum single-request performance
      performance: High
      resource_usage: Medium

    vllm:
      best_for: High concurrency, production workloads
      performance: Very High (batched)
      resource_usage: High (requires GPU)

# ============================================================================
# HEALTH CHECK CONFIGURATION
# ============================================================================

health_checks:
  enabled: true
  interval_seconds: 60
  timeout_seconds: 5
  retry_attempts: 3

  endpoints:
    ollama: http://127.0.0.1:11434/api/tags
    llama_cpp_python: http://127.0.0.1:8000/v1/models
    llama_cpp_native: http://127.0.0.1:8080/v1/models
    vllm: http://127.0.0.1:8001/v1/models
