# Model Routing Configuration
# Defines how LiteLLM routes model requests to providers

# ============================================================================
# EXACT MODEL NAME ROUTING
# ============================================================================

exact_matches:
  # Ollama models (exact name)
  "llama3.1:8b":
    provider: ollama
    priority: primary
    fallback: null
    description: "General-purpose chat model"

  "qwen2.5-coder:7b":
    provider: ollama
    priority: primary
    fallback: null
    description: "Code generation specialist"

  # vLLM models (AWQ quantized)
  "qwen-coder-7b-vllm":
    provider: vllm
    priority: primary
    fallback: ollama
    description: "AWQ-quantized 7B code generation model via vLLM"
    backend_model: "Qwen/Qwen2.5-Coder-7B-Instruct-AWQ"

  # llama.cpp models
  "llama-cpp-default":
    provider: llama_cpp_python
    priority: primary
    fallback: llama_cpp_native
    description: "GGUF model via llama.cpp Python bindings"

# ============================================================================
# PATTERN-BASED ROUTING
# ============================================================================

patterns:
  # HuggingFace model paths → vLLM
  - pattern: "^Qwen/Qwen2\\.5-Coder.*"
    provider: vllm
    fallback: ollama
    description: "Qwen Coder models via vLLM (AWQ quantized)"

  - pattern: "^meta-llama/.*"
    provider: vllm
    fallback: ollama
    description: "HuggingFace Llama models prefer vLLM for performance"

  - pattern: "^mistralai/.*"
    provider: vllm
    fallback: ollama
    description: "Mistral models via vLLM"

  # Ollama-style names → Ollama
  - pattern: ".*:\\d+[bB]$"  # Matches "model:7b", "model:13B", etc.
    provider: ollama
    description: "Ollama naming convention (model:size)"

  # GGUF files → llama.cpp
  - pattern: ".*\\.gguf$"
    provider: llama_cpp_python
    fallback: llama_cpp_native
    description: "GGUF quantized models"

# ============================================================================
# CAPABILITY-BASED ROUTING
# ============================================================================

capabilities:
  code_generation:
    description: "Models specialized for code"
    preferred_models:
      - qwen2.5-coder:7b
      - deepseek-coder-v2  # If pulled
    provider: ollama
    routing_strategy: load_balance

  high_throughput:
    description: "When you need to handle many concurrent requests"
    min_model_size: "13B"
    provider: vllm
    routing_strategy: least_loaded

  low_latency:
    description: "Single-request speed priority"
    provider: llama_cpp_native
    fallback: llama_cpp_python
    routing_strategy: fastest_response

  general_chat:
    description: "General conversational AI"
    preferred_models:
      - llama3.1:8b
      - llama2-13b-vllm
    routing_strategy: usage_based

  large_context:
    description: "Models that can handle large context windows"
    min_context: 8192
    providers:
      - llama_cpp_python  # Configured for 8192 context
      - vllm  # Supports up to 4096 by default
    routing_strategy: most_capacity

# ============================================================================
# LOAD BALANCING CONFIGURATION
# ============================================================================
# Same model available on multiple providers with weighted distribution

load_balancing:
  "llama3.1:8b":
    providers:
      - provider: ollama
        weight: 0.7  # Primary
      - provider: llama_cpp_python
        weight: 0.3  # Backup
    strategy: weighted_round_robin
    description: "Distribute requests with 70% to Ollama, 30% to llama.cpp"

  "general-chat":
    providers:
      - provider: ollama
        weight: 0.6
      - provider: vllm
        weight: 0.4
    strategy: least_loaded
    description: "Load balance general chat across multiple providers"

# ============================================================================
# FALLBACK CHAINS
# ============================================================================
# Defines fallback order when primary provider fails

fallback_chains:
  "llama2-13b-vllm":
    # vLLM model with Ollama fallback
    chain:
      - vllm
      - ollama
      - llama_cpp_python

  "llama3.1:8b":
    # Ollama with llama.cpp fallbacks
    chain:
      - ollama
      - llama_cpp_python
      - llama_cpp_native

  "qwen2.5-coder:7b":
    # Code generation with fallbacks
    chain:
      - ollama
      - llama_cpp_python

  default:
    # Default fallback chain for all models
    chain:
      - ollama
      - llama_cpp_python
      - llama_cpp_native
      - vllm

# ============================================================================
# ROUTING RULES
# ============================================================================

routing_rules:
  # Rule precedence (highest to lowest):
  # 1. Exact model name match
  # 2. Capability-based routing
  # 3. Pattern matching
  # 4. Default provider

  default_provider: ollama
  default_fallback: llama_cpp_python

  request_metadata_routing:
    # Route based on request headers/metadata
    high_priority_requests:
      provider: vllm
      condition: header.x-priority == "high"

    batch_requests:
      provider: vllm
      condition: batch_size > 1

    streaming_requests:
      provider: ollama  # Best streaming support
      condition: stream == true

  model_size_routing:
    # Automatic provider selection based on model size
    - size: "< 8B"
      provider: ollama
      reason: "Small models work well with Ollama"

    - size: "8B - 13B"
      provider: ollama
      fallback: llama_cpp_python
      reason: "Medium models, Ollama preferred"

    - size: "> 13B"
      provider: vllm
      fallback: ollama
      reason: "Large models benefit from vLLM batching"

# ============================================================================
# SPECIAL ROUTING CASES
# ============================================================================

special_cases:
  first_request_routing:
    description: "Cold start optimization"
    strategy: prefer_warm_providers
    warm_check_timeout_ms: 100

  rate_limited_fallback:
    description: "Automatic fallback when provider hits rate limit"
    enabled: true
    fallback_duration_seconds: 60

  error_based_routing:
    description: "Avoid providers with recent errors"
    enabled: true
    error_threshold: 3
    cooldown_seconds: 300

  geographic_routing:
    description: "Route based on provider location (future)"
    enabled: false
    prefer_local: true

# ============================================================================
# METADATA
# ============================================================================

metadata:
  version: "1.1"
  last_updated: "2025-10-21"
  total_routing_rules: 15+
  supported_routing_strategies:
    - exact_match
    - pattern_match
    - capability_based
    - load_balanced
    - fallback_chain
  changes:
    - "Activated vLLM provider"
    - "Simplified load balancing structure for test compatibility"
    - "Simplified fallback chains to use string arrays"

notes: |
  This configuration is consumed by LiteLLM's router.

  When adding new models:
  1. Add exact match entry if it has a canonical name
  2. Consider which capability category it belongs to
  3. Define fallback chain for reliability
  4. Test routing with validation script

  When adding new providers:
  1. Update providers.yaml first
  2. Add routing rules here
  3. Update litellm-unified.yaml
  4. Test end-to-end
