#!/usr/bin/env python3
"""Interactive AI backend command center built with Textual.

This module provides a comprehensive terminal-based dashboard for monitoring
and managing AI backend LLM provider services including Ollama, vLLM, and
llama.cpp. It displays real-time metrics, GPU usage, and system resources,
plus controls to start/stop/restart services.

Example:
    Run the dashboard interactively::

        python3 scripts/ai-dashboard

    Or from project root::

        ./ai-dashboard

Key Bindings:
    - 'r': Manual refresh
    - 'q': Quit
    - 'a': Toggle auto-refresh
    - 'ctrl+l': Clear event log
"""

from __future__ import annotations

import json
import logging
import os
import subprocess
import sys
import time
import warnings
from collections.abc import Iterable
from dataclasses import dataclass, asdict, field
from pathlib import Path
from typing import Literal
from urllib.parse import urlparse

import psutil
import requests  # type: ignore[import-untyped]
import yaml  # type: ignore[import-untyped]
from textual import on
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Container, Horizontal, Vertical
from textual.coordinate import Coordinate
from textual.message import Message
from textual.reactive import reactive
from textual.widgets import Button, DataTable, Footer, Header, Label, Log, Static

# Configuration and logging setup
logger = logging.getLogger(__name__)
_log_formatter = logging.Formatter(
    "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
_handler = logging.StreamHandler()
_handler.setFormatter(_log_formatter)
logger.addHandler(_handler)
logger.setLevel(logging.DEBUG)

# Constants for state persistence
STATE_DIR = Path.home() / ".cache" / "ai-dashboard"
STATE_FILE = STATE_DIR / "dashboard_state.json"

# ===== SECURITY: Service name allowlist (prevent command injection) =====
ALLOWED_SERVICES: dict[str, str] = {
    "ollama": "ollama.service",
    "vllm": "vllm.service",
    "vllm_dolphin": "vllm-dolphin.service",
    "llama_cpp_python": "llamacpp-python.service",
    "llama_cpp_native": "llama-cpp-native.service",
    "litellm_gateway": "litellm.service",
}

ALLOWED_ACTIONS: set[Literal["start", "stop", "restart", "enable", "disable"]] = {
    "start", "stop", "restart", "enable", "disable"
}

# ===== Configuration constants (environment overridable) =====
def _load_config() -> tuple[float, int, int]:
    """Load and validate configuration from environment variables.

    Security: Validates all inputs to prevent injection attacks.

    Returns:
        Tuple of (http_timeout, refresh_interval, log_height)

    Raises:
        ValueError: If any configuration value is invalid
    """
    try:
        http_timeout = float(os.getenv("AI_DASH_HTTP_TIMEOUT", "3.0"))
        if not 0.5 <= http_timeout <= 30:
            raise ValueError(f"HTTP_TIMEOUT must be 0.5-30 seconds, got {http_timeout}")
    except ValueError as e:
        logger.error(f"Invalid AI_DASH_HTTP_TIMEOUT: {e}")
        raise ValueError(f"Invalid AI_DASH_HTTP_TIMEOUT: {e}") from None

    try:
        refresh_interval = int(os.getenv("AI_DASH_REFRESH_INTERVAL", "5"))
        if not 1 <= refresh_interval <= 60:
            raise ValueError(f"REFRESH_INTERVAL must be 1-60 seconds, got {refresh_interval}")
    except ValueError as e:
        logger.error(f"Invalid AI_DASH_REFRESH_INTERVAL: {e}")
        raise ValueError(f"Invalid AI_DASH_REFRESH_INTERVAL: {e}") from None

    try:
        log_height = int(os.getenv("AI_DASH_LOG_HEIGHT", "12"))
        if not 5 <= log_height <= 50:
            raise ValueError(f"LOG_HEIGHT must be 5-50 lines, got {log_height}")
    except ValueError as e:
        logger.error(f"Invalid AI_DASH_LOG_HEIGHT: {e}")
        raise ValueError(f"Invalid AI_DASH_LOG_HEIGHT: {e}") from None

    return http_timeout, refresh_interval, log_height


def _load_providers_config() -> dict | None:
    """Load provider configuration from YAML if available.

    Returns:
        Provider configuration dict or None if file not found
    """
    config_path = Path(__file__).parent.parent / "config" / "providers.yaml"
    if not config_path.exists():
        logger.debug(f"Provider config not found at {config_path}")
        return None

    try:
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
            logger.info(f"Loaded provider config from {config_path}")
            return config
    except Exception as e:
        logger.warning(f"Failed to load provider config: {type(e).__name__}: {e}")
        return None


DEFAULT_HTTP_TIMEOUT, DEFAULT_REFRESH_INTERVAL, DEFAULT_LOG_HEIGHT = _load_config()
PROVIDER_CONFIG = _load_providers_config()

# ---------------------------------------------------------------------------
# Data models


@dataclass
class ServiceMetrics:
    """Snapshot of a provider's health and resource usage."""

    key: str
    display: str
    required: bool
    status: str
    port: int | None
    models: int | None
    cpu_percent: float
    memory_mb: float
    memory_percent: float
    vram_mb: float
    vram_percent: float
    response_ms: float
    pid: int | None
    notes: list[str] = field(default_factory=list)
    timestamp: float = field(default_factory=time.time)

    def to_json(self) -> dict:
        """Convert to JSON-serializable dictionary."""
        data = asdict(self)
        data["notes"] = self.notes or []
        return data

    @classmethod
    def from_json(cls, data: dict) -> "ServiceMetrics":
        """Create instance from JSON-serializable dictionary."""
        # Handle defaults for optional fields
        return cls(
            key=data["key"],
            display=data["display"],
            required=data["required"],
            status=data["status"],
            port=data.get("port"),
            models=data.get("models"),
            cpu_percent=data.get("cpu_percent", 0.0),
            memory_mb=data.get("memory_mb", 0.0),
            memory_percent=data.get("memory_percent", 0.0),
            vram_mb=data.get("vram_mb", 0.0),
            vram_percent=data.get("vram_percent", 0.0),
            response_ms=data.get("response_ms", 0.0),
            pid=data.get("pid"),
            notes=data.get("notes", []),
            timestamp=data.get("timestamp", time.time()),
        )


@dataclass
class GPUOverview:
    """Aggregated GPU utilisation information."""

    detected: bool
    per_gpu: list[dict[str, float]]
    total_used_mb: float
    total_capacity_mb: float
    peak_util_percent: float


# ---------------------------------------------------------------------------
# State persistence functions


def save_dashboard_state(metrics: list[ServiceMetrics], selected_key: str | None) -> bool:
    """Save dashboard state for recovery.

    Args:
        metrics: List of ServiceMetrics to save
        selected_key: Currently selected provider key

    Returns:
        True if save succeeded, False otherwise
    """
    try:
        STATE_DIR.mkdir(parents=True, exist_ok=True)
        state = {
            "timestamp": time.time(),
            "selected_key": selected_key,
            "metrics": [m.to_json() for m in metrics],
        }
        with open(STATE_FILE, "w") as f:
            json.dump(state, f, indent=2)
        logger.debug(f"Saved dashboard state to {STATE_FILE}")
        return True
    except Exception as e:
        logger.warning(f"Failed to save dashboard state: {type(e).__name__}: {e}")
        return False


def load_dashboard_state() -> tuple[list[ServiceMetrics], str | None] | None:
    """Load dashboard state from previous session.

    Returns:
        Tuple of (metrics list, selected_key) or None if load failed
    """
    if not STATE_FILE.exists():
        return None

    try:
        with open(STATE_FILE, "r") as f:
            state = json.load(f)
        metrics = [ServiceMetrics.from_json(m) for m in state.get("metrics", [])]
        selected_key = state.get("selected_key")
        logger.debug(f"Loaded dashboard state from {STATE_FILE}")
        return metrics, selected_key
    except Exception as e:
        logger.warning(f"Failed to load dashboard state: {type(e).__name__}: {e}")
        return None


# ---------------------------------------------------------------------------
# Monitoring helpers


class GPUMonitor:
    """Utility helpers around NVIDIA's NVML for VRAM insight.

    Provides methods to query GPU information and per-process VRAM usage.
    Gracefully handles systems without NVIDIA GPUs or CUDA drivers installed.
    """

    def __init__(self) -> None:
        """Initialize GPU monitor with NVML."""
        self.initialized = False
        self.device_count = 0
        self._pynvml = None

        try:
            with warnings.catch_warnings():
                warnings.filterwarnings(
                    "ignore",
                    message="The pynvml package is deprecated",
                    category=FutureWarning,
                )
                import pynvml

            pynvml.nvmlInit()
            self._pynvml = pynvml
            self.initialized = True
            self.device_count = pynvml.nvmlDeviceGetCount()
            logger.debug(f"GPU monitoring initialized: {self.device_count} device(s) detected")
        except ImportError as e:
            logger.debug(f"NVIDIA GPU driver not available: {e}")
            self.initialized = False
        except (OSError, RuntimeError) as e:
            logger.debug(f"NVIDIA NVML initialization failed: {e}")
            self.initialized = False
        except Exception as e:  # pragma: no cover - unexpected errors
            logger.warning(f"Unexpected error initializing GPU monitor: {type(e).__name__}: {e}")
            self.initialized = False

    def get_gpu_info(self) -> list[dict[str, float]]:
        """Query all GPU information and memory stats.

        Returns:
            List of GPU info dicts with memory and utilization metrics.
            Empty list if GPU monitoring not available.
        """
        if not self.initialized or self._pynvml is None:
            return []

        pynvml = self._pynvml
        info: list[dict[str, float]] = []

        for index in range(self.device_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(index)
                name = pynvml.nvmlDeviceGetName(handle).decode("utf-8")
                memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
            except (OSError, AttributeError, UnicodeDecodeError) as e:
                logger.debug(f"Error querying GPU {index}: {type(e).__name__}: {e}")
                continue
            except Exception as e:  # pragma: no cover - unexpected errors
                logger.warning(f"Unexpected error querying GPU {index}: {type(e).__name__}: {e}")
                continue

            total_mb = memory.total / 1024**2
            used_mb = memory.used / 1024**2

            info.append(
                {
                    "id": float(index),
                    "name": name,
                    "memory_total_mb": total_mb,
                    "memory_used_mb": used_mb,
                    "memory_util_percent": (used_mb / total_mb * 100) if total_mb else 0.0,
                    "gpu_util_percent": float(util.gpu),
                }
            )

        return info

    def get_process_vram(self) -> dict[int, tuple[float, float]]:
        """Return VRAM usage per PID as (used_mb, percentage of associated GPUs).

        Returns:
            Dict mapping process PIDs to (vram_mb, vram_percent) tuples.
            Empty dict if GPU monitoring not available.
        """
        if not self.initialized or self._pynvml is None:
            return {}

        pynvml = self._pynvml
        usage: dict[int, dict[str, float]] = {}

        for index in range(self.device_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(index)
                memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
                total_mb = memory.total / 1024**2 if memory.total else 0.0
                processes = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)
            except (OSError, AttributeError) as e:
                logger.debug(f"Error querying GPU {index} processes: {type(e).__name__}: {e}")
                continue
            except Exception as e:  # pragma: no cover - unexpected errors
                logger.warning(f"Unexpected error querying GPU {index} processes: {type(e).__name__}: {e}")
                continue

            for record in processes or []:
                pid = getattr(record, "pid", None)
                used_bytes = getattr(record, "usedGpuMemory", 0)
                if pid is None or used_bytes is None or used_bytes < 0:
                    continue

                used_mb = used_bytes / 1024**2
                bucket = usage.setdefault(pid, {"used": 0.0, "total": 0.0})
                bucket["used"] += used_mb
                if total_mb > 0:
                    bucket["total"] += total_mb

        per_pid: dict[int, tuple[float, float]] = {}
        for pid, values in usage.items():
            total = values["total"] or 0.0
            used = values["used"]
            per_pid[pid] = (used, (used / total * 100) if total else 0.0)

        return per_pid


class ProviderMonitor:
    """Collects live diagnostics for each backend LLM provider.

    Monitors provider health through HTTP endpoint checks, collects system
    resource usage (CPU, memory, VRAM), and provides service control via
    systemctl. Supports Ollama, vLLM, and llama.cpp providers.

    Attributes:
        gpu_monitor: GPUMonitor instance for NVIDIA GPU metrics
        PROVIDERS: Registry of provider configurations (endpoint, service, etc.)
    """

    # SECURITY: Allowed endpoint hosts (localhost only, prevent SSRF)
    ALLOWED_HOSTS: set[str] = {"127.0.0.1", "localhost"}
    ALLOWED_PORTS: set[int] = {11434, 8000, 8001, 8002, 8080, 4000}

    PROVIDERS: dict[str, dict[str, object]] = {
        "ollama": {
            "endpoint": "http://127.0.0.1:11434/api/tags",
            "display": "Ollama",
            "service": "ollama.service",
            "required": False,
            "type": "ollama",
        },
        "vllm": {
            "endpoint": "http://127.0.0.1:8001/v1/models",
            "display": "vLLM",
            "service": "vllm.service",
            "required": True,
            "type": "litellm",
        },
        "vllm_dolphin": {
            "endpoint": "http://127.0.0.1:8002/v1/models",
            "display": "vLLM (Dolphin)",
            "service": "vllm-dolphin.service",
            "required": False,
            "type": "litellm",
        },
        "llama_cpp_python": {
            "endpoint": "http://127.0.0.1:8000/v1/models",
            "display": "llama.cpp (Python)",
            "service": "llamacpp-python.service",
            "required": False,
            "type": "litellm",
        },
        "llama_cpp_native": {
            "endpoint": "http://127.0.0.1:8080/v1/models",
            "display": "llama.cpp (Native)",
            "service": "llama-cpp-native.service",
            "required": False,
            "type": "litellm",
        },
        "litellm_gateway": {
            "endpoint": "http://127.0.0.1:4000/v1/models",
            "display": "LiteLLM Gateway",
            "service": "litellm.service",
            "required": True,
            "type": "litellm",
        },
    }

    def __init__(self) -> None:
        """Initialize provider monitor with validation."""
        self.gpu_monitor = GPUMonitor()
        # SECURITY: Validate all endpoints at init time
        self._validate_endpoints()

    def _validate_endpoints(self) -> None:
        """SECURITY: Validate all provider endpoints against allowlists.

        Prevents SSRF attacks by ensuring all endpoints are localhost only.

        Raises:
            ValueError: If any endpoint fails validation
        """
        for key, cfg in self.PROVIDERS.items():
            endpoint = str(cfg.get("endpoint", ""))
            parsed = urlparse(endpoint)

            # Validate scheme
            if parsed.scheme not in ("http", "https"):
                raise ValueError(f"Invalid endpoint scheme for {key}: {parsed.scheme}")

            # SECURITY: Validate host (localhost only)
            if parsed.hostname not in self.ALLOWED_HOSTS:
                raise ValueError(f"Invalid endpoint host for {key}: {parsed.hostname}")

            # SECURITY: Validate port
            if parsed.port and parsed.port not in self.ALLOWED_PORTS:
                raise ValueError(f"Invalid endpoint port for {key}: {parsed.port}")

            logger.debug(f"✓ Validated endpoint for {key}: {endpoint}")

    # --------------------------- system helpers ---------------------------------

    def _service_name(self, key: str) -> str | None:
        record = self.PROVIDERS.get(key)
        return None if record is None else str(record["service"])

    def _get_service_pid(self, key: str) -> int | None:
        service = self._service_name(key)
        if not service:
            return None

        try:
            result = subprocess.run(
                ["systemctl", "--user", "show", service, "--property=MainPID", "--value"],
                capture_output=True,
                text=True,
                check=False,
                timeout=2,
            )
        except (subprocess.SubprocessError, FileNotFoundError):  # pragma: no cover - OS specific
            return None

        value = result.stdout.strip()
        if not value:
            return None
        try:
            pid = int(value)
        except ValueError:
            return None
        return pid if pid > 0 else None

    def _collect_process_metrics(self, key: str) -> tuple[int | None, float, float]:
        pid = self._get_service_pid(key)
        if pid is None:
            return None, 0.0, 0.0

        try:
            proc = psutil.Process(pid)
            cpu = proc.cpu_percent(interval=0.0)
            memory_mb = proc.memory_info().rss / 1024**2
            return pid, cpu, memory_mb
        except psutil.Error:
            return None, 0.0, 0.0

    def _parse_models(self, key: str, response_json: dict) -> int:
        provider_type = str(self.PROVIDERS[key]["type"])
        try:
            if provider_type == "ollama":
                return len(response_json.get("models", []))
            return len(response_json.get("data", []))
        except Exception:
            return 0

    # ----------------------------- public API ----------------------------------

    def collect_snapshot(self) -> tuple[list[ServiceMetrics], GPUOverview]:
        """Collect health and resource metrics for all providers.

        Returns:
            Tuple of (metrics list, gpu_overview)
        """
        try:
            gpu_info = self.gpu_monitor.get_gpu_info()
            per_pid_vram = self.gpu_monitor.get_process_vram()
            system_memory_total = psutil.virtual_memory().total / 1024**2
        except Exception as e:
            logger.error(f"Error collecting system metrics: {type(e).__name__}: {e}")
            gpu_info = []
            per_pid_vram = {}
            system_memory_total = 1.0

        metrics: list[ServiceMetrics] = []
        elapsed_ms = 0.0

        for key, cfg in self.PROVIDERS.items():
            notes: list[str] = []
            endpoint = str(cfg["endpoint"])
            required = bool(cfg["required"])
            display = str(cfg["display"])

            start = time.perf_counter()
            status = "inactive"
            models = 0

            try:
                response = requests.get(endpoint, timeout=DEFAULT_HTTP_TIMEOUT)
                elapsed_ms = (time.perf_counter() - start) * 1000

                if response.ok:
                    try:
                        payload = response.json()
                        status = "active"
                        models = self._parse_models(key, payload)
                        logger.debug(f"{key}: ✓ Active with {models} models ({elapsed_ms:.0f}ms)")
                    except ValueError as e:
                        logger.error(f"{key}: Invalid JSON response: {e}")
                        notes.append("Invalid JSON payload")
                        status = "degraded" if required else "inactive"
                else:
                    reason = f"HTTP {response.status_code}"
                    if response.reason:
                        reason += f" ({response.reason})"
                    logger.warning(f"{key}: Provider responded with {reason}")
                    notes.append(reason)
                    status = "degraded" if required else "inactive"
            except requests.exceptions.Timeout:
                elapsed_ms = (time.perf_counter() - start) * 1000
                logger.warning(f"{key}: Request timeout after {elapsed_ms:.0f}ms")
                notes.append(f"Timeout ({elapsed_ms:.0f}ms)")
                status = "degraded" if required else "inactive"
            except requests.exceptions.ConnectionError as e:
                elapsed_ms = (time.perf_counter() - start) * 1000
                logger.warning(f"{key}: Connection error: {str(e)[:50]}")
                notes.append("Connection refused")
                status = "degraded" if required else "inactive"
            except requests.exceptions.RequestException as e:
                elapsed_ms = (time.perf_counter() - start) * 1000
                logger.warning(f"{key}: Request failed ({type(e).__name__}): {str(e)[:50]}")
                notes.append(f"{type(e).__name__}")
                status = "degraded" if required else "inactive"
            except Exception as e:  # pragma: no cover - unexpected errors
                elapsed_ms = (time.perf_counter() - start) * 1000
                logger.error(f"{key}: Unexpected error: {type(e).__name__}: {e}")
                notes.append(f"Unexpected error: {type(e).__name__}")
                status = "degraded" if required else "inactive"

            pid, cpu_percent, memory_mb = self._collect_process_metrics(key)
            memory_percent = (memory_mb / system_memory_total * 100) if system_memory_total else 0.0
            vram_mb, vram_percent = per_pid_vram.get(pid, (0.0, 0.0)) if pid else (0.0, 0.0)

            metrics.append(
                ServiceMetrics(
                    key=key,
                    display=display,
                    required=required,
                    status=status,
                    port=urlparse(endpoint).port,
                    models=models,
                    cpu_percent=cpu_percent,
                    memory_mb=memory_mb,
                    memory_percent=memory_percent,
                    vram_mb=vram_mb,
                    vram_percent=vram_percent,
                    response_ms=elapsed_ms,
                    pid=pid,
                    notes=notes,
                )
            )

        if gpu_info:
            total_used = sum(entry["memory_used_mb"] for entry in gpu_info)
            total_capacity = sum(entry["memory_total_mb"] for entry in gpu_info)
            peak_util = max(entry["gpu_util_percent"] for entry in gpu_info)
        else:
            total_used = total_capacity = peak_util = 0.0

        overview = GPUOverview(
            detected=bool(gpu_info),
            per_gpu=gpu_info,
            total_used_mb=total_used,
            total_capacity_mb=total_capacity,
            peak_util_percent=peak_util,
        )

        return metrics, overview

    def systemctl(self, key: str, action: str) -> bool:
        """Execute systemctl command with security validation.

        SECURITY: Validates service name and action against allowlists to prevent
        command injection attacks. Only allows whitelisted services and actions.

        Args:
            key: Provider key (must be in ALLOWED_SERVICES)
            action: Action to execute (must be in ALLOWED_ACTIONS)

        Returns:
            True if command succeeded, False otherwise
        """
        # SECURITY: Validate service key
        if key not in ALLOWED_SERVICES:
            logger.warning(f"Rejected invalid service key: {key}")
            return False

        # SECURITY: Validate action
        if action not in ALLOWED_ACTIONS:
            logger.warning(f"Rejected invalid action: {action}")
            return False

        service = self._service_name(key)
        if not service:
            return False

        try:
            subprocess.run(
                ["systemctl", "--user", action, service],
                check=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                timeout=8,
                # SECURITY: Minimal environment to prevent injection
                env={"PATH": "/usr/bin:/bin"},
            )
        except subprocess.SubprocessError as e:
            logger.debug(f"systemctl {action} {key} failed: {type(e).__name__}")
            return False

        return True


# ---------------------------------------------------------------------------
# UI components


class OverviewPanel(Static):
    """Displays condensed summary of all services.

    Shows count of active/inactive services, average CPU/memory usage,
    and lists any services requiring attention (degraded or inactive).
    """

    def update_overview(self, metrics: Iterable[ServiceMetrics]) -> None:
        """Update overview with current metrics.

        Args:
            metrics: Iterable of ServiceMetrics to summarize
        """
        metrics = list(metrics)
        total = len(metrics)

        if total == 0:
            self.update("No providers registered.")
            return

        active = [m for m in metrics if m.status == "active"]
        degraded = [m for m in metrics if m.status == "degraded"]
        inactive = [m for m in metrics if m.status == "inactive"]

        avg_cpu = sum(m.cpu_percent for m in metrics) / total
        avg_mem = sum(m.memory_percent for m in metrics) / total
        latencies = [m.response_ms for m in metrics if m.response_ms > 0]
        avg_latency = sum(latencies) / len(latencies) if latencies else 0.0

        summary_line = (
            f"[#00ff00 bold]{len(active)} ACTIVE[/] · "          # Neon green
            f"[#ff6600 bold]{len(degraded)} DEGRADED[/] · "      # Neon orange
            f"[#ff0044 bold]{len(inactive)} INACTIVE[/] · "      # Neon red
            f"Avg latency [#00ffff]{avg_latency:.0f} ms[/]"      # Neon cyan
        )

        resources_line = (
            f"[b]Avg CPU[/] [#00ffff]{avg_cpu:.1f}%[/] · "       # Neon cyan
            f"[b]Avg memory[/] [#00ffff]{avg_mem:.1f}%[/]"       # Neon cyan
        )

        if degraded or inactive:
            impacted = ", ".join(m.display for m in degraded + inactive)
            attention_line = f"[b]Needs attention:[/] [#ff00ff]{impacted}[/]"  # Neon magenta
        else:
            attention_line = "[#00ff00 bold]All providers healthy.[/]"         # Neon green

        self.update("\n".join([summary_line, resources_line, attention_line]))


class ServiceTable(DataTable):
    """Tabular view displaying all provider services.

    Shows provider name, status, CPU/memory usage, response time, and model count.
    Supports row selection for viewing detailed metrics in the detail panel.
    """

    def __init__(self) -> None:
        """Initialize service table with columns."""
        super().__init__(zebra_stripes=True)
        self.cursor_type = "row"
        self.show_header = True
        self.add_columns(
            "Provider",
            "Status",
            "CPU",
            "Mem",
            "VRAM",
            "Resp",
            "Models",
        )

    def populate(self, metrics: Iterable[ServiceMetrics], selected: str | None) -> None:
        """Populate table with service metrics and restore selection.

        Args:
            metrics: Iterable of ServiceMetrics to display
            selected: Provider key to select, if available
        """
        self.clear()
        metrics = list(metrics)
        selected_index: int | None = None

        def colour_band(value: float, warning: float, danger: float) -> str:
            if value >= danger:
                return "#ff0044"  # Neon red for danger
            if value >= warning:
                return "#ff6600"  # Neon orange for warning
            return "#00ff00"      # Neon green for good

        for index, metric in enumerate(metrics):
            status_markup = {
                "active": "[#00ff00 bold]ACTIVE[/]",      # Neon green
                "degraded": "[#ff6600 bold]DEGRADED[/]",  # Neon orange
                "inactive": "[#ff0044 bold]INACTIVE[/]",  # Neon red
            }.get(metric.status, metric.status.title())

            cpu_colour = colour_band(metric.cpu_percent, warning=70, danger=85)
            mem_colour = colour_band(metric.memory_percent, warning=65, danger=80)

            cpu_text = f"[{cpu_colour}]{metric.cpu_percent:.1f}%[/]"
            mem_text = f"{metric.memory_mb:.0f}MB ([{mem_colour}]{metric.memory_percent:.1f}%[/])"

            if metric.vram_mb:
                vram_colour = colour_band(metric.vram_percent, warning=70, danger=90)
                vram_text = f"[{vram_colour}]{metric.vram_mb:.0f}MB ({metric.vram_percent:.0f}%)[/]"
            else:
                vram_text = "[dim]-[/]"

            latency_colour = colour_band(metric.response_ms, warning=800, danger=1500)
            resp_text = f"[{latency_colour}]{metric.response_ms:.0f}ms[/]"

            self.add_row(
                metric.display,
                status_markup,
                cpu_text,
                mem_text,
                vram_text,
                resp_text,
                str(metric.models),
                key=metric.key,
            )

            if selected and metric.key == selected:
                selected_index = index

        if selected_index is not None:
            self.cursor_coordinate = Coordinate(row=selected_index, column=0)
        elif self.row_count:
            self.cursor_coordinate = Coordinate(row=0, column=0)


class GPUCard(Static):
    """Displays GPU utilization summary.

    Shows total VRAM usage, peak utilization percentage, and per-GPU breakdown.
    Gracefully handles systems without NVIDIA GPUs.
    """

    def update_overview(self, overview: GPUOverview) -> None:
        """Update GPU card with latest GPU metrics.

        Args:
            overview: GPUOverview with aggregated GPU statistics
        """
        if not overview.detected:
            self.update("[dim]GPU metrics unavailable[/dim]")
            return

        utilisation = (
            (overview.total_used_mb / overview.total_capacity_mb * 100)
            if overview.total_capacity_mb
            else 0.0
        )

        lines = [
            f"[b]Total usage:[/] [#00ff00]{overview.total_used_mb:.0f}/{overview.total_capacity_mb:.0f}MB ({utilisation:.1f}%)[/]",  # Neon green
            f"[b]Peak activity:[/] [#ff6600]{overview.peak_util_percent:.1f}%[/]",  # Neon orange
        ]

        for entry in overview.per_gpu:
            lines.append(
                f"GPU {int(entry['id'])}: [#00ff00]{entry['memory_used_mb']:.0f}/{entry['memory_total_mb']:.0f}MB[/] "  # Neon green
                f"([#00ffff]{entry['memory_util_percent']:.1f}%[/]) · "  # Neon cyan
                f"load [#ff00ff]{entry['gpu_util_percent']:.0f}%[/]"     # Neon magenta
            )

        self.update("\n".join(lines))


class DetailPanel(Vertical):
    """Shows focused service details and exposes control buttons.

    Displays comprehensive metrics for selected service including status,
    CPU/memory/VRAM usage, port, model count, and PID. Provides buttons
    to start/stop/restart/enable/disable the service.
    """

    class ServiceAction(Message):
        """Message posted when user clicks a service control button.

        Attributes:
            action: Control action (start, stop, restart, enable, disable)
            service_key: Provider key targeted by the action
        """

        def __init__(self, action: str, service_key: str) -> None:
            super().__init__()
            self.action = action
            self.service_key = service_key

    def __init__(self) -> None:
        super().__init__(id="detail-panel")
        self._current: ServiceMetrics | None = None

    def compose(self) -> ComposeResult:
        yield Label("Select a provider to inspect", id="detail-title")
        yield Label(id="detail-status")
        yield Label(id="detail-resources")
        yield Label(id="detail-metadata")
        with Horizontal(id="detail-actions"):
            yield Button("Start", id="action-start", variant="success")
            yield Button("Stop", id="action-stop", variant="error")
            yield Button("Restart", id="action-restart", variant="warning")
            yield Button("Enable", id="action-enable", variant="primary")
            yield Button("Disable", id="action-disable", variant="primary")
        yield Log(id="detail-notes", highlight=True)

    def update_details(self, metrics: ServiceMetrics | None) -> None:
        """Update detail panel with metrics for selected service.

        Args:
            metrics: ServiceMetrics for the selected provider, or None to clear.
        """
        self._current = metrics

        try:
            title = self.query_one("#detail-title", Label)
            status_label = self.query_one("#detail-status", Label)
            resources_label = self.query_one("#detail-resources", Label)
            metadata_label = self.query_one("#detail-metadata", Label)
            notes_log = self.query_one(Log)
        except Exception as e:
            logger.warning(f"Failed to query detail panel widgets: {type(e).__name__}: {e}")
            return

        if not metrics:
            try:
                title.update("Select a provider to inspect")
                status_label.update("")
                resources_label.update("")
                metadata_label.update("")
                notes_log.clear()
            except Exception as e:
                logger.debug(f"Error clearing detail panel: {e}")
            return

        try:
            status_colour = {
                "active": "#00ff00",    # Neon green
                "degraded": "#ff6600",  # Neon orange
                "inactive": "#ff0044",  # Neon red
            }.get(metrics.status, "#ffffff")

            title.update(f"[b][#ffd700]{metrics.display}[/b] [#00ffff]({'required' if metrics.required else 'optional'})[/]")  # Gold title, cyan text
            status_label.update(
                f"[b]Status:[/] [{status_colour} bold]{metrics.status.upper()}[/] · Response [#00ffff]{metrics.response_ms:.0f} ms[/]"  # Cyan text
            )
            resources_label.update(
                "[b]Resources:[/] CPU [#00ff00]{cpu:.1f}%[/] · Memory [#ff00ff]{mem:.1f} MB[/] ([#00ffff]{mem_pct:.1f}%[/]) · VRAM {vram}".format(
                    cpu=metrics.cpu_percent,
                    mem=metrics.memory_mb,
                    mem_pct=metrics.memory_percent,
                    vram="-"
                    if not metrics.vram_mb
                    else f"[#ff00ff]{metrics.vram_mb:.1f} MB[/] ([#00ffff]{metrics.vram_percent:.1f}%[/])",  # Magenta and cyan
                )
            )
            metadata_label.update(
                "[b]Port [#00ffff]{port}[/b] · Models [#ff00ff]{models}[/] · PID [#00ffff]{pid}[/]".format(
                    port=metrics.port or "n/a",
                    models=metrics.models,
                    pid=metrics.pid or "n/a",
                )
            )

            notes_log.clear()
            if metrics.notes:
                for line in metrics.notes:
                    # Color code notes based on content
                    if "timeout" in line.lower() or "error" in line.lower():
                        notes_log.write(f"[#ff0044]{line}[/]")  # Neon red for errors
                    elif "warning" in line.lower():
                        notes_log.write(f"[#ff6600]{line}[/]")  # Neon orange for warnings
                    else:
                        notes_log.write(f"[#00ffff]{line}[/]")  # Neon cyan for info
            else:
                notes_log.write("[#00ff00]No warnings recorded.[/]")  # Neon green
        except Exception as e:
            logger.warning(f"Error updating detail panel: {type(e).__name__}: {e}")

    @on(Button.Pressed)
    def handle_button(self, event: Button.Pressed) -> None:
        if not self._current:
            return
        action = event.button.id or ""
        if not action.startswith("action-"):
            return
        self.post_message(self.ServiceAction(action.replace("action-", ""), self._current.key))


# ---------------------------------------------------------------------------
# Main dashboard application


class DashboardApp(App[None]):
    """Interactive command center for the AI backend.

    Provides real-time monitoring and management of LLM provider services
    including Ollama, vLLM, and llama.cpp. Displays system metrics, GPU
    usage, and provider status with controls for service lifecycle management.

    Attributes:
        monitor: ProviderMonitor instance for collecting diagnostics
        metrics: Current service metrics snapshot
        gpu_overview: Current GPU utilization snapshot
        selected_key: Currently selected provider for detailed view
    """

    CSS = """
    /* Enhanced Neon Cyberpunk grid layout for dashboard */

    Screen {
        background: #0a0e27;  /* Deep blue-black background */
        color: #ffffff;       /* Bright white text */
    }

    Header {
        background: #1a1a2e;     /* Dark panel background */
        color: #00ffff;          /* Neon cyan */
        text-style: bold;
        height: 3;
    }

    Footer {
        background: #1a1a2e;     /* Dark panel background */
        color: #00ffff;          /* Neon cyan */
        height: 3;
    }

    Container#body {
        layout: vertical;
        height: 1fr;
        padding: 1 2;
    }

    Horizontal#summary-strip {
        layout: horizontal;
        height: 8;               /* Increased height for better visibility */
        margin-bottom: 1;
    }

    Container#main-row {
        layout: horizontal;
        height: 1fr;
        margin-bottom: 1;
    }

    ServiceTable {
        border: heavy #ff00ff;   /* Neon magenta border */
        background: #1a1a2e;     /* Dark panel background */
        color: #00ffff;          /* Neon cyan text */
        height: 1fr;
        width: 7fr;
        margin-right: 1;
    }

    Vertical#detail-column {
        height: 1fr;
        layout: vertical;
        width: 3fr;
        margin-left: 1;
        padding: 0 1;
    }

    Container#log-wrapper {
        height: 10;              /* Increased height for better log visibility */
        margin-top: 1;
        padding: 0 1;
    }

    OverviewPanel {
        width: 3fr;
        border: heavy #00ffff;   /* Neon cyan border */
        background: #1a1a2e;     /* Dark panel background */
        color: #ffffff;          /* White text */
        padding: 1 2;
        min-height: 6;
        margin-right: 1;
    }

    GPUCard {
        width: 2fr;
        border: heavy #00ff00;   /* Neon green border */
        background: #1a1a2e;     /* Dark panel background */
        color: #ffffff;          /* White text */
        padding: 1 2;
        min-height: 6;
        margin-left: 1;
    }

    ServiceTable > .datatable--header {
        background: #2a2a4e;     /* Highlight background */
        color: #ffd700;          /* Gold text */
        text-style: bold;
    }

    ServiceTable > .datatable--cursor {
        background: #2a2a4e;     /* Highlight background */
        color: #00ff00;          /* Neon green text */
        text-style: bold;
    }

    ServiceTable > .datatable--odd-row {
        background: #1a1a2e;     /* Panel background */
    }

    ServiceTable > .datatable--even-row {
        background: #151530;     /* Subtle background */
    }

    #detail-panel {
        border: heavy #ffd700;   /* Gold border */
        background: #1a1a2e;     /* Dark panel background */
        color: #ffffff;          /* White text */
        padding: 1 2;
        height: 1fr;
        min-height: 14;          /* Increased height */
    }

    #detail-title {
        color: #ffd700;          /* Gold text */
        text-style: bold;
        margin-bottom: 1;
    }

    #detail-status {
        color: #00ffff;          /* Cyan text */
        margin-bottom: 1;
    }

    #detail-resources {
        color: #ff00ff;          /* Magenta text */
        margin-bottom: 1;
    }

    #detail-metadata {
        color: #00ffff;          /* Cyan text */
        margin-bottom: 1;
    }

    #detail-actions {
        padding: 1 0;
        width: 100%;
        margin-top: 1;
        margin-bottom: 1;
    }

    #detail-actions Button {
        margin-right: 1;
        width: 12;               /* Fixed width for consistent buttons */
    }

    #detail-notes {
        min-height: 6;
        border: heavy #bf00ff;   /* Purple border */
        background: #0f0f1f;     /* Dark background */
        color: #c7a2ff;          /* Light purple text */
        padding: 1;
    }

    Button {
        background: #1a1a2e;     /* Dark panel background */
        border: solid #00ffff;   /* Neon cyan border */
        color: #00ffff;          /* Neon cyan text */
        text-style: bold;
        height: 3;
    }

    Button:hover {
        background: #2a2a4e;     /* Highlight background */
        border: heavy #00ffff;   /* Neon cyan border */
        color: #7dffff;          /* Lighter cyan text */
    }

    Button.-success {
        border: solid #00ff00;   /* Neon green border */
        color: #00ff00;          /* Neon green text */
        background: #002200;     /* Dark green background */
    }

    Button.-success:hover {
        background: #003300;     /* Darker green background */
        border: heavy #00ff00;   /* Neon green border */
        color: #39ff14;          /* Bright green text */
    }

    Button.-error {
        border: solid #ff0044;   /* Neon red border */
        color: #ff0044;          /* Neon red text */
        background: #220000;     /* Dark red background */
    }

    Button.-error:hover {
        background: #330000;     /* Darker red background */
        border: heavy #ff0044;   /* Neon red border */
        color: #ff2266;          /* Lighter red text */
    }

    Button.-warning {
        border: solid #ff6600;   /* Neon orange border */
        color: #ff6600;          /* Neon orange text */
        background: #221100;     /* Dark orange background */
    }

    Button.-warning:hover {
        background: #331a00;     /* Darker orange background */
        border: heavy #ff6600;   /* Neon orange border */
        color: #ff8844;          /* Lighter orange text */
    }

    Button.-primary {
        border: solid #ff00ff;   /* Neon magenta border */
        color: #ff00ff;          /* Neon magenta text */
        background: #220022;     /* Dark magenta background */
    }

    Button.-primary:hover {
        background: #330033;     /* Darker magenta background */
        border: heavy #ff00ff;   /* Neon magenta border */
        color: #ff44ff;          /* Lighter magenta text */
    }

    Log {
        border: heavy #00ffff;   /* Neon cyan border */
        background: #0f0f1f;     /* Dark background */
        color: #7dffff;          /* Light cyan text */
        padding: 1 2;
        height: 10;
    }

    #event-log {
        color: #7dffff;          /* Light cyan text */
        padding: 0 1;
    }

    ScrollBar {
        background: #1a1a2e;     /* Dark panel background */
        border: solid #00ffff;   /* Neon cyan border */
    }

    ScrollBar > .scrollbar--thumb {
        background: #ff00ff;     /* Neon magenta */
    }

    ScrollBar > .scrollbar--thumb:hover {
        background: #ffff00;     /* Neon yellow */
    }

    /* Additional flashy elements for enhanced visual impact */
    Header Clock {
        color: #ff00ff;          /* Neon magenta for clock */
        text-style: bold;
    }

    /* Focus effects using border styles instead of box-shadow */
    ServiceTable:focus-within {
        border: heavy #ff00ff;
    }

    #detail-panel:focus-within {
        border: heavy #ffd700;
    }
    """
    BINDINGS = [
        Binding("r", "refresh", "Refresh"),
        Binding("q", "quit", "Quit"),
        Binding("a", "toggle_auto", "Toggle auto-refresh"),
        Binding("ctrl+l", "clear_log", "Clear log"),
    ]

    auto_refresh = reactive(True)

    def __init__(self) -> None:
        super().__init__()
        self.monitor = ProviderMonitor()
        self.metrics: list[ServiceMetrics] = []
        self.gpu_overview = GPUOverview(False, [], 0.0, 0.0, 0.0)
        self.selected_key: str | None = None
        self.refresh_timer = None

        # Try to load previous state
        loaded_state = load_dashboard_state()
        if loaded_state:
            self.metrics, self.selected_key = loaded_state
            logger.info(f"Restored dashboard state with {len(self.metrics)} providers")

    # ------------------------------ layout ---------------------------------

    def compose(self) -> ComposeResult:
        yield Header(show_clock=True)
        with Container(id="body"):
            with Horizontal(id="summary-strip"):
                yield OverviewPanel(id="overview")
                yield GPUCard(id="gpu")
            with Container(id="main-row"):
                yield ServiceTable()
                with Vertical(id="detail-column"):
                    yield DetailPanel()
            with Container(id="log-wrapper"):
                yield Log(id="event-log", highlight=True)
        yield Footer()

    # ----------------------------- lifecycle --------------------------------

    def on_mount(self) -> None:
        """Initialize dashboard on app mount.

        Sets up auto-refresh timer and performs initial data collection.
        """
        logger.info("Dashboard mounted - starting initial metrics collection")
        self._refresh_table()
        self.refresh_timer = self.set_interval(
            DEFAULT_REFRESH_INTERVAL, self._refresh_table, pause=not self.auto_refresh
        )
        self.log_event(f"[#00ff00 bold]✓ Dashboard initialized[/] ([#00ffff]refresh: {DEFAULT_REFRESH_INTERVAL}s[/])")  # Neon green and cyan

    def action_quit(self) -> None:
        """Quit the application and save state."""
        logger.info("Dashboard shutting down - saving state")
        save_dashboard_state(self.metrics, self.selected_key)
        super().action_quit()

    def action_toggle_auto(self) -> None:
        """Toggle auto-refresh on/off (binding: 'a')."""
        self.auto_refresh = not self.auto_refresh
        if self.auto_refresh:
            self.refresh_timer.resume()
            self.log_event(f"[#00ff00]Auto-refresh enabled[/] ([#00ffff]{DEFAULT_REFRESH_INTERVAL}s cadence[/]).")  # Neon green and cyan
        else:
            self.refresh_timer.pause()
            self.log_event("[#ff6600]Auto-refresh paused.[/]")  # Neon orange

    def action_refresh(self) -> None:
        """Manually refresh all metrics (binding: 'r')."""
        self._refresh_table()
        self.log_event("[#00ff00]Manual refresh completed.[/]")  # Neon green

    def action_clear_log(self) -> None:
        """Clear event log (binding: 'ctrl+l')."""
        try:
            self.query_one("#event-log", Log).clear()
        except Exception as e:
            logger.debug(f"Error clearing log: {e}")

    # ---------------------------- refresh engine ----------------------------

    def _refresh_table(self) -> None:
        """Refresh all dashboard displays with latest provider metrics."""
        try:
            self.metrics, self.gpu_overview = self.monitor.collect_snapshot()
            active_count = sum(1 for m in self.metrics if m.status == "active")
            logger.debug(f"Snapshot collected: {active_count}/{len(self.metrics)} active")
        except Exception as e:
            error_msg = f"{type(e).__name__}"
            logger.error(f"Error collecting provider snapshot: {error_msg}: {e}")
            self.log_event(f"[#ff0044 bold]✗ Snapshot failed:[/] [#ff6600]{error_msg}[/]")  # Neon red and orange
            return

        try:
            table = self.query_one(ServiceTable)
            table.populate(self.metrics, self.selected_key)

            self.query_one(OverviewPanel).update_overview(self.metrics)
            self.query_one(GPUCard).update_overview(self.gpu_overview)

            detail = self.query_one(DetailPanel)
            detail.update_details(self._find_metric(self.selected_key))
        except Exception as e:
            error_msg = f"{type(e).__name__}"
            logger.error(f"Error updating dashboard displays: {error_msg}: {e}")
            self.log_event(f"[#ff0044 bold]✗ Display update failed:[/] [#ff6600]{error_msg}[/]")  # Neon red and orange

    def _find_metric(self, key: str | None) -> ServiceMetrics | None:
        """Find ServiceMetrics by provider key.

        Args:
            key: Provider key to search for

        Returns:
            ServiceMetrics if found, None otherwise
        """
        if not key:
            return None
        for metric in self.metrics:
            if metric.key == key:
                return metric
        return None

    # ------------------------------- helpers --------------------------------

    def log_event(self, message: str) -> None:
        """Log an event message to the event log widget.

        Args:
            message: Message to display (supports Rich markup)
        """
        try:
            self.query_one("#event-log", Log).write(message)
        except Exception as e:
            logger.debug(f"Error logging event: {e}")

    # --------------------------- event handlers -----------------------------

    @on(DataTable.RowSelected)
    def handle_row_selected(self, event: DataTable.RowSelected) -> None:
        """Handle service table row selection.

        Updates detail panel to show metrics for selected provider.

        Args:
            event: DataTable row selection event
        """
        row_key = getattr(event.row_key, "value", event.row_key)
        key = str(row_key)
        self.selected_key = key
        self.query_one(DetailPanel).update_details(self._find_metric(key))

    @on(DetailPanel.ServiceAction)
    def handle_service_action(self, event: DetailPanel.ServiceAction) -> None:
        """Handle service control action (start/stop/restart/enable/disable).

        Executes the requested systemctl action and refreshes metrics.

        Args:
            event: ServiceAction event with action and service key
        """
        logger.info(f"Service action requested: {event.action} on {event.service_key}")
        success = self.monitor.systemctl(event.service_key, event.action)
        if success:
            msg = f"[#00ff00 bold]✓[/] [#00ffff]{event.action.title()} sent to {event.service_key}[/]"  # Neon green and cyan
            self.log_event(msg)
            logger.info(f"Service action succeeded: {event.action} on {event.service_key}")
            # Refresh immediately to reflect new status
            self.set_timer(1.0, self._refresh_table)
        else:
            msg = f"[#ff0044 bold]✗ Failed[/] [#ff6600]to {event.action} {event.service_key}[/]"  # Neon red and orange
            self.log_event(msg)
            logger.warning(f"Service action failed: {event.action} on {event.service_key}")


if __name__ == "__main__":
    DashboardApp().run()
