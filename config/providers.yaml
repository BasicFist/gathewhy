# AI Backend Provider Registry
# Master catalog of all LLM inference providers in LAB ecosystem

providers:
  # ============================================================================
  # ACTIVE PROVIDERS
  # ============================================================================

  ollama:
    type: ollama
    base_url: http://127.0.0.1:11434
    status: active
    description: Local Ollama server for general-purpose models
    features:
      - Easy model management (ollama pull/list)
      - CPU and GPU support
      - QuantizedGGUF format
      - Fast model switching
    models:
      - name: llama3.1:latest
        size: "8B"
        quantization: Q4_K_M
        pulled_at: "2025-10-19"
        specialty: general_chat
      - name: qwen2.5-coder:7b
        size: "7.6B"
        quantization: Q4_K_M
        pulled_at: "2025-10-19"
        specialty: code_generation
      - name: mythomax-l2-13b-q5_k_m
        size: "13B"
        quantization: Q5_K_M
        pulled_at: "2025-10-24"
        specialty: creative_writing
        options:
          num_gpu_layers: -1
    health_endpoint: /api/tags
    docs: https://ollama.ai/docs
    location: ../openwebui (integrated)

  llama_cpp_python:
    type: llama_cpp
    base_url: http://127.0.0.1:8000
    status: active
    description: llama.cpp Python bindings server with CUDA support
    features:
      - GPU offloading (full or partial)
      - Context window up to 8192 tokens
      - Parallel requests (4 concurrent)
      - OpenAI-compatible API
    configuration:
      n_gpu_layers: -1  # Full GPU offload
      n_ctx: 8192
      n_parallel: 4
      threads: 6
    models:
      - name: llama-cpp-default
        description: "GGUF model via llama.cpp Python bindings"
        context_length: 8192
        tags:
          - local
          - gguf
          - cuda_optimized
    health_endpoint: /v1/models
    service_file: ~/.config/systemd/user/llamacpp-python.service
    location: ../openwebui/backends/llama.cpp

  llama_cpp_native:
    type: llama_cpp
    base_url: http://127.0.0.1:8080
    status: active
    description: llama.cpp native C++ server (maximum performance)
    features:
      - Pure C/C++ implementation
      - No Python overhead
      - Maximum throughput
      - CUDA-optimized
    models:
      - name: llama-cpp-native
        description: "GGUF model via llama.cpp native C++ server"
        context_length: 8192
        tags:
          - local
          - gguf
          - native
          - fastest
    health_endpoint: /v1/models
    location: ../openwebui/backends/llama.cpp

  vllm-qwen:
    type: vllm
    base_url: http://127.0.0.1:8001
    status: active
    description: vLLM high-throughput inference with AWQ quantization (Qwen model)
    features:
      - Continuous batching
      - PagedAttention for KV cache
      - AWQ quantization (4-bit)
      - Optimized for code generation
      - Low memory footprint
    models:
      - name: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
        size: "7B"
        quantization: AWQ
        context_length: 4096
        specialty: code_generation
        model_size_gb: 5.2
        kv_cache_available_gb: 7.36
    health_endpoint: /v1/models
    api_format: openai_compatible
    gpu_memory_utilization: 0.45
    max_model_len: 4096
    location: ~/venvs/vllm
    hardware_requirements:
      gpu_vram_gb: 16
      gpu_model: "Quadro RTX 5000"
      compute_capability: "7.5"

  vllm-dolphin:
    type: vllm
    base_url: http://127.0.0.1:8002
    status: disabled  # vLLM runs single instance - use vllm-model-switch.sh to enable
    description: vLLM high-throughput inference with AWQ quantization (Dolphin model)
    features:
      - Continuous batching
      - PagedAttention for KV cache
      - AWQ quantization (4-bit)
      - Optimized for conversational responses
      - Low memory footprint
    models:
      - name: solidrust/dolphin-2.8-mistral-7b-v02-AWQ
        size: "7B"
        quantization: AWQ
        context_length: 4096
        specialty: conversational
    health_endpoint: /v1/models
    api_format: openai_compatible
    gpu_memory_utilization: 0.45
    max_model_len: 4096
    location: ~/venvs/vllm
    hardware_requirements:
      gpu_vram_gb: 16
      gpu_model: "Quadro RTX 5000"
      compute_capability: "7.5"

  ollama_cloud:
    type: ollama
    base_url: https://ollama.com
    status: active
    description: Ollama Cloud API for large models without local GPU
    requires_api_key: true
    env_var: OLLAMA_API_KEY
    features:
      - Cloud-hosted large models (no local GPU required)
      - Data-center-grade hardware
      - Same API as local Ollama
      - Automatic offloading for cloud models
    models:
      - name: deepseek-v3.1:671b-cloud
        size: "671B"
        specialty: advanced_reasoning
      - name: qwen3-coder:480b-cloud
        size: "480B"
        specialty: code_generation
      - name: kimi-k2:1t-cloud
        size: "1T"
        specialty: advanced_reasoning
      - name: gpt-oss:120b-cloud
        size: "120B"
        specialty: general_chat
      - name: gpt-oss:20b-cloud
        size: "20B"
        specialty: general_chat
      - name: glm-4.6:cloud
        size: "4.6B"
        specialty: general_chat
    health_endpoint: /api/tags
    docs: https://docs.ollama.com/cloud
    rate_limits:
      hourly: true
      daily: true
      details: "Free preview with generous limits"

  # ============================================================================
  # TEMPLATE PROVIDERS (Disabled, for reference)
  # ============================================================================

  openai:
    type: openai
    base_url: https://api.openai.com/v1
    status: disabled
    description: OpenAI cloud API (requires API key)
    requires_api_key: true
    env_var: OPENAI_API_KEY
    models:
      - gpt-4
      - gpt-3.5-turbo
    rate_limits:
      requests_per_minute: 3500
      tokens_per_minute: 90000
    cost_per_1k_tokens:
      gpt-4: 0.03
      gpt-3.5-turbo: 0.0015

  anthropic:
    type: anthropic
    base_url: https://api.anthropic.com/v1
    status: disabled
    description: Anthropic Claude API (requires API key)
    requires_api_key: true
    env_var: ANTHROPIC_API_KEY
    models:
      - claude-3-opus-20240229
      - claude-3-sonnet-20240229

  custom_openai_compatible:
    type: openai_compatible
    base_url: http://localhost:CUSTOM_PORT
    status: template
    description: Template for any OpenAI-compatible API server
    notes: |
      Use this template to add:
      - LocalAI servers
      - Text Generation WebUI
      - FastChat servers
      - Any server implementing OpenAI API spec

# ============================================================================
# PROVIDER METADATA
# ============================================================================

metadata:
  version: "1.4"
  last_updated: "2025-10-30"
  total_providers: 5  # ollama, llama_cpp_python, llama_cpp_native, vllm-qwen (active), ollama_cloud
  total_models_available: 11  # 3 Ollama + 1 vLLM + 1 llama.cpp + 6 Ollama Cloud
  notes: "vllm-dolphin disabled - vLLM runs single instance on port 8001"

  provider_types:
    - ollama: Simple local server
    - ollama_cloud: Managed cloud API (Ollama)
    - llama_cpp: High-performance C++ inference
    - vllm: Production-grade batched inference
    - openai: Cloud API providers
    - openai_compatible: Generic OpenAI-compatible servers

  selection_criteria:
    ollama:
      best_for: Quick prototyping, easy model management
      performance: Medium
      resource_usage: Low to Medium

    llama_cpp:
      best_for: Maximum single-request performance
      performance: High
      resource_usage: Medium

    vllm:
      best_for: High concurrency, production workloads
      performance: Very High (batched)
      resource_usage: High (requires GPU)

    ollama_cloud:
      best_for: Massive models without local GPU
      performance: Data-center grade
      resource_usage: Remote (billable)

# ============================================================================
# HEALTH CHECK CONFIGURATION
# ============================================================================

health_checks:
  enabled: true
  interval_seconds: 60
  timeout_seconds: 5
  retry_attempts: 3

  endpoints:
    ollama: http://127.0.0.1:11434/api/tags
    llama_cpp_python: http://127.0.0.1:8000/v1/models
    llama_cpp_native: http://127.0.0.1:8080/v1/models
    vllm-qwen: http://127.0.0.1:8001/v1/models
    vllm-dolphin: http://127.0.0.1:8002/v1/models
