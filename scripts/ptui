#!/usr/bin/env bash
# Provider TUI (ptui) - AI Backend Unified Command Center
# Comprehensive management interface for LiteLLM, vLLM, Ollama, and llama.cpp

set -euo pipefail

readonly VERSION="2.0.0"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly SCRIPT_DIR
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
readonly PROJECT_ROOT
INVOCATION_NAME="$(basename "$0")"
readonly INVOCATION_NAME
readonly DISPLAY_INVOCATION="${PTUI_CLI_NAME:-$INVOCATION_NAME}"

# Service endpoints
readonly LITELLM_URL="http://localhost:4000"
readonly OLLAMA_URL="http://localhost:11434"
readonly LLAMACPP_PYTHON_URL="http://localhost:8000"
readonly LLAMACPP_NATIVE_URL="http://localhost:8080"
readonly VLLM_URL="http://localhost:8001"
readonly DEFAULT_HTTP_TIMEOUT="${PTUI_HTTP_TIMEOUT:-5}"
readonly AUTO_REFRESH_SECONDS="${PTUI_REFRESH_SECONDS:-5}"
readonly PYTHON_BIN="${PYTHON:-python3}"

readonly -a SERVICE_NAMES=(
    "LiteLLM Gateway"
    "Ollama"
    "llama.cpp (Python)"
    "llama.cpp (Native)"
    "vLLM"
)
readonly -a SERVICE_URLS=(
    "$LITELLM_URL"
    "$OLLAMA_URL"
    "$LLAMACPP_PYTHON_URL"
    "$LLAMACPP_NATIVE_URL"
    "$VLLM_URL"
)
readonly -a SERVICE_ENDPOINTS=(
    "/health"
    "/api/tags"
    "/v1/models"
    "/v1/models"
    "/v1/models"
)
readonly -a SERVICE_REQUIRED=(
    "required"
    "optional"
    "optional"
    "optional"
    "optional"
)

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
WHITE='\033[1;37m'
GRAY='\033[0;90m'
NC='\033[0m' # No Color
BOLD='\033[1m'

# Icons
ICON_RUNNING="‚úÖ"
ICON_STOPPED="‚ùå"
ICON_WARNING="‚ö†Ô∏è "
ICON_INFO="‚ÑπÔ∏è "
ICON_ROCKET="üöÄ"
ICON_GEAR="‚öôÔ∏è "
ICON_CHART="üìä"
ICON_SERVER="üñ•Ô∏è "
ICON_MODEL="ü§ñ"
ICON_HEALTH="üíö"

# Systemd service names
readonly LITELLM_UNIT="litellm.service"
readonly OLLAMA_UNIT="ollama.service"
readonly VLLM_UNIT="vllm.service"
readonly VLLM_DOLPHIN_UNIT="vllm-dolphin.service"
readonly LLAMACPP_UNIT="llama-cpp-native.service"

# Logging
log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[‚úì]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[‚ö†]${NC} $1"; }
log_error() { echo -e "${RED}[‚úó]${NC} $1"; }
log_header() { echo -e "${BOLD}${CYAN}$1${NC}"; }

get_unit_status() {
    local unit=$1
    if systemctl --user is-active --quiet "$unit"; then
        echo "running"
    else
        echo "stopped"
    fi
}

start_unit() {
    local unit=$1
    if systemctl --user start "$unit"; then
        log_success "Started $unit"
    else
        log_error "Failed to start $unit"
    fi
}

stop_unit() {
    local unit=$1
    if systemctl --user stop "$unit"; then
        log_success "Stopped $unit"
    else
        log_error "Failed to stop $unit"
    fi
}

pause() {
    read -rp "Press Enter to continue..." _
}

require_command() {
    local command_name=$1
    local hint=${2:-}

    if ! command -v "$command_name" >/dev/null 2>&1; then
        log_error "Required command not found: $command_name"
        [[ -n "$hint" ]] && echo -e "${YELLOW}${hint}${NC}"
        exit 1
    fi
}

ensure_dependencies() {
    require_command curl "Install curl to allow endpoint checks."
    require_command jq "Install jq (e.g. 'sudo apt install jq') for JSON parsing."
}

show_help() {
    cat <<EOF
PTUI - Provider TUI v$VERSION
AI Backend Unified Command Center

Primary commands: ptui (alias: pui)

Usage:
  $DISPLAY_INVOCATION              Interactive TUI mode
  $DISPLAY_INVOCATION status       Show service status
  $DISPLAY_INVOCATION models       List all models
  $DISPLAY_INVOCATION health       Run health check
  $DISPLAY_INVOCATION vllm <cmd>   vLLM management (status|qwen|dolphin|stop)
  $DISPLAY_INVOCATION test         Test endpoints interactively
  $DISPLAY_INVOCATION help         Show this help message
  $DISPLAY_INVOCATION version      Show current version

Interactive mode provides the full menu-driven interface described in docs/ptui-user-guide.md.
EOF
}

show_version() {
    echo "PTUI v$VERSION (command: $DISPLAY_INVOCATION)"
}

# Clear screen and show header
show_header() {
    clear 2>/dev/null || true
    echo -e "${BOLD}${CYAN}"
    cat << 'EOF'
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                  AI Backend Unified - PTUI                    ‚ïë
‚ïë              Provider Command & Control Center                ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF
    echo -e "${NC}"
    echo -e "${GRAY}ai-dashboard${NC}"
    echo
}

# Check if service is running
check_service_health() {
    local name=$1
    local url=$2
    local endpoint=${3:-"/health"}
    local timeout=${4:-$DEFAULT_HTTP_TIMEOUT}

    if curl -s -f --max-time "$timeout" "$url$endpoint" > /dev/null 2>&1; then
        echo -e "${GREEN}${ICON_RUNNING}${NC} ${BOLD}$name${NC} ${GRAY}($url)${NC}"
        return 0
    else
        echo -e "${RED}${ICON_STOPPED}${NC} ${BOLD}$name${NC} ${GRAY}($url)${NC}"
        return 1
    fi
}

# Get vLLM current model
get_vllm_model() {
    if pgrep -f "vllm serve" > /dev/null 2>&1; then
        local pid
        pid=$(pgrep -f "vllm serve")
        local model
        model=$(ps -p "$pid" -o args= | grep -oP '(?<=vllm serve )[^ ]+' || echo "unknown")
        echo "$model"
    else
        echo "not running"
    fi
}

# Service status overview
show_status() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Service Status ‚îÅ‚îÅ‚îÅ"
    echo
    local total_services=${#SERVICE_NAMES[@]}
    local running=0
    local -a service_statuses=()

    for idx in "${!SERVICE_NAMES[@]}"; do
        local name="${SERVICE_NAMES[$idx]}"
        local url="${SERVICE_URLS[$idx]}"
        local endpoint="${SERVICE_ENDPOINTS[$idx]}"

        service_statuses[$idx]=1
        if check_service_health "$name" "$url" "$endpoint"; then
            service_statuses[$idx]=0
            ((running++))

            if [[ "$name" == "vLLM" ]]; then
                local vllm_model
                vllm_model=$(get_vllm_model)
                echo -e "  ${GRAY}‚îî‚îÄ Model: ${WHITE}$vllm_model${NC}"
            fi
        fi
    done

    echo
    log_header "‚îÅ‚îÅ‚îÅ Quick Stats ‚îÅ‚îÅ‚îÅ"
    echo

    echo -e "${ICON_SERVER} Services Running: ${GREEN}${running}/${total_services}${NC}"

    # Show available models if LiteLLM is up
    if [[ ${service_statuses[0]:-1} -eq 0 ]]; then
        local model_count
        model_count=$(curl -s "$LITELLM_URL/v1/models" 2>/dev/null | jq -r '.data | length' 2>/dev/null || echo "0")
        echo -e "${ICON_MODEL} Models Available: ${GREEN}${model_count}${NC}"
    fi

    echo
    return 0
}

# List all available models
list_models() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Available Models ‚îÅ‚îÅ‚îÅ"
    echo

    if ! curl -s -f "$LITELLM_URL/v1/models" > /dev/null 2>&1; then
        log_error "LiteLLM Gateway is not running"
        return 1
    fi

    echo -e "${BOLD}LiteLLM Models:${NC}"
    curl -s "$LITELLM_URL/v1/models" | jq -r '.data[] | "  ‚Ä¢ \(.id)"' 2>/dev/null || echo "  (unable to fetch)"
    echo

    if curl -s -f "$OLLAMA_URL/api/tags" > /dev/null 2>&1; then
        echo -e "${BOLD}Ollama Models:${NC}"
        curl -s "$OLLAMA_URL/api/tags" | jq -r '.models[]? | "  ‚Ä¢ \(.name) (\(.size / 1024 / 1024 / 1024 | floor)GB)"' 2>/dev/null || echo "  (none)"
        echo
    fi

    if pgrep -f "vllm serve" > /dev/null 2>&1; then
        echo -e "${BOLD}vLLM Current Model:${NC}"
        local vllm_model
        vllm_model=$(get_vllm_model)
        echo -e "  ${GREEN}‚Ä¢${NC} $vllm_model"
        echo
    fi

    return 0
}

# Health check with details
health_check() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Comprehensive Health Check ‚îÅ‚îÅ‚îÅ"
    echo

    local all_healthy=true

    # Test each endpoint
    echo -e "${BOLD}Testing Endpoints:${NC}"

    for idx in "${!SERVICE_NAMES[@]}"; do
        local name="${SERVICE_NAMES[$idx]}"
        local url="${SERVICE_URLS[$idx]}"
        local endpoint="${SERVICE_ENDPOINTS[$idx]}"
        local required="${SERVICE_REQUIRED[$idx]}"
        local label="${name} endpoint"

        if [[ "$name" == "Ollama" || "$name" == "vLLM" || "$name" == "llama.cpp (Python)" || "$name" == "llama.cpp (Native)" ]]; then
            label="${name} API"
        elif [[ "$name" == "LiteLLM Gateway" ]]; then
            label="LiteLLM health endpoint"
        fi

        if curl -s -f -m "$DEFAULT_HTTP_TIMEOUT" "$url$endpoint" > /dev/null 2>&1; then
            log_success "$label"

            if [[ "$name" == "LiteLLM Gateway" ]]; then
                local test_model
                test_model=$(curl -s "$LITELLM_URL/v1/models" 2>/dev/null | jq -r '.data[0].id // empty' 2>/dev/null)
                if [[ -z "$test_model" ]]; then
                    test_model="llama3.1:latest"
                fi
                local test_response
                test_response=$(curl -s -X POST "$LITELLM_URL/v1/chat/completions" \
                    -H "Content-Type: application/json" \
                    -d "{\"model\": \"$test_model\", \"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}], \"max_tokens\": 5}" \
                    2>/dev/null | jq -r '.choices[0].message.content // "ERROR"' 2>/dev/null)

                if [[ "$test_response" != "ERROR" && -n "$test_response" ]]; then
                    log_success "LiteLLM completion test"
                else
                    log_warning "LiteLLM completion test failed"
                    all_healthy=false
                fi
            fi
        else
            if [[ "$required" == "required" ]]; then
                log_error "$label"
                all_healthy=false
            else
                log_warning "$label (optional)"
            fi
        fi
    done

    echo
    if $all_healthy; then
        echo -e "${GREEN}${ICON_HEALTH} System Status: HEALTHY${NC}"
    else
        echo -e "${YELLOW}${ICON_WARNING} System Status: DEGRADED${NC}"
    fi
    echo
    return 0
}

# vLLM model management
vllm_menu() {
    while true; do
        show_header
        log_header "‚îÅ‚îÅ‚îÅ vLLM Model Management ‚îÅ‚îÅ‚îÅ"
        echo

        local current_model
        current_model=$(get_vllm_model)
        echo -e "${BOLD}Current Model:${NC} ${CYAN}$current_model${NC}"
        echo

        echo "1. Switch to Qwen Coder (code generation)"
        echo "2. Switch to Dolphin (uncensored conversational)"
        echo "3. Check vLLM status"
        echo "4. Stop vLLM"
        echo "5. View vLLM logs"
        echo "b. Back to main menu"
        echo
        read -rp "Select option: " choice

        case $choice in
            1)
                log_info "Switching to Qwen Coder..."
                "$SCRIPT_DIR/vllm-model-switch.sh" qwen
                pause
                ;;
            2)
                log_info "Switching to Dolphin..."
                "$SCRIPT_DIR/vllm-model-switch.sh" dolphin
                pause
                ;;
            3)
                "$SCRIPT_DIR/vllm-model-switch.sh" status
                pause
                ;;
            4)
                "$SCRIPT_DIR/vllm-model-switch.sh" stop
                pause
                ;;
            5)
                echo
                log_info "Last 50 lines of vLLM logs:"
                echo
                if [[ -f /tmp/vllm-qwen-coder.log ]]; then
                    tail -50 /tmp/vllm-qwen-coder.log
                elif [[ -f /tmp/vllm-dolphin.log ]]; then
                    tail -50 /tmp/vllm-dolphin.log
                else
                    echo "No logs found"
                fi
                pause
                ;;
            b|B)
                break
                ;;
            *)
                log_error "Invalid option"
                sleep 1
                ;;
        esac
    done
}

# Configuration viewer
view_config() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Configuration Viewer ‚îÅ‚îÅ‚îÅ"
    echo

    echo "1. LiteLLM Unified Config"
    echo "2. Model Mappings"
    echo "3. Provider Registry"
    echo "4. vLLM Model Info"
    echo "b. Back"
    echo
    read -rp "Select config to view: " choice

    case $choice in
        1)
            if [[ -f "$PROJECT_ROOT/config/litellm-unified.yaml" ]]; then
                less "$PROJECT_ROOT/config/litellm-unified.yaml"
            else
                log_error "Config file not found"
            fi
            ;;
        2)
            if [[ -f "$PROJECT_ROOT/config/model-mappings.yaml" ]]; then
                less "$PROJECT_ROOT/config/model-mappings.yaml"
            else
                log_error "Config file not found"
            fi
            ;;
        3)
            if [[ -f "$PROJECT_ROOT/config/providers.yaml" ]]; then
                less "$PROJECT_ROOT/config/providers.yaml"
            else
                log_error "Config file not found"
            fi
            ;;
        4)
            show_header
            log_header "‚îÅ‚îÅ‚îÅ vLLM Model Information ‚îÅ‚îÅ‚îÅ"
            echo
            if pgrep -f "vllm serve" > /dev/null; then
                pgrep -a -f "vllm serve" || echo "No vLLM process details available"
            else
                echo "vLLM is not running"
            fi
            pause
            ;;
        b|B)
            return
            ;;
    esac
}

# Service logs
view_logs() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Service Logs ‚îÅ‚îÅ‚îÅ"
    echo

    echo "1. LiteLLM logs (journalctl)"
    echo "2. vLLM logs"
    echo "3. System logs (dmesg)"
    echo "b. Back"
    echo
    read -rp "Select logs to view: " choice

    case $choice in
        1)
            echo
            log_info "Last 100 lines of LiteLLM service:"
            echo
            journalctl --user -u litellm.service -n 100 --no-pager
            pause
            ;;
        2)
            echo
            log_info "vLLM logs:"
            echo
            if [[ -f /tmp/vllm-qwen-coder.log ]]; then
                tail -100 /tmp/vllm-qwen-coder.log
            elif [[ -f /tmp/vllm-dolphin.log ]]; then
                tail -100 /tmp/vllm-dolphin.log
            else
                echo "No vLLM logs found"
            fi
            pause
            ;;
        3)
            dmesg | tail -50
            pause
            ;;
        b|B)
            return
            ;;
    esac
}

# Test endpoints
test_endpoints() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Endpoint Testing ‚îÅ‚îÅ‚îÅ"
    echo

    echo "1. Test LiteLLM completion"
    echo "2. Test Ollama"
    echo "3. Test vLLM"
    echo "4. Test all endpoints"
    echo "b. Back"
    echo
    read -rp "Select test: " choice

    case $choice in
        1)
            echo
            log_info "Testing LiteLLM completion..."
            local test_model
            test_model=$(curl -s "$LITELLM_URL/v1/models" 2>/dev/null | jq -r '.data[0].id // empty' 2>/dev/null)
            if [[ -z "$test_model" ]]; then
                test_model="llama3.1:latest"
            fi
            curl -s -X POST "$LITELLM_URL/v1/chat/completions" \
                -H "Content-Type: application/json" \
                -d "{\"model\": \"$test_model\", \"messages\": [{\"role\": \"user\", \"content\": \"Say hello\"}], \"max_tokens\": 20}" \
                | jq
            pause
            ;;
        2)
            echo
            log_info "Testing Ollama..."
            curl -s "$OLLAMA_URL/api/tags" | jq
            pause
            ;;
        3)
            echo
            log_info "Testing vLLM..."
            curl -s "$VLLM_URL/v1/models" | jq
            pause
            ;;
        4)
            health_check
            pause
            ;;
        b|B)
            return
            ;;
    esac
}

# Quick actions
quick_actions() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Quick Actions ‚îÅ‚îÅ‚îÅ"
    echo

    echo "1. Restart LiteLLM service"
    echo "2. Run validation script"
    echo "3. Check GPU status"
    echo "4. Check port usage"
    echo "5. Kill all vLLM processes"
    echo "b. Back"
    echo
    read -rp "Select action: " choice

    case $choice in
        1)
            log_info "Restarting LiteLLM service..."
            systemctl --user restart litellm.service
            sleep 2
            systemctl --user status litellm.service --no-pager
            pause
            ;;
        2)
            if [[ -f "$PROJECT_ROOT/scripts/validate-unified-backend.sh" ]]; then
                "$PROJECT_ROOT/scripts/validate-unified-backend.sh"
            else
                log_error "Validation script not found"
            fi
            pause
            ;;
        3)
            nvidia-smi 2>/dev/null || log_warning "nvidia-smi not available"
            pause
            ;;
        4)
            echo
            log_info "Port usage:"
            echo
            lsof -i :4000 -i :8001 -i :11434 -i :8000 -i :8080 2>/dev/null || echo "No processes found"
            pause
            ;;
        5)
            log_warning "Killing all vLLM processes..."
            if pkill -f "vllm serve"; then
                log_success "vLLM processes killed"
            else
                log_info "No vLLM processes found"
            fi
            pause
            ;;
        b|B)
            return
            ;;
    esac
}

service_control() {
    while true; do
        show_header
        log_header "‚îÅ‚îÅ‚îÅ Service Control ‚îÅ‚îÅ‚îÅ"
        echo

        printf "  %-25s %s\n" "LiteLLM" "$(get_unit_status "$LITELLM_UNIT")"
        printf "  %-25s %s\n" "Ollama" "$(get_unit_status "$OLLAMA_UNIT")"
        printf "  %-25s %s\n" "vLLM" "$(get_unit_status "$VLLM_UNIT")"
        printf "  %-25s %s\n" "vLLM (Dolphin)" "$(get_unit_status "$VLLM_DOLPHIN_UNIT")"
        printf "  %-25s %s\n" "llama.cpp (Native)" "$(get_unit_status "$LLAMACPP_UNIT")"
        echo
        echo "  ${CYAN}1${NC}. Toggle LiteLLM"
        echo "  ${CYAN}2${NC}. Toggle Ollama"
        echo "  ${CYAN}3${NC}. Toggle vLLM"
        echo "  ${CYAN}4${NC}. Toggle vLLM (Dolphin)"
        echo "  ${CYAN}5${NC}. Toggle llama.cpp (Native)"
        echo "  ${CYAN}b${NC}. Back"
        echo
        read -rp "Select service: " choice

        case $choice in
            1)
                if [[ $(get_unit_status "$LITELLM_UNIT") == "running" ]]; then
                    stop_unit "$LITELLM_UNIT"
                else
                    start_unit "$LITELLM_UNIT"
                fi
                sleep 1
                ;;
            2)
                if [[ $(get_unit_status "$OLLAMA_UNIT") == "running" ]]; then
                    stop_unit "$OLLAMA_UNIT"
                else
                    start_unit "$OLLAMA_UNIT"
                fi
                sleep 1
                ;;
            3)
                if [[ $(get_unit_status "$VLLM_UNIT") == "running" ]]; then
                    stop_unit "$VLLM_UNIT"
                else
                    start_unit "$VLLM_UNIT"
                fi
                sleep 1
                ;;
            4)
                if [[ $(get_unit_status "$VLLM_DOLPHIN_UNIT") == "running" ]]; then
                    stop_unit "$VLLM_DOLPHIN_UNIT"
                else
                    start_unit "$VLLM_DOLPHIN_UNIT"
                fi
                sleep 1
                ;;
            5)
                if [[ $(get_unit_status "$LLAMACPP_UNIT") == "running" ]]; then
                    stop_unit "$LLAMACPP_UNIT"
                else
                    start_unit "$LLAMACPP_UNIT"
                fi
                sleep 1
                ;;
            b|B)
                break
                ;;
            *)
                log_error "Invalid option"
                sleep 1
                ;;
        esac
    done
}

# Provider management
provider_management() {
    while true; do
        show_header
        log_header "‚îÅ‚îÅ‚îÅ Provider Management ‚îÅ‚îÅ‚îÅ"
        echo

        # Show current models status
        python3 "$SCRIPT_DIR/manage-providers.py" list

        echo
        log_header "‚îÅ‚îÅ‚îÅ Management Options ‚îÅ‚îÅ‚îÅ"
        echo
        echo "  ${CYAN}1${NC}. Enable a model"
        echo "  ${CYAN}2${NC}. Disable a model"
        echo "  ${CYAN}3${NC}. Enable all models from a provider"
        echo "  ${CYAN}4${NC}. Disable all models from a provider"
        echo "  ${CYAN}5${NC}. Apply changes (restart LiteLLM)"
        echo "  ${CYAN}b${NC}. Back to main menu"
        echo
        read -rp "Select option: " choice

        case $choice in
            1)
                echo
                read -rp "Enter model name to enable: " model_name
                python3 "$SCRIPT_DIR/manage-providers.py" enable "$model_name"
                pause
                ;;
            2)
                echo
                read -rp "Enter model name to disable: " model_name
                python3 "$SCRIPT_DIR/manage-providers.py" disable "$model_name"
                pause
                ;;
            3)
                echo
                python3 "$SCRIPT_DIR/manage-providers.py" providers
                echo
                read -rp "Enter provider name: " provider_name
                python3 "$SCRIPT_DIR/manage-providers.py" enable-provider "$provider_name"
                pause
                ;;
            4)
                echo
                python3 "$SCRIPT_DIR/manage-providers.py" providers
                echo
                read -rp "Enter provider name: " provider_name
                python3 "$SCRIPT_DIR/manage-providers.py" disable-provider "$provider_name"
                pause
                ;;
            5)
                echo
                log_info "Restarting LiteLLM service to apply changes..."
                if systemctl --user restart litellm.service; then
                    log_success "LiteLLM service restarted successfully"
                    log_info "Waiting for service to be ready..."
                    sleep 3
                    if curl -s -f "$LITELLM_URL/health" > /dev/null 2>&1; then
                        log_success "LiteLLM is healthy and ready"
                    else
                        log_warning "LiteLLM may still be starting up"
                    fi
                else
                    log_error "Failed to restart LiteLLM service"
                fi
                pause
                ;;
            b|B)
                return
                ;;
            *)
                log_error "Invalid option"
                sleep 1
                ;;
        esac
    done
}

# Main menu
main_menu() {
    while true; do
        show_status

        log_header "‚îÅ‚îÅ‚îÅ Main Menu ‚îÅ‚îÅ‚îÅ"
        echo
        echo "  ${CYAN}1${NC}. ${ICON_CHART} Show detailed status"
        echo "  ${CYAN}2${NC}. ${ICON_MODEL} List all models"
        echo "  ${CYAN}3${NC}. ${ICON_HEALTH} Run health check"
        echo "  ${CYAN}4${NC}. ${ICON_GEAR} vLLM model management"
        echo "  ${CYAN}5${NC}. ${ICON_INFO} View configuration"
        echo "  ${CYAN}6${NC}. ${ICON_SERVER} View service logs"
        echo "  ${CYAN}7${NC}. ${ICON_ROCKET} Test endpoints"
        echo "  ${CYAN}8${NC}. ${ICON_GEAR} Quick actions"
        echo "  ${CYAN}9${NC}. üîå Provider management (enable/disable)"
        echo "  ${CYAN}10${NC}. ‚ö° Service control"
        echo "  ${CYAN}q${NC}. Quit"
        echo
        read -rp "Select option: " choice

        case $choice in
            1)
                show_status
                pause
                ;;
            2)
                list_models
                pause
                ;;
            3)
                health_check
                pause
                ;;
            4)
                vllm_menu
                ;;
            5)
                view_config
                ;;
            6)
                view_logs
                ;;
            7)
                test_endpoints
                ;;
            8)
                quick_actions
                ;;
            9)
                provider_management
                ;;
            10)
                service_control
                ;;
            q|Q)
                clear 2>/dev/null || true
                echo -e "${GREEN}${ICON_ROCKET} Thanks for using PTUI!${NC}"
                echo
                exit 0
                ;;
            *)
                log_error "Invalid option. Please try again."
                sleep 1
                ;;
        esac
    done
}

# Command-line interface (non-interactive mode)
needs_dependencies=true
if [[ $# -gt 0 ]]; then
    case "$1" in
        help|--help|-h|version|--version|-V)
            needs_dependencies=false
            ;;
    esac
fi

if [[ $# -eq 0 ]]; then
    needs_dependencies=false
fi

if [[ "$needs_dependencies" == "true" ]]; then
    ensure_dependencies
fi

if [[ $# -gt 0 ]]; then
    case "$1" in
        status|s)
            show_status
            exit $?
            ;;
        models|m)
            list_models
            exit $?
            ;;
        health|h)
            health_check
            exit $?
            ;;
        vllm|v)
            shift
            if [[ -f "$SCRIPT_DIR/vllm-model-switch.sh" ]]; then
                "$SCRIPT_DIR/vllm-model-switch.sh" "$@"
            else
                log_error "vLLM management script not found"
                exit 1
            fi
            exit $?
            ;;
        test|t)
            test_endpoints
            exit $?
            ;;
        help|--help|-h)
            show_help
            exit 0
            ;;
        version|--version|-v|-V)
            show_version
            exit 0
            ;;
        *)
            log_error "Unknown command: $1"
            echo "Run '$DISPLAY_INVOCATION help' for usage information"
            exit 1
            ;;
    esac
else
    if ! PTUI_HTTP_TIMEOUT="$DEFAULT_HTTP_TIMEOUT" \
        PTUI_REFRESH_SECONDS="$AUTO_REFRESH_SECONDS" \
        PTUI_VERSION="$VERSION" \
        "$PYTHON_BIN" "$SCRIPT_DIR/ptui_dashboard.py"
    then
        echo "‚ö†Ô∏è  Falling back to legacy PTUI menu..."
        ensure_dependencies
        main_menu
    fi
    exit 0
fi
