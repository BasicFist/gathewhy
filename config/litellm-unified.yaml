# ============================================================================
# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# ============================================================================
#
# Generated by: scripts/generate-litellm-config.py
# Source files: config/providers.yaml, config/model-mappings.yaml
# Generated at: 2025-11-16T13:32:49.337783
# Version: git-6da51ef
#
# To modify this configuration:
#   1. Edit config/providers.yaml or config/model-mappings.yaml
#   2. Run: python3 scripts/generate-litellm-config.py
#   3. Validate: python3 scripts/validate-config-schema.py
#
# ============================================================================

model_list:
  - model_name: llama3.1:latest
    litellm_params:
      model: ollama/llama3.1:latest
      api_base: http://127.0.0.1:11434
    model_info:
      tags:
        - general_chat
        - 8b
        - q4_k_m
      provider: ollama
  - model_name: qwen2.5-coder:7b
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://127.0.0.1:11434
    model_info:
      tags:
        - code_generation
        - 7.6b
        - q4_k_m
      provider: ollama
  - model_name: mythomax-l2-13b-q5_k_m
    litellm_params:
      model: ollama/mythomax-l2-13b-q5_k_m
      api_base: http://127.0.0.1:11434
      extra_body:
        options:
          num_gpu_layers: -1
    model_info:
      tags:
        - creative_writing
        - 13b
        - q5_k_m
      provider: ollama
  - model_name: llama-cpp-default
    litellm_params:
      model: openai/local-model
      api_base: http://127.0.0.1:8000
      stream: true
    model_info:
      tags:
        - general
      provider: llama_cpp_python
      context_length: 8192
      notes: GGUF model via llama.cpp Python bindings
  - model_name: llama-cpp-native
    litellm_params:
      model: openai/local-model
      api_base: http://127.0.0.1:8080
      stream: true
    model_info:
      tags:
        - general
      provider: llama_cpp_native
      context_length: 8192
      notes: GGUF model via llama.cpp native C++ server
  - model_name: qwen-coder-vllm
    litellm_params:
      model: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
      api_base: http://127.0.0.1:8001/v1
      custom_llm_provider: openai
      stream: true
      api_key: not-needed
    model_info:
      tags:
        - code_generation
        - 7b
        - awq
      provider: vllm-qwen
      context_length: 4096
  - model_name: deepseek-v3.1:671b-cloud
    litellm_params:
      model: ollama_chat/deepseek-v3.1:671b-cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - advanced_reasoning
        - 671b
      provider: ollama_cloud
  - model_name: qwen3-coder:480b-cloud
    litellm_params:
      model: ollama_chat/qwen3-coder:480b-cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - code_generation
        - 480b
      provider: ollama_cloud
  - model_name: kimi-k2:1t-cloud
    litellm_params:
      model: ollama_chat/kimi-k2:1t-cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - advanced_reasoning
        - 1t
      provider: ollama_cloud
  - model_name: gpt-oss:120b-cloud
    litellm_params:
      model: ollama_chat/gpt-oss:120b-cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - general_chat
        - 120b
      provider: ollama_cloud
  - model_name: gpt-oss:20b-cloud
    litellm_params:
      model: ollama_chat/gpt-oss:20b-cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - general_chat
        - 20b
      provider: ollama_cloud
  - model_name: glm-4.6:cloud
    litellm_params:
      model: ollama_chat/glm-4.6:cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - general_chat
        - 4.6b
      provider: ollama_cloud
litellm_settings:
  request_timeout: 60
  stream_timeout: 120
  num_retries: 3
  timeout: 300
  cache: true
  cache_params:
    type: redis
    host: 127.0.0.1
    port: 6379
    ttl: 3600
  set_verbose: true
  json_logs: true
router_settings:
  routing_strategy: simple-shuffle
  model_group_alias:
    code:
      - qwen3-coder:480b-cloud
    analytical:
      - deepseek-v3.1:671b-cloud
    reasoning:
      - deepseek-v3.1:671b-cloud
    creative:
      - mythomax-l2-13b-q5_k_m
    chat:
      - llama3.1:latest
    large_context:
      - kimi-k2:1t-cloud
  allowed_fails: 5
  num_retries: 2
  timeout: 60
  cooldown_time: 60
  enable_pre_call_checks: true
  redis_host: 127.0.0.1
  redis_port: 6379
  fallbacks:
    - qwen2.5-coder:7b:
        - qwen-coder-vllm
        - qwen3-coder:480b-cloud
    - llama3.1:latest:
        - llama-cpp-default
        - gpt-oss:20b-cloud
    - mythomax-l2-13b-q5_k_m:
        - gpt-oss:20b-cloud
    - qwen-coder-vllm:
        - qwen3-coder:480b-cloud
        - llama3.1:latest
    - llama-cpp-default:
        - llama-cpp-native
    - deepseek-v3.1:671b-cloud:
        - gpt-oss:120b-cloud
        - gpt-oss:20b-cloud
        - llama3.1:latest
    - kimi-k2:1t-cloud:
        - gpt-oss:120b-cloud
        - gpt-oss:20b-cloud
        - llama3.1:latest
    - qwen3-coder:480b-cloud:
        - gpt-oss:120b-cloud
        - llama3.1:latest
    - gpt-oss:120b-cloud:
        - gpt-oss:20b-cloud
        - llama3.1:latest
    - gpt-oss:20b-cloud:
        - llama-cpp-default
        - llama-cpp-native
    - glm-4.6:cloud:
        - llama3.1:latest
        - mythomax-l2-13b-q5_k_m
    - default:
        - gpt-oss:120b-cloud
        - gpt-oss:20b-cloud
        - llama3.1:latest
        - qwen2.5-coder:7b
        - qwen-coder-vllm
    - code:
        - qwen-coder-vllm
        - qwen2.5-coder:7b
    - analytical:
        - kimi-k2:1t-cloud
        - qwen2.5-coder:7b
    - reasoning:
        - kimi-k2:1t-cloud
        - llama3.1:latest
    - creative:
        - llama3.1:latest
        - gpt-oss:120b-cloud
    - chat:
        - gpt-oss:120b-cloud
        - gpt-oss:20b-cloud
        - glm-4.6:cloud
    - large_context:
        - llama_cpp_python
  lab_extensions:
    capabilities:
      code:
        description: Code generation, analysis, and debugging
        preferred_models:
          - qwen3-coder:480b-cloud
          - qwen-coder-vllm
          - qwen2.5-coder:7b
        routing_strategy: complexity_based
        complexity_thresholds:
          high: qwen3-coder:480b-cloud
          medium: qwen-coder-vllm
          low: qwen2.5-coder:7b
      analytical:
        description: Technical analysis, structured problem-solving, planning
        preferred_models:
          - deepseek-v3.1:671b-cloud
          - kimi-k2:1t-cloud
          - qwen2.5-coder:7b
        routing_strategy: context_based
        context_thresholds:
          large: kimi-k2:1t-cloud
          complex: deepseek-v3.1:671b-cloud
          standard: qwen2.5-coder:7b
      reasoning:
        description: Multi-step logical reasoning and inference
        preferred_models:
          - deepseek-v3.1:671b-cloud
          - kimi-k2:1t-cloud
          - llama3.1:latest
        routing_strategy: direct
      creative:
        description: Creative writing, storytelling, roleplay
        preferred_models:
          - mythomax-l2-13b-q5_k_m
          - llama3.1:latest
          - gpt-oss:120b-cloud
        routing_strategy: usage_based
      chat:
        description: Conversational AI and general assistance
        preferred_models:
          - llama3.1:latest
          - gpt-oss:120b-cloud
          - gpt-oss:20b-cloud
          - glm-4.6:cloud
        routing_strategy: usage_based
      high_throughput:
        description: Batch processing and concurrent request handling
        min_model_size: 13B
        provider: vllm-qwen
        routing_strategy: least_loaded
        notes: 'vLLM single-instance: only Qwen model available for throughput optimization'
      low_latency:
        description: Single-request speed priority (TTFB optimization)
        provider: llama_cpp_native
        fallback: llama_cpp_python
        routing_strategy: fastest_response
      large_context:
        description: Large context window processing (> 8K tokens)
        min_context: 8192
        preferred_models:
          - kimi-k2:1t-cloud
          - llama_cpp_python
        routing_strategy: most_capacity
    pattern_routes:
      - pattern: ^Qwen/Qwen2\.5-Coder.*
        provider: vllm-qwen
        fallback: ollama
        description: Qwen Coder models via vLLM (AWQ quantized)
      - pattern: ^solidrust/dolphin.*AWQ$
        provider: vllm-dolphin
        fallback: ollama
        description: Dolphin uncensored models via vLLM (AWQ quantized)
      - pattern: ^meta-llama/.*
        provider: vllm-qwen
        fallback: ollama
        description: HuggingFace Llama models prefer vLLM for performance
      - pattern: ^mistralai/.*
        provider: vllm-dolphin
        fallback: ollama
        description: Mistral models via vLLM
      - pattern: .*:\d+[bB]$
        provider: ollama
        description: Ollama naming convention (model:size)
      - pattern: .*\.gguf$
        provider: llama_cpp_python
        fallback: llama_cpp_native
        description: GGUF quantized models
    load_balancing:
      llama3.1:latest:
        providers:
          - provider: ollama
            weight: 0.7
          - provider: llama_cpp_python
            weight: 0.3
        strategy: weighted_round_robin
        description: 'Distribute general chat: 70% Ollama, 30% llama.cpp'
      general-chat:
        providers:
          - provider: ollama
            weight: 0.6
          - provider: vllm-qwen
            weight: 0.4
        strategy: least_loaded
        description: Dynamic load balancing based on current provider load
      code-generation:
        providers:
          - provider: vllm-qwen
            weight: 0.5
            condition: request_count > 5 OR batch_mode
          - provider: ollama
            weight: 0.3
            condition: request_count <= 5
          - provider: ollama_cloud
            weight: 0.2
            condition: local_saturated OR complexity == 'high'
        strategy: adaptive_weighted
        description: 'Intelligent code routing: local for simple/medium, cloud for complex/overflow'
        parameters:
          saturation_threshold: 0.8
          complexity_detection: token_count
      creative-tasks:
        providers:
          - provider: ollama
            weight: 0.7
            preferred_model: mythomax-l2-13b-q5_k_m
          - provider: ollama_cloud
            weight: 0.3
            preferred_model: gpt-oss:120b-cloud
            condition: length > 1000 OR quality_tier == 'high'
        strategy: quality_based
        description: 'Creative writing: local for standard, cloud for high-quality or long-form'
    routing_rules:
      default_provider: ollama
      default_fallback: llama_cpp_python
      request_metadata_routing:
        high_priority_requests:
          provider: vllm-qwen
          condition: header.x-priority == "high"
        batch_requests:
          provider: vllm-qwen
          condition: batch_size > 1
        streaming_requests:
          provider: ollama
          condition: stream == true
      model_size_routing:
        - size: < 8B
          provider: ollama
          reason: Small models work well with Ollama
        - size: 8B - 13B
          provider: ollama
          fallback: llama_cpp_python
          reason: Medium models, Ollama preferred
        - size: '> 13B'
          provider: vllm-qwen
          fallback: ollama
          reason: Large models benefit from vLLM batching
    special_cases:
      first_request_routing:
        description: Cold start optimization
        strategy: prefer_warm_providers
        warm_check_timeout_ms: 100
      rate_limited_fallback:
        description: Automatic fallback when provider hits rate limit
        enabled: true
        fallback_duration_seconds: 60
      error_based_routing:
        description: Avoid providers with recent errors
        enabled: true
        error_threshold: 3
        cooldown_seconds: 300
      geographic_routing:
        description: Route based on provider location (future)
        enabled: false
        prefer_local: true
  max_concurrent_requests: 64
server_settings:
  port: 4000
  host: 0.0.0.0
  cors:
    enabled: true
    allowed_origins:
      - http://localhost:*
      - http://127.0.0.1:*
      - http://[::1]:*
  health_check_endpoint: /health
rate_limit_settings:
  enabled: true
  limits:
    llama3.1:latest:
      rpm: 100
      tpm: 50000
    qwen2.5-coder:7b:
      rpm: 100
      tpm: 50000
    mythomax-l2-13b-q5_k_m:
      rpm: 100
      tpm: 50000
    llama-cpp-default:
      rpm: 120
      tpm: 60000
    llama-cpp-native:
      rpm: 120
      tpm: 60000
    qwen-coder-vllm:
      rpm: 50
      tpm: 100000
    deepseek-v3.1:671b-cloud:
      rpm: 100
      tpm: 50000
    qwen3-coder:480b-cloud:
      rpm: 100
      tpm: 50000
    kimi-k2:1t-cloud:
      rpm: 100
      tpm: 50000
    gpt-oss:120b-cloud:
      rpm: 100
      tpm: 50000
    gpt-oss:20b-cloud:
      rpm: 100
      tpm: 50000
    glm-4.6:cloud:
      rpm: 100
      tpm: 50000
general_settings:
  background_health_checks: true
  health_check_interval: 60
  health_check_details: false
  '# Authentication disabled intentionally':
  '# Set master_key in this section only if you need request signing':
debug: false
debug_router: false
test_mode: false
