model_list:
  - model_name: llama3.1:latest
    litellm_params:
      model: ollama/llama3.1:latest
      api_base: http://127.0.0.1:11434
    model_info:
      tags:
        - 8b
        - q4_k_m
      provider: ollama
  - model_name: qwen2.5-coder:7b
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://127.0.0.1:11434
    model_info:
      tags:
        - code_generation
        - 7.6b
        - q4_k_m
      provider: ollama
  # ============================================================================
  # vLLM Models - SINGLE INSTANCE STRATEGY (16GB VRAM constraint)
  # ============================================================================
  # IMPORTANT: Both models use port 8001. Only ONE can run at a time due to
  # VRAM constraints (each needs ~12-13GB, total available: 16GB).
  #
  # To switch models:
  #   1. Stop current: pkill -f "vllm serve"
  #   2. Start desired model (see scripts/vllm-model-switch.sh)
  #
  # Current default: Qwen Coder (code generation optimized)
  # Alternative: Dolphin (uncensored conversational)
  # ============================================================================
  - model_name: qwen-coder-vllm
    litellm_params:
      model: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
      api_base: http://127.0.0.1:8001/v1
      api_key: not-needed  # pragma: allowlist secret
      custom_llm_provider: openai
      stream: true
    model_info:
      tags:
        - code
        - vllm
        - awq
      provider: vllm
      vram_required: "~12.6GB"
      note: "Default vLLM model - code generation specialist"
  - model_name: dolphin-uncensored-vllm
    litellm_params:
      model: solidrust/dolphin-2.8-mistral-7b-v02-AWQ
      api_base: http://127.0.0.1:8001/v1
      api_key: not-needed  # pragma: allowlist secret
      custom_llm_provider: openai
      stream: true
    model_info:
      tags:
        - uncensored
        - mistral
        - dolphin
        - vllm
        - awq
        - 7b
      provider: vllm
      vram_required: "~12-13GB"
      note: "Alternate vLLM model - requires manual switch (see config header)"
litellm_settings:
  request_timeout: 60
  stream_timeout: 0
  num_retries: 3
  timeout: 300
  cache: true
  cache_params:
    type: redis
    host: 127.0.0.1
    port: 6379
    ttl: 3600
  set_verbose: false
  json_logs: true
  fallbacks: [{"qwen-coder-vllm": ["qwen2.5-coder:7b"]}, {"dolphin-uncensored-vllm": ["llama3.1:latest"]}]
router_settings:
  routing_strategy: usage-based-routing-v2
  allowed_fails: 3
  num_retries: 2
  timeout: 30
  cooldown_time: 60
  enable_pre_call_checks: true
  redis_host: 127.0.0.1
  redis_port: 6379
server_settings:
  port: 4000
  host: 0.0.0.0
  cors:
    enabled: true
    allowed_origins:
      - http://localhost:*
      - http://127.0.0.1:*
      - http://[::1]:*
  health_check_endpoint: /health
  prometheus:
    enabled: true
    port: 9090
rate_limit_settings:
  enabled: true
  limits:
    llama3.1:latest:
      rpm: 100
      tpm: 50000
    qwen2.5-coder:7b:
      rpm: 100
      tpm: 50000
    dolphin-uncensored-vllm:
      rpm: 100
      tpm: 50000
general_settings:
  '# Master Key Authentication': null
  '# Uncomment to enable': null
  '# master_key': ${LITELLM_MASTER_KEY}
  '# Salt Key for DB encryption': null
  '# salt_key': ${LITELLM_SALT_KEY}
debug: false
debug_router: false
test_mode: false
