#!/usr/bin/env python3
"""
AI Backend Enhanced Dashboard

A professional terminal dashboard for monitoring AI backend providers with VRAM monitoring.
"""

import asyncio
import subprocess
import threading
import time
from dataclasses import dataclass, field
from typing import Dict, List, Optional

import psutil
import pynvml
import requests
from textual import work
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Container, Horizontal, Vertical, Grid
from textual.reactive import reactive
from textual.widgets import (
    Button,
    Footer,
    Header,
    Label,
    ProgressBar,
    Static,
    DataTable,
)


@dataclass
class ProviderStatus:
    """Represents the status of a provider."""
    name: str
    status: str  # 'active', 'inactive', 'error'
    port: int
    model_count: int
    response_time: float
    vram_usage_mb: float = 0.0
    vram_percentage: float = 0.0
    pid: Optional[int] = None
    uptime: str = "N/A"


@dataclass
class GPUInfo:
    """Represents GPU information."""
    id: int
    name: str
    memory_total_mb: float
    memory_used_mb: float
    memory_util_percent: float
    gpu_util_percent: float
    temperature: float = 0.0
    power_draw_w: float = 0.0


class ProviderMonitor:
    """Handles monitoring of AI backend providers."""

    PROVIDER_ENDPOINTS = {
        "ollama": "http://127.0.0.1:11434/api/tags",
        "llama_cpp_python": "http://127.0.0.1:8000/v1/models",
        "llama_cpp_native": "http://127.0.0.1:8080/v1/models",
        "vllm-qwen": "http://127.0.0.1:8001/v1/models",
        "vllm-dolphin": "http://127.0.0.1:8002/v1/models",
    }

    def __init__(self):
        self.gpu_monitor = GPUMonitor()

    def check_provider_health(self, name: str, endpoint: str) -> ProviderStatus:
        """Check the health of a provider."""
        start_time = time.time()

        try:
            response = requests.get(endpoint, timeout=3)
            response_time = time.time() - start_time

            if response.status_code == 200:
                try:
                    data = response.json()
                    if "data" in data and isinstance(data["data"], list):
                        model_count = len(data["data"])
                    else:
                        model_count = 0
                except (ValueError, TypeError, KeyError):
                    model_count = 0

                port = int(endpoint.split(":")[2].split("/")[0])

                return ProviderStatus(
                    name=name,
                    status="active",
                    port=port,
                    model_count=model_count,
                    response_time=round(response_time * 1000, 1)
                )
            else:
                return ProviderStatus(
                    name=name,
                    status="error",
                    port=int(endpoint.split(":")[2].split("/")[0]),
                    model_count=0,
                    response_time=0.0
                )
        except requests.exceptions.Timeout:
            return ProviderStatus(
                name=name,
                status="inactive",
                port=int(endpoint.split(":")[2].split("/")[0]),
                model_count=0,
                response_time=3000.0
            )
        except requests.exceptions.ConnectionError:
            return ProviderStatus(
                name=name,
                status="inactive",
                port=int(endpoint.split(":")[2].split("/")[0]),
                model_count=0,
                response_time=0.0
            )
        except Exception:
            return ProviderStatus(
                name=name,
                status="error",
                port=int(endpoint.split(":")[2].split("/")[0]),
                model_count=0,
                response_time=0.0
            )

    def get_all_provider_status(self) -> List[ProviderStatus]:
        """Get status for all providers."""
        statuses = [
            self.check_provider_health(name, endpoint)
            for name, endpoint in self.PROVIDER_ENDPOINTS.items()
        ]

        # Correlate with VRAM usage
        vram_info = self.gpu_monitor.get_provider_vram_usage()
        for status in statuses:
            if status.name in vram_info:
                status.vram_usage_mb = vram_info[status.name]["memory_used_mb"]
                status.vram_percentage = vram_info[status.name]["memory_percentage"]
                status.pid = vram_info[status.name]["pid"]

        return statuses

    def systemctl_command(self, command: str, service_name: str) -> bool:
        """Execute a systemctl command."""
        try:
            # Special handling for enable/disable commands which might need different approach
            if command in ["enable", "disable"]:
                # For enable/disable, we might want to check if the service exists first
                check_result = subprocess.run(
                    ["systemctl", "--user", "list-unit-files", service_name],
                    capture_output=True,
                    text=True,
                    check=False
                )
                if service_name not in check_result.stdout:
                    # Service doesn't exist, return False
                    return False

            result = subprocess.run(
                ["systemctl", "--user", command, service_name],
                capture_output=True,
                text=True,
                check=False,
                timeout=10  # Longer timeout for enable/disable
            )
            return result.returncode == 0
        except subprocess.TimeoutExpired:
            return False
        except Exception:
            return False


class GPUMonitor:
    """Monitors GPU resources and correlates with AI provider processes."""

    def __init__(self):
        self.initialized = False
        try:
            pynvml.nvmlInit()
            self.initialized = True
            self.device_count = pynvml.nvmlDeviceGetCount()
        except Exception:
            self.initialized = False
            self.device_count = 0

    def get_gpu_info(self) -> List[GPUInfo]:
        """Get information about all GPUs."""
        if not self.initialized:
            return []

        gpus = []
        try:
            for i in range(self.device_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')

                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)

                # Try to get temperature
                try:
                    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                except (pynvml.NVMLError, AttributeError):
                    temp = 0.0

                # Try to get power draw
                try:
                    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert mW to W
                except (pynvml.NVMLError, AttributeError):
                    power = 0.0

                gpus.append(GPUInfo(
                    id=i,
                    name=name,
                    memory_total_mb=memory_info.total / 1024**2,
                    memory_used_mb=memory_info.used / 1024**2,
                    memory_util_percent=(memory_info.used / memory_info.total) * 100 if memory_info.total > 0 else 0,
                    gpu_util_percent=utilization.gpu,
                    temperature=temp,
                    power_draw_w=power
                ))
        except Exception:
            pass

        return gpus

    def get_provider_vram_usage(self) -> Dict[str, Dict]:
        """Get VRAM usage correlated with AI provider processes."""
        if not self.initialized:
            return {}

        provider_vram = {}
        try:
            # Map process names to provider names
            provider_process_map = {
                "ollama": ["ollama"],
                "vllm-qwen": ["vllm", "qwen"],
                "vllm-dolphin": ["vllm", "dolphin"],
                "llama_cpp_python": ["llama-cpp", "llama_cpp"],
                "llama_cpp_native": ["llama-cpp", "llama_cpp"]
            }

            for i in range(self.device_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                procs = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)

                for proc in procs:
                    try:
                        ps_proc = psutil.Process(proc.pid)
                        proc_name = ps_proc.name().lower()
                        cmdline = ' '.join(ps_proc.cmdline()).lower()

                        # Match process to provider
                        for provider, keywords in provider_process_map.items():
                            if any(keyword in proc_name or keyword in cmdline for keyword in keywords):
                                if provider not in provider_vram:
                                    provider_vram[provider] = {
                                        "memory_used_mb": 0.0,
                                        "memory_percentage": 0.0,
                                        "pid": proc.pid
                                    }

                                # Accumulate memory usage
                                memory_mb = proc.usedGpuMemory / 1024**2
                                provider_vram[provider]["memory_used_mb"] += memory_mb

                                # Calculate percentage (assuming 24GB total as example)
                                # In a real implementation, we'd get the actual GPU memory
                                provider_vram[provider]["memory_percentage"] += min(memory_mb / 24000 * 100, 100)

                                break
                    except (psutil.NoSuchProcess, Exception):
                        continue

        except Exception:
            pass

        return provider_vram


class ProviderCard(Vertical):
    """Interactive card for a single provider."""

    def __init__(self, provider_name: str):
        super().__init__()
        self.provider_name = provider_name
        self.provider_status = None

    def compose(self) -> ComposeResult:
        # Header with provider name and status
        with Horizontal(classes="card-header"):
            yield Label(self.provider_name, classes="provider-name")
            yield Label("● Initializing...", id=f"status_{self.provider_name}", classes="status-indicator initializing")

        # Metrics section
        with Vertical(classes="metrics-section"):
            yield Label("Port: -", id=f"port_{self.provider_name}", classes="metric")
            yield Label("Models: -", id=f"models_{self.provider_name}", classes="metric")
            yield Label("VRAM: -", id=f"vram_{self.provider_name}", classes="metric vram-low")
            yield Label("Resp: -", id=f"response_{self.provider_name}", classes="metric")

        # Action buttons
        with Horizontal(classes="actions-section"):
            yield Button("Start", id=f"start_{self.provider_name}", variant="success", classes="action-btn")
            yield Button("Stop", id=f"stop_{self.provider_name}", variant="error", classes="action-btn")
            yield Button("Restart", id=f"restart_{self.provider_name}", variant="warning", classes="action-btn")
            yield Button("Enable", id=f"enable_{self.provider_name}", variant="primary", classes="action-btn")
            yield Button("Disable", id=f"disable_{self.provider_name}", variant="error", classes="action-btn")

    def update_status(self, status: ProviderStatus):
        """Update the provider card with new status."""
        self.provider_status = status

        # Update status indicator
        status_widget = self.query_one(f"#status_{self.provider_name}")
        status_text = f"● {status.status.title()}"
        status_widget.update(status_text)

        # Clear all status classes and add current one
        for cls in ["initializing", "active", "inactive", "error"]:
            status_widget.remove_class(cls)
        status_widget.add_class(status.status.lower())

        # Update metrics
        self.query_one(f"#port_{self.provider_name}").update(f"Port: {status.port}")
        self.query_one(f"#models_{self.provider_name}").update(f"Models: {status.model_count}")

        if status.vram_usage_mb > 0:
            vram_widget = self.query_one(f"#vram_{self.provider_name}")
            vram_widget.update(f"VRAM: {status.vram_usage_mb:.0f}MB ({status.vram_percentage:.1f}%)")

            # Add VRAM usage coloring
            vram_widget.remove_class("vram-low", "vram-medium", "vram-high")
            if status.vram_percentage > 80:
                vram_widget.add_class("vram-high")
            elif status.vram_percentage > 50:
                vram_widget.add_class("vram-medium")
            else:
                vram_widget.add_class("vram-low")
        else:
            self.query_one(f"#vram_{self.provider_name}").update("VRAM: -")

        response_time = f"{status.response_time}ms" if status.response_time > 0 else "-"
        self.query_one(f"#response_{self.provider_name}").update(f"Resp: {response_time}")


class GPUPanel(Vertical):
    """Panel showing GPU information."""

    def __init__(self):
        super().__init__()
        self.gpu_monitor = GPUMonitor()

    def compose(self) -> ComposeResult:
        yield Label("[b]GPU Resources[/b]", classes="panel-title")
        yield Label("Loading GPU information...", id="gpu-info", classes="gpu-info")
        yield ProgressBar(id="gpu-memory-bar", total=100, show_eta=False)
        yield Label("GPU Utilization: -%", id="gpu-util", classes="metric")

    @work(thread=True)
    async def update_gpu_info(self):
        """Update GPU information in background."""
        if not self.gpu_monitor.initialized:
            self.app.call_from_thread(self.update_gpu_display, "No NVIDIA GPU detected", 0, 0)
            return

        try:
            gpus = self.gpu_monitor.get_gpu_info()
            if gpus:
                gpu = gpus[0]  # For simplicity, show first GPU
                info_text = f"{gpu.name} | {gpu.memory_used_mb:.0f}/{gpu.memory_total_mb:.0f}MB"
                memory_percent = gpu.memory_util_percent
                util_percent = gpu.gpu_util_percent

                self.app.call_from_thread(self.update_gpu_display, info_text, memory_percent, util_percent)
        except Exception:
            self.app.call_from_thread(self.update_gpu_display, "Error reading GPU info", 0, 0)

    def update_gpu_display(self, info_text: str, memory_percent: float, util_percent: float):
        """Update GPU display on main thread."""
        try:
            self.query_one("#gpu-info").update(info_text)
            memory_bar = self.query_one("#gpu-memory-bar")
            memory_bar.total = 100
            memory_bar.progress = memory_percent

            util_widget = self.query_one("#gpu-util")
            util_widget.update(f"GPU Utilization: {util_percent:.1f}%")

            # Add coloring based on usage
            util_widget.remove_class("low", "medium", "high")
            if util_percent > 80:
                util_widget.add_class("high")
            elif util_percent > 50:
                util_widget.add_class("medium")
            else:
                util_widget.add_class("low")
        except (AttributeError, ValueError):
            pass


class SystemMetrics(Static):
    """Display system metrics."""

    cpu_percent = reactive(0.0)
    memory_percent = reactive(0.0)

    def on_mount(self) -> None:
        self.set_interval(2.0, self.update_metrics)

    def update_metrics(self):
        """Update system metrics."""
        self.cpu_percent = psutil.cpu_percent(interval=None)
        self.memory_percent = psutil.virtual_memory().percent

    def watch_cpu_percent(self, cpu: float):
        """Update when CPU changes."""
        try:
            cpu_widget = self.query_one("#cpu_label")
            cpu_widget.update(f"CPU: {cpu:.1f}%")

            # Add color coding based on usage
            cpu_widget.remove_class("low", "medium", "high")
            if cpu > 80:
                cpu_widget.add_class("high")
            elif cpu > 50:
                cpu_widget.add_class("medium")
            else:
                cpu_widget.add_class("low")
        except (AttributeError, ValueError):
            pass

    def watch_memory_percent(self, memory: float):
        """Update when memory changes."""
        try:
            mem_widget = self.query_one("#memory_label")
            mem_widget.update(f"MEM: {memory:.1f}%")

            # Add color coding based on usage
            mem_widget.remove_class("low", "medium", "high")
            if memory > 80:
                mem_widget.add_class("high")
            elif memory > 50:
                mem_widget.add_class("medium")
            else:
                mem_widget.add_class("low")
        except (AttributeError, ValueError):
            pass

    def compose(self) -> ComposeResult:
        with Horizontal(classes="system-metrics"):
            yield Label("CPU: 0.0%", id="cpu_label", classes="metric low")
            yield Label("MEM: 0.0%", id="memory_label", classes="metric low")


class AIEnhancedDashboard(App):
    """Enhanced AI Backend Dashboard with VRAM monitoring."""

    CSS = """
    Screen {
        layout: grid;
        grid-size: 3;
        grid-columns: 1fr 1fr 1fr;
        grid-rows: auto 1fr auto;
        grid-gutter: 1;
        background: $surface;
    }

    Header {
        height: 1;
        background: $primary;
        color: $text;
        text-style: bold;
        column-span: 3;
    }

    Footer {
        height: 1;
        background: $primary;
        color: $text;
        column-span: 3;
    }

    #providers-panel {
        column-span: 2;
        border: solid $primary;
        padding: 1;
        height: 1fr;
    }

    #gpu-panel {
        border: solid $secondary;
        padding: 1;
        height: 1fr;
    }

    #metrics-panel {
        column-span: 3;
        border: solid $accent;
        padding: 1;
        height: auto;
    }

    .panel-title {
        text-style: bold;
        margin-bottom: 1;
        color: $primary;
    }

    .card-header {
        height: auto;
        margin-bottom: 1;
    }

    .provider-name {
        width: 20;
        text-style: bold;
        background: $boost;
        text-align: center;
    }

    .status-indicator {
        width: 15;
        text-align: center;
        text-style: bold;
    }

    .status-indicator.active {
        color: $success;
    }

    .status-indicator.inactive {
        color: $warning;
    }

    .status-indicator.error {
        color: $error;
    }

    .status-indicator.initializing {
        color: $text-disabled;
    }

    .metrics-section {
        height: auto;
        margin-bottom: 1;
    }

    .metric {
        width: 100%;
        height: 1;
        margin: 0 0 1 0;
        background: $boost;
        padding: 0 1;
    }

    .metric.vram-low {
        color: $success;
    }

    .metric.vram-medium {
        color: $warning;
    }

    .metric.vram-high {
        color: $error;
    }

    .actions-section {
        height: auto;
        align: center middle;
    }

    .actions-section {
        height: auto;
        align: center middle;
        overflow-x: auto;
    }

    .action-btn {
        width: 8;
        height: 1;
        margin: 0 1;
        min-width: 8;
    }

    .gpu-info {
        margin: 0 0 1 0;
        background: $boost;
        padding: 0 1;
    }

    .system-metrics {
        margin: 1 0;
        height: auto;
        align: center middle;
    }

    ProgressBar {
        height: 1;
        margin: 0 0 1 0;
    }

    .metric.low {
        color: $success;
    }

    .metric.medium {
        color: $warning;
    }

    .metric.high {
        color: $error;
    }
    """

    BINDINGS = [
        Binding("r", "refresh", "Refresh"),
        Binding("q", "quit", "Quit"),
        Binding("ctrl+d", "toggle_dark", "Toggle Dark Mode"),
        Binding("f1", "show_help", "Help"),
    ]

    def __init__(self):
        super().__init__()
        self.provider_monitor = ProviderMonitor()
        self.gpu_panel = GPUPanel()

    def compose(self) -> ComposeResult:
        """Create the dashboard UI."""
        yield Header("AI Backend Enhanced Dashboard")

        # Providers panel (spans 2 columns)
        with Vertical(id="providers-panel"):
            yield Label("[b]AI Providers Status[/b]", classes="panel-title")
            yield ProviderCard("ollama")
            yield ProviderCard("vllm-qwen")
            yield ProviderCard("vllm-dolphin")
            yield ProviderCard("llama_cpp_python")
            yield ProviderCard("llama_cpp_native")

        # GPU panel (right column)
        with Vertical(id="gpu-panel"):
            yield self.gpu_panel

        # System metrics panel (bottom, spans all columns)
        with Vertical(id="metrics-panel"):
            yield Label("[b]System Metrics[/b]", classes="panel-title")
            yield SystemMetrics()

        yield Footer()

    def on_mount(self) -> None:
        """Start auto-refresh when app mounts."""
        self.refresh_data()
        self.set_interval(5.0, self.refresh_data)
        self.set_interval(2.0, self.update_gpu_info)

    def refresh_data(self):
        """Refresh all provider data."""
        self.refresh_providers()

    @work(thread=True)
    async def refresh_providers(self):
        """Refresh provider data in background."""
        statuses = self.provider_monitor.get_all_provider_status()

        for status in statuses:
            self.call_from_thread(self.update_provider_status, status.name, status)

    def update_provider_status(self, provider_name: str, status: ProviderStatus):
        """Update provider status on main thread."""
        try:
            # Find the provider card and update it
            for card in self.query(ProviderCard).results():
                if card.provider_name == provider_name:
                    card.update_status(status)
                    break
        except (AttributeError, StopIteration):
            pass

    def update_gpu_info(self):
        """Update GPU information."""
        self.gpu_panel.update_gpu_info()

    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses."""
        button_id = event.button.id

        if button_id.startswith("start_"):
            provider = button_id.replace("start_", "")
            service_map = {
                "ollama": "ollama.service",
                "vllm-qwen": "vllm.service",
                "vllm-dolphin": "vllm-dolphin.service",
                "llama_cpp_python": "llamacpp-python.service",
                "llama_cpp_native": "llama-cpp-native.service"
            }

            service_name = service_map.get(provider)
            if service_name:
                self.notify(f"Starting {provider}...")
                success = self.provider_monitor.systemctl_command("start", service_name)
                if success:
                    self.notify(f"{provider} started successfully!")
                else:
                    self.notify(f"Failed to start {provider}!", severity="error")

        elif button_id.startswith("stop_"):
            provider = button_id.replace("stop_", "")
            service_map = {
                "ollama": "ollama.service",
                "vllm-qwen": "vllm.service",
                "vllm-dolphin": "vllm-dolphin.service",
                "llama_cpp_python": "llamacpp-python.service",
                "llama_cpp_native": "llama-cpp-native.service"
            }

            service_name = service_map.get(provider)
            if service_name:
                self.notify(f"Stopping {provider}...")
                success = self.provider_monitor.systemctl_command("stop", service_name)
                if success:
                    self.notify(f"{provider} stopped successfully!")
                else:
                    self.notify(f"Failed to stop {provider}!", severity="error")

        elif button_id.startswith("restart_"):
            provider = button_id.replace("restart_", "")
            service_map = {
                "ollama": "ollama.service",
                "vllm-qwen": "vllm.service",
                "vllm-dolphin": "vllm-dolphin.service",
                "llama_cpp_python": "llamacpp-python.service",
                "llama_cpp_native": "llama-cpp-native.service"
            }

            service_name = service_map.get(provider)
            if service_name:
                self.notify(f"Restarting {provider}...")
                success = self.provider_monitor.systemctl_command("restart", service_name)
                if success:
                    self.notify(f"{provider} restarted successfully!")
                else:
                    self.notify(f"Failed to restart {provider}!", severity="error")

        elif button_id.startswith("enable_"):
            provider = button_id.replace("enable_", "")
            service_map = {
                "ollama": "ollama.service",
                "vllm-qwen": "vllm.service",
                "vllm-dolphin": "vllm-dolphin.service",
                "llama_cpp_python": "llamacpp-python.service",
                "llama_cpp_native": "llama-cpp-native.service"
            }

            service_name = service_map.get(provider)
            if service_name:
                self.notify(f"Enabling {provider}...")
                success = self.provider_monitor.systemctl_command("enable", service_name)
                if success:
                    self.notify(f"{provider} enabled successfully!")
                else:
                    self.notify(f"Failed to enable {provider}!", severity="error")

        elif button_id.startswith("disable_"):
            provider = button_id.replace("disable_", "")
            service_map = {
                "ollama": "ollama.service",
                "vllm-qwen": "vllm.service",
                "vllm-dolphin": "vllm-dolphin.service",
                "llama_cpp_python": "llamacpp-python.service",
                "llama_cpp_native": "llama-cpp-native.service"
            }

            service_name = service_map.get(provider)
            if service_name:
                self.notify(f"Disabling {provider}...")
                success = self.provider_monitor.systemctl_command("disable", service_name)
                if success:
                    self.notify(f"{provider} disabled successfully!")
                else:
                    self.notify(f"Failed to disable {provider}!", severity="error")

    def action_refresh(self) -> None:
        """Refresh all data."""
        self.refresh_data()
        self.notify("Dashboard refreshed!")

    def action_show_help(self) -> None:
        """Show help information."""
        self.notify("AI Backend Enhanced Dashboard\n\n"
                   "R - Refresh all data\n"
                   "Q - Quit application\n"
                   "Ctrl+D - Toggle dark mode\n"
                   "F1 - Show this help\n\n"
                   "Click provider buttons to control services:\n"
                   "- Start/Stop: Start or stop the service\n"
                   "- Restart: Restart the service\n"
                   "- Enable/Disable: Enable or disable autostart\n\n"
                   "Dashboard shows real-time provider status and VRAM usage")


if __name__ == "__main__":
    app = AIEnhancedDashboard()
    app.run()
