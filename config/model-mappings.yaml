# Model Routing Configuration
# Defines how LiteLLM routes model requests to providers

# ============================================================================
# EXACT MODEL NAME ROUTING
# ============================================================================

exact_matches:
  # Ollama models (exact name)
  "llama3.1:latest":
    provider: ollama
    priority: primary
    fallback: null
    description: "General-purpose chat model"

  "qwen2.5-coder:7b":
    provider: ollama
    priority: primary
    fallback: null
    description: "Code generation specialist"
  "mythomax-l2-13b-q5_k_m":
    provider: ollama
    priority: primary
    fallback: llama_cpp_python
    description: "Creative writing and roleplay tuned MythoMax L2"
  # vLLM models (AWQ quantized)
  "qwen-coder-vllm":
    provider: vllm-qwen
    priority: primary
    fallback: ollama
    description: "AWQ-quantized 7B code generation model via vLLM"
    backend_model: "Qwen/Qwen2.5-Coder-7B-Instruct-AWQ"

  # Dolphin Uncensored (Mistral-based) - vLLM Dolphin disabled (single instance mode)
  # "dolphin-uncensored-vllm":
  #   provider: vllm-dolphin
  #   priority: primary
  #   fallback: ollama
  #   description: "7B uncensored Mistral-based model via vLLM (AWQ quantized) - DISABLED: use vllm-model-switch.sh to enable"
  #   backend_model: "solidrust/dolphin-2.8-mistral-7b-v02-AWQ"
  # Note: vLLM runs single instance. Enable Dolphin with: ./scripts/vllm-model-switch.sh dolphin

  # llama.cpp models
  "llama-cpp-default":
    provider: llama_cpp_python
    priority: primary
    fallback: llama_cpp_native
    description: "GGUF model via llama.cpp Python bindings"

  # Ollama Cloud models (large models not available locally)
  "deepseek-v3.1:671b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: null
    description: "671B advanced reasoning model via Ollama Cloud"

  "qwen3-coder:480b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "480B code generation model via Ollama Cloud"

  "kimi-k2:1t-cloud":
    provider: ollama_cloud
    priority: secondary
    fallback: null
    description: "1 trillion parameter model via Ollama Cloud (requires valid cloud API key; falls back via default chain)"

  "gpt-oss:120b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "120B open-source GPT via Ollama Cloud"

  "gpt-oss:20b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "20B general chat model via Ollama Cloud"

  "glm-4.6:cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "4.6B general chat model via Ollama Cloud"

  # OpenAI models
  "gpt-4o":
    provider: openai
    priority: primary
    fallback: claude-3-5-sonnet-20241022
    description: "Latest GPT-4 Omni with vision and function calling"

  "gpt-4o-mini":
    provider: openai
    priority: primary
    fallback: claude-3-5-haiku-20241022
    description: "Fast, cost-effective GPT-4 mini"

  "gpt-4-turbo":
    provider: openai
    priority: primary
    fallback: claude-3-opus-20240229
    description: "GPT-4 Turbo with vision"

  "gpt-4":
    provider: openai
    priority: primary
    fallback: claude-3-opus-20240229
    description: "Standard GPT-4"

  "gpt-3.5-turbo":
    provider: openai
    priority: primary
    fallback: llama3.1:latest
    description: "Fast, cost-effective chat model"

  "o1":
    provider: openai
    priority: primary
    fallback: null
    description: "Advanced reasoning model with extended thinking"

  "o1-mini":
    provider: openai
    priority: primary
    fallback: qwen2.5-coder:7b
    description: "Fast reasoning optimized for code"

  # Anthropic models
  "claude-3-5-sonnet-20241022":
    provider: anthropic
    priority: primary
    fallback: gpt-4o
    description: "Latest Claude 3.5 Sonnet with superior coding"

  "claude-3-5-haiku-20241022":
    provider: anthropic
    priority: primary
    fallback: gpt-4o-mini
    description: "Fast, cost-effective Claude"

  "claude-3-opus-20240229":
    provider: anthropic
    priority: primary
    fallback: gpt-4-turbo
    description: "Most capable Claude 3 model"

  "claude-3-sonnet-20240229":
    provider: anthropic
    priority: primary
    fallback: gpt-4
    description: "Balanced Claude 3 model"

  "claude-3-haiku-20240307":
    provider: anthropic
    priority: primary
    fallback: gpt-3.5-turbo
    description: "Fast Claude 3 model"

# ============================================================================
# PATTERN-BASED ROUTING
# ============================================================================

patterns:
  # HuggingFace model paths → vLLM
  - pattern: "^Qwen/Qwen2\\.5-Coder.*"
    provider: vllm-qwen
    fallback: ollama
    description: "Qwen Coder models via vLLM (AWQ quantized)"

  - pattern: "^solidrust/dolphin.*AWQ$"
    provider: vllm-dolphin
    fallback: ollama
    description: "Dolphin uncensored models via vLLM (AWQ quantized)"

  - pattern: "^meta-llama/.*"
    provider: vllm-qwen
    fallback: ollama
    description: "HuggingFace Llama models prefer vLLM for performance"

  - pattern: "^mistralai/.*"
    provider: vllm-dolphin
    fallback: ollama
    description: "Mistral models via vLLM"

  # Ollama-style names → Ollama
  - pattern: ".*:\\d+[bB]$"  # Matches "model:7b", "model:13B", etc.
    provider: ollama
    description: "Ollama naming convention (model:size)"

  # GGUF files → llama.cpp
  - pattern: ".*\\.gguf$"
    provider: llama_cpp_python
    fallback: llama_cpp_native
    description: "GGUF quantized models"

  # OpenAI model patterns
  - pattern: "^gpt-4.*"
    provider: openai
    fallback: anthropic
    description: "GPT-4 family models"

  - pattern: "^gpt-3\\.5.*"
    provider: openai
    fallback: ollama
    description: "GPT-3.5 family models"

  - pattern: "^o1.*"
    provider: openai
    fallback: null
    description: "OpenAI reasoning models"

  # Anthropic model patterns
  - pattern: "^claude-3.*"
    provider: anthropic
    fallback: openai
    description: "Claude 3 family models"

# ============================================================================
# CAPABILITY-BASED ROUTING
# ============================================================================

capabilities:
  code_generation:
    description: "Models specialized for code"
    preferred_models:
      - o1-mini  # OpenAI reasoning for code
      - claude-3-5-sonnet-20241022  # Anthropic coding specialist
      - qwen2.5-coder:7b  # Local option
      - qwen3-coder:480b-cloud  # Ollama Cloud
      - gpt-4o  # OpenAI general purpose
    provider: openai
    routing_strategy: cost_optimized

  analysis:
    description: "Models tuned for structured analysis and planning"
    preferred_models:
      - claude-3-5-sonnet-20241022  # Anthropic excels at analysis
      - gpt-4o  # OpenAI multi-modal analysis
      - claude-3-opus-20240229  # Anthropic most capable
      - qwen2.5-coder:7b  # Local option
      - llama3.1:latest  # Local fallback
    provider: anthropic
    routing_strategy: quality_first

  reasoning:
    description: "Models optimized for multi-step reasoning and problem solving"
    preferred_models:
      - o1  # OpenAI advanced reasoning
      - claude-3-opus-20240229  # Anthropic reasoning specialist
      - deepseek-v3.1:671b-cloud  # Ollama Cloud for advanced reasoning
      - gpt-4-turbo  # OpenAI reasoning
      - llama3.1:latest  # Local fallback
      - qwen-coder-vllm  # Local high-throughput
    provider: openai
    routing_strategy: direct

  creative_writing:
    description: "Storytelling and roleplay optimized models"
    preferred_models:
      - claude-3-opus-20240229  # Anthropic excels at creative writing
      - gpt-4o  # OpenAI creative capabilities
      - mythomax-l2-13b-q5_k_m  # Local creative model
      - llama3.1:latest  # Local fallback
    provider: anthropic
    routing_strategy: quality_first

  # uncensored:  # Disabled: vLLM runs single instance
  #   description: "Uncensored models without content filters"
  #   preferred_models:
  #     - dolphin-uncensored-vllm
  #   provider: vllm-dolphin
  #   routing_strategy: direct

  conversational:
    description: "Natural conversation and general assistance"
    preferred_models:
      - gpt-4o-mini  # OpenAI fast conversational
      - claude-3-5-haiku-20241022  # Anthropic fast
      - gpt-3.5-turbo  # OpenAI cost-effective
      - llama3.1:latest  # Local option
      - mythomax-l2-13b-q5_k_m  # Local creative option
    provider: openai
    routing_strategy: cost_optimized

  vision:
    description: "Multi-modal models with vision capabilities"
    preferred_models:
      - gpt-4o  # OpenAI vision specialist
      - gpt-4-turbo  # OpenAI vision
      - claude-3-5-sonnet-20241022  # Anthropic vision
      - claude-3-opus-20240229  # Anthropic vision
    provider: openai
    routing_strategy: quality_first

  function_calling:
    description: "Models with advanced function/tool calling capabilities"
    preferred_models:
      - gpt-4o  # OpenAI function calling
      - gpt-4-turbo  # OpenAI function calling
      - claude-3-5-sonnet-20241022  # Anthropic tool use
      - claude-3-opus-20240229  # Anthropic tool use
    provider: openai
    routing_strategy: reliability_first

  extended_context:
    description: "Models supporting very large context windows (100K+ tokens)"
    preferred_models:
      - claude-3-5-sonnet-20241022  # 200K context
      - claude-3-opus-20240229  # 200K context
      - o1  # 200K context
      - gpt-4o  # 128K context
      - gpt-4-turbo  # 128K context
    provider: anthropic
    routing_strategy: context_optimized

  high_throughput:
    description: "When you need to handle many concurrent requests"
    min_model_size: "13B"
    provider: vllm-qwen
    routing_strategy: least_loaded

  low_latency:
    description: "Single-request speed priority"
    provider: llama_cpp_native
    fallback: llama_cpp_python
    routing_strategy: fastest_response

  general_chat:
    description: "General conversational AI"
    preferred_models:
      - llama3.1:latest
      - mythomax-l2-13b-q5_k_m
    routing_strategy: usage_based

  large_context:
    description: "Models that can handle large context windows"
    min_context: 8192
    providers:
      - llama_cpp_python  # Configured for 8192 context
      - vllm  # Supports up to 4096 by default
    routing_strategy: most_capacity

# ============================================================================
# LOAD BALANCING CONFIGURATION
# ============================================================================
# Same model available on multiple providers with weighted distribution

load_balancing:
  "llama3.1:latest":
    providers:
      - provider: ollama
        weight: 0.7  # Primary
      - provider: llama_cpp_python
        weight: 0.3  # Backup
    strategy: weighted_round_robin
    description: "Distribute requests with 70% to Ollama, 30% to llama.cpp"

  "general-chat":
    providers:
      - provider: ollama
        weight: 0.6
      - provider: vllm-qwen
        weight: 0.4
    strategy: least_loaded
    description: "Load balance general chat across multiple providers"

# ============================================================================
# FALLBACK CHAINS
# ============================================================================
# Defines fallback order when primary provider fails

fallback_chains:
  "qwen2.5-coder:7b":
    # Ollama code model can fall back to higher throughput or creative options
    chain:
      - qwen-coder-vllm
      - llama3.1:latest

  "qwen3-coder:480b-cloud":
    # Ollama Cloud code model fallback
    chain:
      - qwen2.5-coder:7b
      - qwen-coder-vllm

  "deepseek-v3.1:671b-cloud":
    # Cloud model fallback to local alternatives
    chain:
      - llama3.1:latest
      - qwen-coder-vllm

  "gpt-oss:120b-cloud":
    chain:
      - qwen2.5-coder:7b
      - qwen-coder-vllm

  "gpt-oss:20b-cloud":
    chain:
      - qwen2.5-coder:7b
      - qwen-coder-vllm

  "glm-4.6:cloud":
    chain:
      - llama3.1:latest

  "mythomax-l2-13b-q5_k_m":
    chain:
      - llama3.1:latest
      - qwen2.5-coder:7b

  "qwen-coder-vllm":
    chain:
      - llama3.1:latest

  # "dolphin-uncensored-vllm":  # Disabled: vLLM runs single instance
  #   chain:
  #     - qwen-coder-vllm
  #     - llama3.1:latest

  # OpenAI model fallbacks
  "gpt-4o":
    chain:
      - claude-3-5-sonnet-20241022
      - gpt-4-turbo
      - claude-3-opus-20240229

  "gpt-4o-mini":
    chain:
      - claude-3-5-haiku-20241022
      - gpt-3.5-turbo
      - llama3.1:latest

  "gpt-4-turbo":
    chain:
      - claude-3-opus-20240229
      - gpt-4o
      - qwen2.5-coder:7b

  "gpt-4":
    chain:
      - claude-3-opus-20240229
      - gpt-4-turbo

  "gpt-3.5-turbo":
    chain:
      - llama3.1:latest
      - claude-3-5-haiku-20241022

  "o1":
    chain:
      - claude-3-opus-20240229
      - gpt-4-turbo
      - deepseek-v3.1:671b-cloud

  "o1-mini":
    chain:
      - claude-3-5-sonnet-20241022
      - qwen2.5-coder:7b
      - qwen-coder-vllm

  # Anthropic model fallbacks
  "claude-3-5-sonnet-20241022":
    chain:
      - gpt-4o
      - claude-3-opus-20240229
      - qwen2.5-coder:7b

  "claude-3-5-haiku-20241022":
    chain:
      - gpt-4o-mini
      - gpt-3.5-turbo
      - llama3.1:latest

  "claude-3-opus-20240229":
    chain:
      - gpt-4-turbo
      - claude-3-5-sonnet-20241022
      - o1

  "claude-3-sonnet-20240229":
    chain:
      - gpt-4
      - claude-3-5-sonnet-20241022
      - llama3.1:latest

  "claude-3-haiku-20240307":
    chain:
      - gpt-3.5-turbo
      - claude-3-5-haiku-20241022
      - llama3.1:latest

  default:
    # Default fallback chain for all models with cloud and local options
    chain:
      - gpt-4o-mini  # Fast, cost-effective cloud fallback
      - claude-3-5-haiku-20241022  # Alternative cloud option
      - qwen2.5-coder:7b  # Local primary
      - qwen-coder-vllm  # Local high-throughput
      - llama3.1:latest  # Local general-purpose
      - gpt-oss:120b-cloud  # Ollama Cloud
      - gpt-oss:20b-cloud  # Ollama Cloud fallback

# ============================================================================
# ROUTING RULES
# ============================================================================

routing_rules:
  # Rule precedence (highest to lowest):
  # 1. Exact model name match
  # 2. Capability-based routing
  # 3. Pattern matching
  # 4. Default provider

  default_provider: ollama
  default_fallback: llama_cpp_python

  request_metadata_routing:
    # Route based on request headers/metadata
    high_priority_requests:
      provider: vllm-qwen
      condition: header.x-priority == "high"

    batch_requests:
      provider: vllm-qwen
      condition: batch_size > 1

    streaming_requests:
      provider: ollama  # Best streaming support
      condition: stream == true

  model_size_routing:
    # Automatic provider selection based on model size
    - size: "< 8B"
      provider: ollama
      reason: "Small models work well with Ollama"

    - size: "8B - 13B"
      provider: ollama
      fallback: llama_cpp_python
      reason: "Medium models, Ollama preferred"

    - size: "> 13B"
      provider: vllm-qwen
      fallback: ollama
      reason: "Large models benefit from vLLM batching"

# ============================================================================
# SPECIAL ROUTING CASES
# ============================================================================

special_cases:
  first_request_routing:
    description: "Cold start optimization"
    strategy: prefer_warm_providers
    warm_check_timeout_ms: 100

  rate_limited_fallback:
    description: "Automatic fallback when provider hits rate limit"
    enabled: true
    fallback_duration_seconds: 60

  error_based_routing:
    description: "Avoid providers with recent errors"
    enabled: true
    error_threshold: 3
    cooldown_seconds: 300

  geographic_routing:
    description: "Route based on provider location (future)"
    enabled: false
    prefer_local: true

# ============================================================================
# METADATA
# ============================================================================

metadata:
  version: "2.0"
  last_updated: "2025-11-09"
  total_routing_rules: 50+
  supported_routing_strategies:
    - exact_match
    - pattern_match
    - capability_based
    - load_balanced
    - fallback_chain
    - cost_optimized
    - quality_first
    - context_optimized
  changes:
    - "Added OpenAI provider (GPT-4o, GPT-4 Turbo, GPT-3.5, o1 models)"
    - "Added Anthropic provider (Claude 3.5 Sonnet, Opus, Haiku models)"
    - "Enhanced capability routing with vision, function calling, and extended context"
    - "Implemented cross-provider fallback chains (OpenAI ↔ Anthropic ↔ Local)"
    - "Added new routing strategies: cost_optimized, quality_first, context_optimized"
    - "Updated default fallback chain to prioritize cloud → local for reliability"

notes: |
  This configuration is consumed by LiteLLM's router.

  When adding new models:
  1. Add exact match entry if it has a canonical name
  2. Consider which capability category it belongs to
  3. Define fallback chain for reliability
  4. Test routing with validation script

  When adding new providers:
  1. Update providers.yaml first
  2. Add routing rules here
  3. Update litellm-unified.yaml
  4. Test end-to-end
