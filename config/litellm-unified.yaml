# ============================================================================
# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# ============================================================================
#
# Generated by: scripts/generate-litellm-config.py
# Source files: config/providers.yaml, config/model-mappings.yaml
# Generated at: 2025-10-29T23:46:58.497896
# Version: git-e355954
#
# To modify this configuration:
#   1. Edit config/providers.yaml or config/model-mappings.yaml
#   2. Run: python3 scripts/generate-litellm-config.py
#   3. Validate: python3 scripts/validate-config-schema.py
#
# ============================================================================

model_list:
  - model_name: llama3.1:latest
    litellm_params:
      model: ollama/llama3.1:latest
      api_base: http://127.0.0.1:11434
    model_info:
      tags:
        - general_chat
        - 8b
        - q4_k_m
      provider: ollama
  - model_name: qwen2.5-coder:7b
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://127.0.0.1:11434
    model_info:
      tags:
        - code_generation
        - 7.6b
        - q4_k_m
      provider: ollama
  - model_name: mythomax-l2-13b-q5_k_m
    litellm_params:
      model: ollama/mythomax-l2-13b-q5_k_m
      api_base: http://127.0.0.1:11434
      extra_body:
        options:
          num_gpu_layers: -1
    model_info:
      tags:
        - creative_writing
        - 13b
        - q5_k_m
      provider: ollama
  - model_name: qwen-coder-vllm
    litellm_params:
      model: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
      api_base: http://127.0.0.1:8001/v1
      custom_llm_provider: openai
      stream: true
      api_key: not-needed
    model_info:
      tags:
        - code_generation
        - 7b
        - awq
      provider: vllm-qwen
      context_length: 4096
  - model_name: dolphin-uncensored-vllm
    litellm_params:
      model: solidrust/dolphin-2.8-mistral-7b-v02-AWQ
      api_base: http://127.0.0.1:8002/v1
      custom_llm_provider: openai
      stream: true
      api_key: not-needed
    model_info:
      tags:
        - conversational
        - 7b
        - awq
      provider: vllm-dolphin
      context_length: 4096
  - model_name: deepseek-v3.1:671b-cloud
    litellm_params:
      model: ollama_chat/deepseek-v3.1:671b-cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - advanced_reasoning
        - 671b
      provider: ollama_cloud
  - model_name: qwen3-coder:480b-cloud
    litellm_params:
      model: ollama_chat/qwen3-coder:480b-cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - code_generation
        - 480b
      provider: ollama_cloud
  - model_name: kimi-k2:1t-cloud
    litellm_params:
      model: ollama_chat/kimi-k2:1t-cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - advanced_reasoning
        - 1t
      provider: ollama_cloud
  - model_name: gpt-oss:120b-cloud
    litellm_params:
      model: ollama_chat/gpt-oss:120b-cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - general_chat
        - 120b
      provider: ollama_cloud
  - model_name: gpt-oss:20b-cloud
    litellm_params:
      model: ollama_chat/gpt-oss:20b-cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - general_chat
        - 20b
      provider: ollama_cloud
  - model_name: glm-4.6:cloud
    litellm_params:
      model: ollama_chat/glm-4.6:cloud
      api_base: https://ollama.com
      api_key: os.environ/OLLAMA_API_KEY
    model_info:
      tags:
        - general_chat
        - 4.6b
      provider: ollama_cloud
litellm_settings:
  request_timeout: 60
  stream_timeout: 120
  num_retries: 3
  timeout: 300
  cache: true
  cache_params:
    type: redis
    host: 127.0.0.1
    port: 6379
    ttl: 3600
  set_verbose: true
  json_logs: true
router_settings:
  routing_strategy: simple-shuffle
  model_group_alias:
    code_generation:
      - qwen2.5-coder:7b
    analysis:
      - qwen2.5-coder:7b
    reasoning:
      - llama3.1:latest
    creative_writing:
      - mythomax-l2-13b-q5_k_m
    uncensored:
      - dolphin-uncensored-vllm
    conversational:
      - dolphin-uncensored-vllm
    general_chat:
      - llama3.1:latest
  allowed_fails: 3
  num_retries: 2
  timeout: 30
  cooldown_time: 60
  enable_pre_call_checks: true
  redis_host: 127.0.0.1
  redis_port: 6379
  fallbacks:
    - qwen2.5-coder:7b:
        - qwen-coder-vllm
        - llama3.1:latest
    - qwen3-coder:480b-cloud:
        - qwen2.5-coder:7b
        - qwen-coder-vllm
    - deepseek-v3.1:671b-cloud:
        - llama3.1:latest
        - qwen-coder-vllm
    - gpt-oss:120b-cloud:
        - qwen2.5-coder:7b
        - qwen-coder-vllm
    - gpt-oss:20b-cloud:
        - qwen2.5-coder:7b
        - qwen-coder-vllm
    - glm-4.6:cloud:
        - llama3.1:latest
    - mythomax-l2-13b-q5_k_m:
        - llama3.1:latest
        - qwen2.5-coder:7b
    - qwen-coder-vllm:
        - llama3.1:latest
    - dolphin-uncensored-vllm:
        - qwen-coder-vllm
        - llama3.1:latest
    - default:
        - qwen2.5-coder:7b
        - qwen-coder-vllm
        - dolphin-uncensored-vllm
        - llama3.1:latest
        - gpt-oss:120b-cloud
        - gpt-oss:20b-cloud
    - code_generation:
        - qwen3-coder:480b-cloud
    - analysis:
        - llama3.1:latest
    - reasoning:
        - deepseek-v3.1:671b-cloud
        - qwen-coder-vllm
    - creative_writing:
        - llama3.1:latest
    - conversational:
        - llama3.1:latest
        - mythomax-l2-13b-q5_k_m
    - general_chat:
        - mythomax-l2-13b-q5_k_m
  lab_extensions:
    capabilities:
      code_generation:
        description: Models specialized for code
        preferred_models:
          - qwen2.5-coder:7b
          - qwen3-coder:480b-cloud
        provider: ollama
        routing_strategy: load_balance
      analysis:
        description: Models tuned for structured analysis and planning
        preferred_models:
          - qwen2.5-coder:7b
          - llama3.1:latest
        provider: ollama
        routing_strategy: usage_based
      reasoning:
        description: Models optimized for multi-step reasoning and problem solving
        preferred_models:
          - llama3.1:latest
          - deepseek-v3.1:671b-cloud
          - qwen-coder-vllm
        provider: ollama
        routing_strategy: direct
      creative_writing:
        description: Storytelling and roleplay optimized models
        preferred_models:
          - mythomax-l2-13b-q5_k_m
          - llama3.1:latest
        provider: ollama
        routing_strategy: usage_based
      uncensored:
        description: Uncensored models without content filters
        preferred_models:
          - dolphin-uncensored-vllm
        provider: vllm-dolphin
        routing_strategy: direct
      conversational:
        description: Natural conversation and general assistance
        preferred_models:
          - dolphin-uncensored-vllm
          - llama3.1:latest
          - mythomax-l2-13b-q5_k_m
        routing_strategy: usage_based
      high_throughput:
        description: When you need to handle many concurrent requests
        min_model_size: 13B
        provider: vllm-qwen
        routing_strategy: least_loaded
      low_latency:
        description: Single-request speed priority
        provider: llama_cpp_native
        fallback: llama_cpp_python
        routing_strategy: fastest_response
      general_chat:
        description: General conversational AI
        preferred_models:
          - llama3.1:latest
          - mythomax-l2-13b-q5_k_m
        routing_strategy: usage_based
      large_context:
        description: Models that can handle large context windows
        min_context: 8192
        providers:
          - llama_cpp_python
          - vllm
        routing_strategy: most_capacity
    pattern_routes:
      - pattern: ^Qwen/Qwen2\.5-Coder.*
        provider: vllm-qwen
        fallback: ollama
        description: Qwen Coder models via vLLM (AWQ quantized)
      - pattern: ^solidrust/dolphin.*AWQ$
        provider: vllm-dolphin
        fallback: ollama
        description: Dolphin uncensored models via vLLM (AWQ quantized)
      - pattern: ^meta-llama/.*
        provider: vllm-qwen
        fallback: ollama
        description: HuggingFace Llama models prefer vLLM for performance
      - pattern: ^mistralai/.*
        provider: vllm-dolphin
        fallback: ollama
        description: Mistral models via vLLM
      - pattern: .*:\d+[bB]$
        provider: ollama
        description: Ollama naming convention (model:size)
      - pattern: .*\.gguf$
        provider: llama_cpp_python
        fallback: llama_cpp_native
        description: GGUF quantized models
    load_balancing:
      llama3.1:latest:
        providers:
          - provider: ollama
            weight: 0.7
          - provider: llama_cpp_python
            weight: 0.3
        strategy: weighted_round_robin
        description: Distribute requests with 70% to Ollama, 30% to llama.cpp
      general-chat:
        providers:
          - provider: ollama
            weight: 0.6
          - provider: vllm-qwen
            weight: 0.4
        strategy: least_loaded
        description: Load balance general chat across multiple providers
    routing_rules:
      default_provider: ollama
      default_fallback: llama_cpp_python
      request_metadata_routing:
        high_priority_requests:
          provider: vllm-qwen
          condition: header.x-priority == "high"
        batch_requests:
          provider: vllm-qwen
          condition: batch_size > 1
        streaming_requests:
          provider: ollama
          condition: stream == true
      model_size_routing:
        - size: < 8B
          provider: ollama
          reason: Small models work well with Ollama
        - size: 8B - 13B
          provider: ollama
          fallback: llama_cpp_python
          reason: Medium models, Ollama preferred
        - size: '> 13B'
          provider: vllm-qwen
          fallback: ollama
          reason: Large models benefit from vLLM batching
    special_cases:
      first_request_routing:
        description: Cold start optimization
        strategy: prefer_warm_providers
        warm_check_timeout_ms: 100
      rate_limited_fallback:
        description: Automatic fallback when provider hits rate limit
        enabled: true
        fallback_duration_seconds: 60
      error_based_routing:
        description: Avoid providers with recent errors
        enabled: true
        error_threshold: 3
        cooldown_seconds: 300
      geographic_routing:
        description: Route based on provider location (future)
        enabled: false
        prefer_local: true
server_settings:
  port: 4000
  host: 0.0.0.0
  cors:
    enabled: true
    allowed_origins:
      - http://localhost:*
      - http://127.0.0.1:*
      - http://[::1]:*
  health_check_endpoint: /health
rate_limit_settings:
  enabled: true
  limits:
    llama3.1:latest:
      rpm: 100
      tpm: 50000
    qwen2.5-coder:7b:
      rpm: 100
      tpm: 50000
    mythomax-l2-13b-q5_k_m:
      rpm: 100
      tpm: 50000
    qwen-coder-vllm:
      rpm: 50
      tpm: 100000
    dolphin-uncensored-vllm:
      rpm: 50
      tpm: 100000
    deepseek-v3.1:671b-cloud:
      rpm: 100
      tpm: 50000
    qwen3-coder:480b-cloud:
      rpm: 100
      tpm: 50000
    kimi-k2:1t-cloud:
      rpm: 100
      tpm: 50000
    gpt-oss:120b-cloud:
      rpm: 100
      tpm: 50000
    gpt-oss:20b-cloud:
      rpm: 100
      tpm: 50000
    glm-4.6:cloud:
      rpm: 100
      tpm: 50000
general_settings:
  background_health_checks: false
  health_check_interval: 300
  health_check_details: false
  '# Master Key Authentication':
  '# Uncomment to enable':
  '# master_key': ${LITELLM_MASTER_KEY}
  '# Salt Key for DB encryption':
  '# salt_key': ${LITELLM_SALT_KEY}
debug: false
debug_router: false
test_mode: false
