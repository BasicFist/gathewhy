# AI Backend Unified - Architecture Consolidation

## Overview

This document provides a comprehensive view of the consolidated AI backend architecture after removing the duplicate "gateway" artifact that was generated by an AI analysis agent.

## Architecture Evolution

### Before Consolidation
- **Main Backend**: `/home/miko/LAB/ai/backend/ai-backend-unified/` (primary unified infrastructure)
- **Duplicate Gateway**: `/home/miko/LAB/ai/services/litellm-gateway/` (AI-generated artifact with basic config)

### After Consolidation
- **Single Backend**: `/home/miko/LAB/ai/backend/ai-backend-unified/` (sole unified infrastructure)
- **Duplicate Removed**: `/home/miko/LAB/ai/services/litellm-gateway/` (deleted to prevent confusion)

## Current Architecture

The AI Backend Unified is a single, comprehensive entry point for all LLM providers in the LAB ecosystem:

```
┌─────────────────────────────────────────────────────────┐
│            LAB AI Backend Infrastructure                │
│                                                         │
│  ┌──────────────────────────────────────────────────┐  │
│  │     Single Entry Point: LiteLLM :4000            │  │
│  │  OpenAI-Compatible API + MCP Protocol            │  │
│  └────────────────┬─────────────────────────────────┘  │
│                   │                                     │
│         ┌─────────┼──────────┬──────────┐              │
│         │         │          │          │              │
│         ▼         ▼          ▼          ▼              │
│    ┌────────┐ ┌──────┐ ┌────────┐ ┌─────────┐         │
│    │ Ollama │ │llama │ │ vLLM   │ │ Future  │         │
│    │ :11434 │ │.cpp  │ │ :8001  │ │Providers│         │
│    └────────┘ └──────┘ └────────┘ └─────────┘         │
└─────────────────────────────────────────────────────────┘
```

## Key Components

### 1. Configuration Management (Source of Truth)
- `config/providers.yaml` - Provider registry with models and settings
- `config/model-mappings.yaml` - Model routing rules and fallback chains  
- `config/litellm-unified.yaml` - Generated LiteLLM configuration (AUTO-GENERATED)

### 2. Dashboard & Monitoring
- Enhanced Textual TUI dashboard (`scripts/ai-dashboard-enhanced.py`)
- Curses-based dashboard for SSH sessions (`scripts/ptui_dashboard.py`)
- Configuration validation and hot-reload capability

### 3. Provider Integration
- **Ollama**: Local models at :11434 (llama3.1:8b, qwen2.5-coder:7b)
- **llama.cpp**: Python and native servers at :8000/:8080 (GGUF models)
- **vLLM**: High-throughput at :8001 (Qwen2.5-Coder-7B-Instruct-AWQ)

### 4. Observability Stack
- Prometheus + Grafana monitoring (`monitoring/` directory)
- 5 pre-built dashboards for performance tracking
- Request tracing and performance profiling
- Load testing suite (Locust + k6)

## Configuration Workflow

The unified backend follows a configuration-driven approach:

1. **Source Configuration**: Edit `config/providers.yaml` and `config/model-mappings.yaml`
2. **Automatic Generation**: `scripts/generate-litellm-config.py` creates `litellm-unified.yaml`
3. **Validation**: Pydantic schemas ensure configuration consistency
4. **Hot-Reload**: `scripts/reload-litellm-config.sh` safely applies changes

## API Usage

All LAB projects should use the unified endpoint:

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:4000",  # LiteLLM unified gateway
    api_key="not-needed"  # pragma: allowlist secret
)

# Route to any provider transparently
response = client.chat.completions.create(
    model="llama3.1:8b",  # Routes to Ollama
    # model="llama-cpp-llama3.1",  # Routes to llama.cpp
    # model="qwen-coder-vllm",     # Routes to vLLM
    messages=[{"role": "user", "content": "Hello"}]
)
```

## Dashboard Selection

The project provides three dashboard implementations for different use cases:

| Dashboard | Best For | Technology | Features |
|-----------|----------|------------|----------|
| **Textual** | Local development, full-featured monitoring | Textual (Python) | GPU utilization, service control, event logging |
| **Curses** | Remote SSH sessions, universal compatibility | Python curses (stdlib) | Basic monitoring, authenticated access |
| **Grafana** | Production monitoring, web access | Grafana (web interface) | Historical metrics, alerting, professional dashboards |

## Monitoring & Observability

The comprehensive observability stack includes:

1. **Monitoring**: Prometheus + Grafana with 5 pre-built dashboards
2. **Debugging**: JSON structured logging and request ID tracing
3. **Profiling**: Performance analysis with latency and throughput testing
4. **Load Testing**: Locust and k6 with multiple test scenarios

## Quality Assurance

The system includes comprehensive validation:

- Automatic model name consistency validation
- Configuration hot-reload with validation and rollback
- Redis cache management with monitoring
- Port conflict detection and resolution
- 75+ automated tests (unit, integration, contract)

## Security & Access Control

- Bearer token authentication (when required)
- CORS configuration for controlled origins
- Rate limiting to prevent abuse
- Secure credential management via environment variables

## Future Enhancements

- OpenAI/Anthropic provider integration
- Enhanced caching strategies (semantic caching)
- Request queuing and prioritization
- Advanced load balancing algorithms
- Alert integration (PagerDuty, Slack)

## Troubleshooting

### Common Issues
- **Provider not responding**: Check individual provider services before the gateway
- **Invalid configuration**: Use hot-reload validation tools to check before applying
- **Port conflicts**: Run `scripts/check-port-conflicts.sh` to diagnose
- **Redis cache issues**: Use `scripts/monitor-redis-cache.sh` to check cache health

### Health Checks
- API endpoints: `curl http://localhost:4000/health`
- Individual providers: Check each provider's endpoint directly
- Dashboard: `python3 scripts/ai-dashboard` for TUI monitoring

---
**Document Version**: 1.0
**Last Updated**: 2025-11-06
**Status**: Active - Reflects consolidated architecture