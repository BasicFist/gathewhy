#!/usr/bin/env bash
# Provider TUI (ptui) - AI Backend Unified Command Center
# Comprehensive management interface for LiteLLM, vLLM, Ollama, and llama.cpp

set -euo pipefail

readonly VERSION="2.0.0"
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
readonly INVOCATION_NAME="$(basename "$0")"

# Service endpoints
readonly LITELLM_URL="http://localhost:4000"
readonly OLLAMA_URL="http://localhost:11434"
readonly LLAMACPP_PYTHON_URL="http://localhost:8000"
readonly LLAMACPP_NATIVE_URL="http://localhost:8080"
readonly VLLM_URL="http://localhost:8001"
readonly DEFAULT_HTTP_TIMEOUT=15
readonly AUTO_REFRESH_SECONDS=5
readonly PYTHON_BIN="${PYTHON:-python3}"

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
WHITE='\033[1;37m'
GRAY='\033[0;90m'
NC='\033[0m' # No Color
BOLD='\033[1m'

# Icons
ICON_RUNNING="‚úÖ"
ICON_STOPPED="‚ùå"
ICON_WARNING="‚ö†Ô∏è "
ICON_INFO="‚ÑπÔ∏è "
ICON_ROCKET="üöÄ"
ICON_GEAR="‚öôÔ∏è "
ICON_CHART="üìä"
ICON_SERVER="üñ•Ô∏è "
ICON_MODEL="ü§ñ"
ICON_HEALTH="üíö"

# Logging
log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[‚úì]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[‚ö†]${NC} $1"; }
log_error() { echo -e "${RED}[‚úó]${NC} $1"; }
log_header() { echo -e "${BOLD}${CYAN}$1${NC}"; }

pause() {
    read -rp "Press Enter to continue..." _
}

require_command() {
    local command_name=$1
    local hint=${2:-}

    if ! command -v "$command_name" >/dev/null 2>&1; then
        log_error "Required command not found: $command_name"
        [[ -n "$hint" ]] && echo -e "${YELLOW}${hint}${NC}"
        exit 1
    fi
}

ensure_dependencies() {
    require_command curl "Install curl to allow endpoint checks."
    require_command jq "Install jq (e.g. 'sudo apt install jq') for JSON parsing."
}

show_help() {
    cat <<EOF
PTUI - Provider TUI v$VERSION
AI Backend Unified Command Center

Usage:
  $INVOCATION_NAME              Interactive TUI mode
  $INVOCATION_NAME status       Show service status
  $INVOCATION_NAME models       List all models
  $INVOCATION_NAME health       Run health check
  $INVOCATION_NAME vllm <cmd>   vLLM management (status|qwen|dolphin|stop)
  $INVOCATION_NAME test         Test endpoints interactively
  $INVOCATION_NAME help         Show this help message
  $INVOCATION_NAME version      Show current version

Interactive mode provides the full menu-driven interface described in docs/ptui-user-guide.md.
EOF
}

show_version() {
    echo "PTUI v$VERSION"
}

# Clear screen and show header
show_header() {
    clear 2>/dev/null || true
    echo -e "${BOLD}${CYAN}"
    cat << 'EOF'
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                  AI Backend Unified - PTUI                    ‚ïë
‚ïë              Provider Command & Control Center                ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF
    echo -e "${NC}"
    echo -e "${GRAY}Version: $VERSION | Project: ai-backend-unified${NC}"
    echo
}

# Check if service is running
check_service_health() {
    local name=$1
    local url=$2
    local endpoint=${3:-"/health"}
    local timeout=${4:-$DEFAULT_HTTP_TIMEOUT}

    if curl -s -f --max-time "$timeout" "$url$endpoint" > /dev/null 2>&1; then
        echo -e "${GREEN}${ICON_RUNNING}${NC} ${BOLD}$name${NC} ${GRAY}($url)${NC}"
        return 0
    else
        echo -e "${RED}${ICON_STOPPED}${NC} ${BOLD}$name${NC} ${GRAY}($url)${NC}"
        return 1
    fi
}

# Get vLLM current model
get_vllm_model() {
    if pgrep -f "vllm serve" > /dev/null 2>&1; then
        local pid=$(pgrep -f "vllm serve")
        local model=$(ps -p "$pid" -o args= | grep -oP '(?<=vllm serve )[^ ]+' || echo "unknown")
        echo "$model"
    else
        echo "not running"
    fi
}

# Service status overview
show_status() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Service Status ‚îÅ‚îÅ‚îÅ"
    echo

    # LiteLLM Gateway
    if check_service_health "LiteLLM Gateway" "$LITELLM_URL" "/health"; then
        litellm_status=0
    else
        litellm_status=1
    fi

    # Ollama
    if check_service_health "Ollama" "$OLLAMA_URL" "/api/tags"; then
        ollama_status=0
    else
        ollama_status=1
    fi

    # llama.cpp Python
    if check_service_health "llama.cpp (Python)" "$LLAMACPP_PYTHON_URL" "/v1/models"; then
        llamacpp_py_status=0
    else
        llamacpp_py_status=1
    fi

    # llama.cpp Native
    if check_service_health "llama.cpp (Native)" "$LLAMACPP_NATIVE_URL" "/v1/models"; then
        llamacpp_native_status=0
    else
        llamacpp_native_status=1
    fi

    # vLLM
    if check_service_health "vLLM" "$VLLM_URL" "/v1/models"; then
        vllm_model=$(get_vllm_model)
        echo -e "  ${GRAY}‚îî‚îÄ Model: ${WHITE}$vllm_model${NC}"
        vllm_status=0
    else
        vllm_status=1
    fi

    echo
    log_header "‚îÅ‚îÅ‚îÅ Quick Stats ‚îÅ‚îÅ‚îÅ"
    echo

    # Count running services
    local running=0
    [[ $litellm_status -eq 0 ]] && ((running++))
    [[ $ollama_status -eq 0 ]] && ((running++))
    [[ $llamacpp_py_status -eq 0 ]] && ((running++))
    [[ $llamacpp_native_status -eq 0 ]] && ((running++))
    [[ $vllm_status -eq 0 ]] && ((running++))

    echo -e "${ICON_SERVER} Services Running: ${GREEN}${running}/5${NC}"

    # Show available models if LiteLLM is up
    if [[ $litellm_status -eq 0 ]]; then
        local model_count=$(curl -s "$LITELLM_URL/v1/models" 2>/dev/null | jq -r '.data | length' 2>/dev/null || echo "0")
        echo -e "${ICON_MODEL} Models Available: ${GREEN}${model_count}${NC}"
    fi

    echo
    return 0
}

# List all available models
list_models() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Available Models ‚îÅ‚îÅ‚îÅ"
    echo

    if ! curl -s -f "$LITELLM_URL/v1/models" > /dev/null 2>&1; then
        log_error "LiteLLM Gateway is not running"
        return 1
    fi

    echo -e "${BOLD}LiteLLM Models:${NC}"
    curl -s "$LITELLM_URL/v1/models" | jq -r '.data[] | "  ‚Ä¢ \(.id)"' 2>/dev/null || echo "  (unable to fetch)"
    echo

    if curl -s -f "$OLLAMA_URL/api/tags" > /dev/null 2>&1; then
        echo -e "${BOLD}Ollama Models:${NC}"
        curl -s "$OLLAMA_URL/api/tags" | jq -r '.models[]? | "  ‚Ä¢ \(.name) (\(.size / 1024 / 1024 / 1024 | floor)GB)"' 2>/dev/null || echo "  (none)"
        echo
    fi

    if pgrep -f "vllm serve" > /dev/null 2>&1; then
        echo -e "${BOLD}vLLM Current Model:${NC}"
        local vllm_model=$(get_vllm_model)
        echo -e "  ${GREEN}‚Ä¢${NC} $vllm_model"
        echo
    fi

    return 0
}

# Health check with details
health_check() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Comprehensive Health Check ‚îÅ‚îÅ‚îÅ"
    echo

    local all_healthy=true

    # Test each endpoint
    echo -e "${BOLD}Testing Endpoints:${NC}"

    # LiteLLM
    if curl -s -f -m 5 "$LITELLM_URL/health" > /dev/null 2>&1; then
        log_success "LiteLLM health endpoint"

        # Test a completion
        local test_response=$(curl -s -X POST "$LITELLM_URL/v1/chat/completions" \
            -H "Content-Type: application/json" \
            -d '{"model": "llama3.1:latest", "messages": [{"role": "user", "content": "Hi"}], "max_tokens": 5}' \
            2>/dev/null | jq -r '.choices[0].message.content // "ERROR"' 2>/dev/null)

        if [[ "$test_response" != "ERROR" && -n "$test_response" ]]; then
            log_success "LiteLLM completion test"
        else
            log_warning "LiteLLM completion test failed"
            all_healthy=false
        fi
    else
        log_error "LiteLLM health endpoint"
        all_healthy=false
    fi

    # Ollama
    if curl -s -f -m 5 "$OLLAMA_URL/api/tags" > /dev/null 2>&1; then
        log_success "Ollama API"
    else
        log_warning "Ollama API (optional)"
    fi

    # vLLM
    if curl -s -f -m 5 "$VLLM_URL/v1/models" > /dev/null 2>&1; then
        log_success "vLLM API"
    else
        log_warning "vLLM API (optional)"
    fi

    echo
    if $all_healthy; then
        echo -e "${GREEN}${ICON_HEALTH} System Status: HEALTHY${NC}"
    else
        echo -e "${YELLOW}${ICON_WARNING} System Status: DEGRADED${NC}"
    fi
    echo
    return 0
}

# vLLM model management
vllm_menu() {
    while true; do
        show_header
        log_header "‚îÅ‚îÅ‚îÅ vLLM Model Management ‚îÅ‚îÅ‚îÅ"
        echo

        local current_model=$(get_vllm_model)
        echo -e "${BOLD}Current Model:${NC} ${CYAN}$current_model${NC}"
        echo

        echo "1. Switch to Qwen Coder (code generation)"
        echo "2. Switch to Dolphin (uncensored conversational)"
        echo "3. Check vLLM status"
        echo "4. Stop vLLM"
        echo "5. View vLLM logs"
        echo "b. Back to main menu"
        echo
        read -rp "Select option: " choice

        case $choice in
            1)
                log_info "Switching to Qwen Coder..."
                "$SCRIPT_DIR/vllm-model-switch.sh" qwen
                pause
                ;;
            2)
                log_info "Switching to Dolphin..."
                "$SCRIPT_DIR/vllm-model-switch.sh" dolphin
                pause
                ;;
            3)
                "$SCRIPT_DIR/vllm-model-switch.sh" status
                pause
                ;;
            4)
                "$SCRIPT_DIR/vllm-model-switch.sh" stop
                pause
                ;;
            5)
                echo
                log_info "Last 50 lines of vLLM logs:"
                echo
                if [[ -f /tmp/vllm-qwen-coder.log ]]; then
                    tail -50 /tmp/vllm-qwen-coder.log
                elif [[ -f /tmp/vllm-dolphin.log ]]; then
                    tail -50 /tmp/vllm-dolphin.log
                else
                    echo "No logs found"
                fi
                pause
                ;;
            b|B)
                break
                ;;
            *)
                log_error "Invalid option"
                sleep 1
                ;;
        esac
    done
}

# Configuration viewer
view_config() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Configuration Viewer ‚îÅ‚îÅ‚îÅ"
    echo

    echo "1. LiteLLM Unified Config"
    echo "2. Model Mappings"
    echo "3. Provider Registry"
    echo "4. vLLM Model Info"
    echo "b. Back"
    echo
    read -rp "Select config to view: " choice

    case $choice in
        1)
            if [[ -f "$PROJECT_ROOT/config/litellm-unified.yaml" ]]; then
                less "$PROJECT_ROOT/config/litellm-unified.yaml"
            else
                log_error "Config file not found"
            fi
            ;;
        2)
            if [[ -f "$PROJECT_ROOT/config/model-mappings.yaml" ]]; then
                less "$PROJECT_ROOT/config/model-mappings.yaml"
            else
                log_error "Config file not found"
            fi
            ;;
        3)
            if [[ -f "$PROJECT_ROOT/config/providers.yaml" ]]; then
                less "$PROJECT_ROOT/config/providers.yaml"
            else
                log_error "Config file not found"
            fi
            ;;
        4)
            show_header
            log_header "‚îÅ‚îÅ‚îÅ vLLM Model Information ‚îÅ‚îÅ‚îÅ"
            echo
            if pgrep -f "vllm serve" > /dev/null; then
                ps aux | grep "vllm serve" | grep -v grep
            else
                echo "vLLM is not running"
            fi
            pause
            ;;
        b|B)
            return
            ;;
    esac
}

# Service logs
view_logs() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Service Logs ‚îÅ‚îÅ‚îÅ"
    echo

    echo "1. LiteLLM logs (journalctl)"
    echo "2. vLLM logs"
    echo "3. System logs (dmesg)"
    echo "b. Back"
    echo
    read -rp "Select logs to view: " choice

    case $choice in
        1)
            echo
            log_info "Last 100 lines of LiteLLM service:"
            echo
            journalctl --user -u litellm.service -n 100 --no-pager
            pause
            ;;
        2)
            echo
            log_info "vLLM logs:"
            echo
            if [[ -f /tmp/vllm-qwen-coder.log ]]; then
                tail -100 /tmp/vllm-qwen-coder.log
            elif [[ -f /tmp/vllm-dolphin.log ]]; then
                tail -100 /tmp/vllm-dolphin.log
            else
                echo "No vLLM logs found"
            fi
            pause
            ;;
        3)
            dmesg | tail -50
            pause
            ;;
        b|B)
            return
            ;;
    esac
}

# Test endpoints
test_endpoints() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Endpoint Testing ‚îÅ‚îÅ‚îÅ"
    echo

    echo "1. Test LiteLLM completion"
    echo "2. Test Ollama"
    echo "3. Test vLLM"
    echo "4. Test all endpoints"
    echo "b. Back"
    echo
    read -rp "Select test: " choice

    case $choice in
        1)
            echo
            log_info "Testing LiteLLM completion..."
            curl -X POST "$LITELLM_URL/v1/chat/completions" \
                -H "Content-Type: application/json" \
                -d '{"model": "llama3.1:latest", "messages": [{"role": "user", "content": "Say hello"}], "max_tokens": 20}' \
                | jq
            pause
            ;;
        2)
            echo
            log_info "Testing Ollama..."
            curl -s "$OLLAMA_URL/api/tags" | jq
            pause
            ;;
        3)
            echo
            log_info "Testing vLLM..."
            curl -s "$VLLM_URL/v1/models" | jq
            pause
            ;;
        4)
            health_check
            pause
            ;;
        b|B)
            return
            ;;
    esac
}

# Quick actions
quick_actions() {
    show_header
    log_header "‚îÅ‚îÅ‚îÅ Quick Actions ‚îÅ‚îÅ‚îÅ"
    echo

    echo "1. Restart LiteLLM service"
    echo "2. Run validation script"
    echo "3. Check GPU status"
    echo "4. Check port usage"
    echo "5. Kill all vLLM processes"
    echo "b. Back"
    echo
    read -rp "Select action: " choice

    case $choice in
        1)
            log_info "Restarting LiteLLM service..."
            systemctl --user restart litellm.service
            sleep 2
            systemctl --user status litellm.service --no-pager
            pause
            ;;
        2)
            if [[ -f "$PROJECT_ROOT/scripts/validate-unified-backend.sh" ]]; then
                "$PROJECT_ROOT/scripts/validate-unified-backend.sh"
            else
                log_error "Validation script not found"
            fi
            pause
            ;;
        3)
            nvidia-smi 2>/dev/null || log_warning "nvidia-smi not available"
            pause
            ;;
        4)
            echo
            log_info "Port usage:"
            echo
            lsof -i :4000 -i :8001 -i :11434 -i :8000 -i :8080 2>/dev/null || echo "No processes found"
            pause
            ;;
        5)
            log_warning "Killing all vLLM processes..."
            pkill -f "vllm serve" && log_success "vLLM processes killed" || log_info "No vLLM processes found"
            pause
            ;;
        b|B)
            return
            ;;
    esac
}

# Provider management
provider_management() {
    while true; do
        show_header
        log_header "‚îÅ‚îÅ‚îÅ Provider Management ‚îÅ‚îÅ‚îÅ"
        echo

        # Show current models status
        python3 "$SCRIPT_DIR/manage-providers.py" list

        echo
        log_header "‚îÅ‚îÅ‚îÅ Management Options ‚îÅ‚îÅ‚îÅ"
        echo
        echo "  ${CYAN}1${NC}. Enable a model"
        echo "  ${CYAN}2${NC}. Disable a model"
        echo "  ${CYAN}3${NC}. Enable all models from a provider"
        echo "  ${CYAN}4${NC}. Disable all models from a provider"
        echo "  ${CYAN}5${NC}. Apply changes (restart LiteLLM)"
        echo "  ${CYAN}b${NC}. Back to main menu"
        echo
        read -rp "Select option: " choice

        case $choice in
            1)
                echo
                read -rp "Enter model name to enable: " model_name
                python3 "$SCRIPT_DIR/manage-providers.py" enable "$model_name"
                pause
                ;;
            2)
                echo
                read -rp "Enter model name to disable: " model_name
                python3 "$SCRIPT_DIR/manage-providers.py" disable "$model_name"
                pause
                ;;
            3)
                echo
                python3 "$SCRIPT_DIR/manage-providers.py" providers
                echo
                read -rp "Enter provider name: " provider_name
                python3 "$SCRIPT_DIR/manage-providers.py" enable-provider "$provider_name"
                pause
                ;;
            4)
                echo
                python3 "$SCRIPT_DIR/manage-providers.py" providers
                echo
                read -rp "Enter provider name: " provider_name
                python3 "$SCRIPT_DIR/manage-providers.py" disable-provider "$provider_name"
                pause
                ;;
            5)
                echo
                log_info "Restarting LiteLLM service to apply changes..."
                if systemctl --user restart litellm.service; then
                    log_success "LiteLLM service restarted successfully"
                    log_info "Waiting for service to be ready..."
                    sleep 3
                    if curl -s -f "$LITELLM_URL/health" > /dev/null 2>&1; then
                        log_success "LiteLLM is healthy and ready"
                    else
                        log_warning "LiteLLM may still be starting up"
                    fi
                else
                    log_error "Failed to restart LiteLLM service"
                fi
                pause
                ;;
            b|B)
                return
                ;;
            *)
                log_error "Invalid option"
                sleep 1
                ;;
        esac
    done
}

# Main menu
main_menu() {
    while true; do
        show_status

        log_header "‚îÅ‚îÅ‚îÅ Main Menu ‚îÅ‚îÅ‚îÅ"
        echo
        echo "  ${CYAN}1${NC}. ${ICON_CHART} Show detailed status"
        echo "  ${CYAN}2${NC}. ${ICON_MODEL} List all models"
        echo "  ${CYAN}3${NC}. ${ICON_HEALTH} Run health check"
        echo "  ${CYAN}4${NC}. ${ICON_GEAR} vLLM model management"
        echo "  ${CYAN}5${NC}. ${ICON_INFO} View configuration"
        echo "  ${CYAN}6${NC}. ${ICON_SERVER} View service logs"
        echo "  ${CYAN}7${NC}. ${ICON_ROCKET} Test endpoints"
        echo "  ${CYAN}8${NC}. ${ICON_GEAR} Quick actions"
        echo "  ${CYAN}9${NC}. üîå Provider management (enable/disable)"
        echo "  ${CYAN}q${NC}. Quit"
        echo
        read -rp "Select option: " choice

        case $choice in
            1)
                show_status
                pause
                ;;
            2)
                list_models
                pause
                ;;
            3)
                health_check
                pause
                ;;
            4)
                vllm_menu
                ;;
            5)
                view_config
                ;;
            6)
                view_logs
                ;;
            7)
                test_endpoints
                ;;
            8)
                quick_actions
                ;;
            9)
                provider_management
                ;;
            q|Q)
                clear 2>/dev/null || true
                echo -e "${GREEN}${ICON_ROCKET} Thanks for using PTUI!${NC}"
                echo
                exit 0
                ;;
            *)
                log_error "Invalid option. Please try again."
                sleep 1
                ;;
        esac
    done
}

# Command-line interface (non-interactive mode)
needs_dependencies=true
if [[ $# -gt 0 ]]; then
    case "$1" in
        help|--help|-h|version|--version|-V)
            needs_dependencies=false
            ;;
    esac
fi

if [[ $# -eq 0 ]]; then
    needs_dependencies=false
fi

if [[ "$needs_dependencies" == "true" ]]; then
    ensure_dependencies
fi

if [[ $# -gt 0 ]]; then
    case "$1" in
        status|s)
            show_status
            exit $?
            ;;
        models|m)
            list_models
            exit $?
            ;;
        health|h)
            health_check
            exit $?
            ;;
        vllm|v)
            shift
            if [[ -f "$SCRIPT_DIR/vllm-model-switch.sh" ]]; then
                "$SCRIPT_DIR/vllm-model-switch.sh" "$@"
            else
                log_error "vLLM management script not found"
                exit 1
            fi
            exit $?
            ;;
        test|t)
            test_endpoints
            exit $?
            ;;
        help|--help|-h)
            show_help
            exit 0
            ;;
        version|--version|-v|-V)
            show_version
            exit 0
            ;;
        *)
            log_error "Unknown command: $1"
            echo "Run '$INVOCATION_NAME help' for usage information"
            exit 1
            ;;
    esac
else
    PTUI_HTTP_TIMEOUT="$DEFAULT_HTTP_TIMEOUT" \
        PTUI_REFRESH_SECONDS="$AUTO_REFRESH_SECONDS" \
        PTUI_VERSION="$VERSION" \
        "$PYTHON_BIN" "$SCRIPT_DIR/ptui_dashboard.py"
    python_exit=$?
    if [[ $python_exit -ne 0 ]]; then
        echo "‚ö†Ô∏è  Falling back to legacy PTUI menu..."
        ensure_dependencies
        main_menu
    fi
    exit 0
fi
