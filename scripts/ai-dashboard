#!/usr/bin/env python3
"""Interactive AI backend command center built with Textual.

This module provides a comprehensive terminal-based dashboard for monitoring
and managing AI backend LLM provider services including Ollama, vLLM, and
llama.cpp. It displays real-time metrics, GPU usage, and system resources,
plus controls to start/stop/restart services.

Example:
    Run the dashboard interactively::

        python3 scripts/ai-dashboard

    Or from project root::

        ./ai-dashboard

Key Bindings:
    - 'r': Manual refresh
    - 'q': Quit
    - 'a': Toggle auto-refresh
    - 'ctrl+l': Clear event log
"""

from __future__ import annotations

import logging
import os
import re
import subprocess
import time
import warnings
from collections.abc import Iterable
from dataclasses import dataclass
from typing import Literal
from urllib.parse import urlparse

import psutil
import requests  # type: ignore[import-untyped]
from textual import on
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Container, Horizontal, Vertical
from textual.coordinate import Coordinate
from textual.message import Message
from textual.reactive import reactive
from textual.widgets import Button, DataTable, Footer, Header, Label, Log, Static

# Configuration and logging setup
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG)

# ===== SECURITY: Service name allowlist (prevent command injection) =====
ALLOWED_SERVICES: dict[str, str] = {
    "ollama": "ollama.service",
    "vllm": "vllm.service",
    "llama_cpp_python": "llamacpp-python.service",
    "llama_cpp_native": "llama-cpp-native.service",
    "litellm_gateway": "litellm.service",
}

ALLOWED_ACTIONS: set[Literal["start", "stop", "restart", "enable", "disable"]] = {
    "start", "stop", "restart", "enable", "disable"
}

# ===== Configuration constants (environment overridable) =====
def _load_config() -> tuple[float, int, int]:
    """Load and validate configuration from environment variables.

    Security: Validates all inputs to prevent injection attacks.

    Returns:
        Tuple of (http_timeout, refresh_interval, log_height)

    Raises:
        ValueError: If any configuration value is invalid
    """
    try:
        http_timeout = float(os.getenv("AI_DASH_HTTP_TIMEOUT", "3.0"))
        if not 0.5 <= http_timeout <= 30:
            raise ValueError(f"HTTP_TIMEOUT must be 0.5-30 seconds, got {http_timeout}")
    except ValueError as e:
        raise ValueError(f"Invalid AI_DASH_HTTP_TIMEOUT: {e}") from None

    try:
        refresh_interval = int(os.getenv("AI_DASH_REFRESH_INTERVAL", "5"))
        if not 1 <= refresh_interval <= 60:
            raise ValueError(f"REFRESH_INTERVAL must be 1-60 seconds, got {refresh_interval}")
    except ValueError as e:
        raise ValueError(f"Invalid AI_DASH_REFRESH_INTERVAL: {e}") from None

    try:
        log_height = int(os.getenv("AI_DASH_LOG_HEIGHT", "12"))
        if not 5 <= log_height <= 50:
            raise ValueError(f"LOG_HEIGHT must be 5-50 lines, got {log_height}")
    except ValueError as e:
        raise ValueError(f"Invalid AI_DASH_LOG_HEIGHT: {e}") from None

    return http_timeout, refresh_interval, log_height

DEFAULT_HTTP_TIMEOUT, DEFAULT_REFRESH_INTERVAL, DEFAULT_LOG_HEIGHT = _load_config()

# ---------------------------------------------------------------------------
# Data models


@dataclass
class ServiceMetrics:
    """Snapshot of a provider's health and resource usage."""

    key: str
    display: str
    required: bool
    status: str
    port: int | None
    models: int | None
    cpu_percent: float
    memory_mb: float
    memory_percent: float
    vram_mb: float
    vram_percent: float
    response_ms: float
    pid: int | None
    notes: list[str]


@dataclass
class GPUOverview:
    """Aggregated GPU utilisation information."""

    detected: bool
    per_gpu: list[dict[str, float]]
    total_used_mb: float
    total_capacity_mb: float
    peak_util_percent: float


# ---------------------------------------------------------------------------
# Monitoring helpers


class GPUMonitor:
    """Utility helpers around NVIDIA's NVML for VRAM insight.

    Provides methods to query GPU information and per-process VRAM usage.
    Gracefully handles systems without NVIDIA GPUs or CUDA drivers installed.
    """

    def __init__(self) -> None:
        """Initialize GPU monitor with NVML."""
        self.initialized = False
        self.device_count = 0
        self._pynvml = None

        try:
            with warnings.catch_warnings():
                warnings.filterwarnings(
                    "ignore",
                    message="The pynvml package is deprecated",
                    category=FutureWarning,
                )
                import pynvml

            pynvml.nvmlInit()
            self._pynvml = pynvml
            self.initialized = True
            self.device_count = pynvml.nvmlDeviceGetCount()
            logger.debug(f"GPU monitoring initialized: {self.device_count} device(s) detected")
        except ImportError as e:
            logger.debug(f"NVIDIA GPU driver not available: {e}")
            self.initialized = False
        except (OSError, RuntimeError) as e:
            logger.debug(f"NVIDIA NVML initialization failed: {e}")
            self.initialized = False
        except Exception as e:  # pragma: no cover - unexpected errors
            logger.warning(f"Unexpected error initializing GPU monitor: {type(e).__name__}: {e}")
            self.initialized = False

    def get_gpu_info(self) -> list[dict[str, float]]:
        """Query all GPU information and memory stats.

        Returns:
            List of GPU info dicts with memory and utilization metrics.
            Empty list if GPU monitoring not available.
        """
        if not self.initialized or self._pynvml is None:
            return []

        pynvml = self._pynvml
        info: list[dict[str, float]] = []

        for index in range(self.device_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(index)
                name = pynvml.nvmlDeviceGetName(handle).decode("utf-8")
                memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
            except (OSError, AttributeError, UnicodeDecodeError) as e:
                logger.debug(f"Error querying GPU {index}: {type(e).__name__}: {e}")
                continue
            except Exception as e:  # pragma: no cover - unexpected errors
                logger.warning(f"Unexpected error querying GPU {index}: {type(e).__name__}: {e}")
                continue

            total_mb = memory.total / 1024**2
            used_mb = memory.used / 1024**2

            info.append(
                {
                    "id": float(index),
                    "name": name,
                    "memory_total_mb": total_mb,
                    "memory_used_mb": used_mb,
                    "memory_util_percent": (used_mb / total_mb * 100) if total_mb else 0.0,
                    "gpu_util_percent": float(util.gpu),
                }
            )

        return info

    def get_process_vram(self) -> dict[int, tuple[float, float]]:
        """Return VRAM usage per PID as (used_mb, percentage of associated GPUs).

        Returns:
            Dict mapping process PIDs to (vram_mb, vram_percent) tuples.
            Empty dict if GPU monitoring not available.
        """
        if not self.initialized or self._pynvml is None:
            return {}

        pynvml = self._pynvml
        usage: dict[int, dict[str, float]] = {}

        for index in range(self.device_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(index)
                memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
                total_mb = memory.total / 1024**2 if memory.total else 0.0
                processes = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)
            except (OSError, AttributeError) as e:
                logger.debug(f"Error querying GPU {index} processes: {type(e).__name__}: {e}")
                continue
            except Exception as e:  # pragma: no cover - unexpected errors
                logger.warning(f"Unexpected error querying GPU {index} processes: {type(e).__name__}: {e}")
                continue

            for record in processes or []:
                pid = getattr(record, "pid", None)
                used_bytes = getattr(record, "usedGpuMemory", 0)
                if pid is None or used_bytes is None or used_bytes < 0:
                    continue

                used_mb = used_bytes / 1024**2
                bucket = usage.setdefault(pid, {"used": 0.0, "total": 0.0})
                bucket["used"] += used_mb
                if total_mb > 0:
                    bucket["total"] += total_mb

        per_pid: dict[int, tuple[float, float]] = {}
        for pid, values in usage.items():
            total = values["total"] or 0.0
            used = values["used"]
            per_pid[pid] = (used, (used / total * 100) if total else 0.0)

        return per_pid


class ProviderMonitor:
    """Collects live diagnostics for each backend LLM provider.

    Monitors provider health through HTTP endpoint checks, collects system
    resource usage (CPU, memory, VRAM), and provides service control via
    systemctl. Supports Ollama, vLLM, and llama.cpp providers.

    Attributes:
        gpu_monitor: GPUMonitor instance for NVIDIA GPU metrics
        PROVIDERS: Registry of provider configurations (endpoint, service, etc.)
    """

    # SECURITY: Allowed endpoint hosts (localhost only, prevent SSRF)
    ALLOWED_HOSTS: set[str] = {"127.0.0.1", "localhost"}
    ALLOWED_PORTS: set[int] = {11434, 8000, 8001, 8080, 4000}

    PROVIDERS: dict[str, dict[str, object]] = {
        "ollama": {
            "endpoint": "http://127.0.0.1:11434/api/tags",
            "display": "Ollama",
            "service": "ollama.service",
            "required": False,
            "type": "ollama",
        },
        "vllm": {
            "endpoint": "http://127.0.0.1:8001/v1/models",
            "display": "vLLM",
            "service": "vllm.service",
            "required": True,
            "type": "litellm",
        },
        "llama_cpp_python": {
            "endpoint": "http://127.0.0.1:8000/v1/models",
            "display": "llama.cpp (Python)",
            "service": "llamacpp-python.service",
            "required": False,
            "type": "litellm",
        },
        "llama_cpp_native": {
            "endpoint": "http://127.0.0.1:8080/v1/models",
            "display": "llama.cpp (Native)",
            "service": "llama-cpp-native.service",
            "required": False,
            "type": "litellm",
        },
        "litellm_gateway": {
            "endpoint": "http://127.0.0.1:4000/v1/models",
            "display": "LiteLLM Gateway",
            "service": "litellm.service",
            "required": True,
            "type": "litellm",
        },
    }

    def __init__(self) -> None:
        """Initialize provider monitor with validation."""
        self.gpu_monitor = GPUMonitor()
        # SECURITY: Validate all endpoints at init time
        self._validate_endpoints()

    def _validate_endpoints(self) -> None:
        """SECURITY: Validate all provider endpoints against allowlists.

        Prevents SSRF attacks by ensuring all endpoints are localhost only.

        Raises:
            ValueError: If any endpoint fails validation
        """
        for key, cfg in self.PROVIDERS.items():
            endpoint = str(cfg.get("endpoint", ""))
            parsed = urlparse(endpoint)

            # Validate scheme
            if parsed.scheme not in ("http", "https"):
                raise ValueError(f"Invalid endpoint scheme for {key}: {parsed.scheme}")

            # SECURITY: Validate host (localhost only)
            if parsed.hostname not in self.ALLOWED_HOSTS:
                raise ValueError(f"Invalid endpoint host for {key}: {parsed.hostname}")

            # SECURITY: Validate port
            if parsed.port and parsed.port not in self.ALLOWED_PORTS:
                raise ValueError(f"Invalid endpoint port for {key}: {parsed.port}")

            logger.debug(f"✓ Validated endpoint for {key}: {endpoint}")

    # --------------------------- system helpers ---------------------------------

    def _service_name(self, key: str) -> str | None:
        record = self.PROVIDERS.get(key)
        return None if record is None else str(record["service"])

    def _get_service_pid(self, key: str) -> int | None:
        service = self._service_name(key)
        if not service:
            return None

        try:
            result = subprocess.run(
                ["systemctl", "--user", "show", service, "--property=MainPID", "--value"],
                capture_output=True,
                text=True,
                check=False,
                timeout=2,
            )
        except (subprocess.SubprocessError, FileNotFoundError):  # pragma: no cover - OS specific
            return None

        value = result.stdout.strip()
        if not value:
            return None
        try:
            pid = int(value)
        except ValueError:
            return None
        return pid if pid > 0 else None

    def _collect_process_metrics(self, key: str) -> tuple[int | None, float, float]:
        pid = self._get_service_pid(key)
        if pid is None:
            return None, 0.0, 0.0

        try:
            proc = psutil.Process(pid)
            cpu = proc.cpu_percent(interval=0.0)
            memory_mb = proc.memory_info().rss / 1024**2
            return pid, cpu, memory_mb
        except psutil.Error:
            return None, 0.0, 0.0

    def _parse_models(self, key: str, response_json: dict) -> int:
        provider_type = str(self.PROVIDERS[key]["type"])
        try:
            if provider_type == "ollama":
                return len(response_json.get("models", []))
            return len(response_json.get("data", []))
        except Exception:
            return 0

    # ----------------------------- public API ----------------------------------

    def collect_snapshot(self) -> tuple[list[ServiceMetrics], GPUOverview]:
        gpu_info = self.gpu_monitor.get_gpu_info()
        per_pid_vram = self.gpu_monitor.get_process_vram()
        system_memory_total = psutil.virtual_memory().total / 1024**2

        metrics: list[ServiceMetrics] = []

        for key, cfg in self.PROVIDERS.items():
            notes: list[str] = []
            endpoint = str(cfg["endpoint"])
            required = bool(cfg["required"])
            display = str(cfg["display"])

            start = time.perf_counter()
            status = "inactive"
            models = 0

            try:
                response = requests.get(endpoint, timeout=DEFAULT_HTTP_TIMEOUT)
                elapsed_ms = (time.perf_counter() - start) * 1000

                if response.ok:
                    try:
                        payload = response.json()
                        status = "active"
                        models = self._parse_models(key, payload)
                    except ValueError as e:
                        logger.debug(f"{key}: Invalid JSON response: {e}")
                        notes.append("Invalid JSON payload")
                        status = "degraded" if required else "inactive"
                else:
                    notes.append(f"HTTP {response.status_code} {response.reason}")
                    status = "degraded" if required else "inactive"
            except requests.exceptions.Timeout:
                elapsed_ms = (time.perf_counter() - start) * 1000
                logger.debug(f"{key}: Request timeout after {elapsed_ms:.0f}ms")
                notes.append("Timed out")
                status = "degraded" if required else "inactive"
            except requests.exceptions.ConnectionError as e:
                elapsed_ms = (time.perf_counter() - start) * 1000
                logger.debug(f"{key}: Connection error: {e}")
                notes.append("Connection error")
                status = "degraded" if required else "inactive"
            except requests.exceptions.RequestException as e:
                elapsed_ms = (time.perf_counter() - start) * 1000
                logger.debug(f"{key}: Request failed: {type(e).__name__}: {e}")
                notes.append(f"Request error: {type(e).__name__}")
                status = "degraded" if required else "inactive"

            pid, cpu_percent, memory_mb = self._collect_process_metrics(key)
            memory_percent = (memory_mb / system_memory_total * 100) if system_memory_total else 0.0
            vram_mb, vram_percent = per_pid_vram.get(pid, (0.0, 0.0)) if pid else (0.0, 0.0)

            metrics.append(
                ServiceMetrics(
                    key=key,
                    display=display,
                    required=required,
                    status=status,
                    port=urlparse(endpoint).port,
                    models=models,
                    cpu_percent=cpu_percent,
                    memory_mb=memory_mb,
                    memory_percent=memory_percent,
                    vram_mb=vram_mb,
                    vram_percent=vram_percent,
                    response_ms=elapsed_ms,
                    pid=pid,
                    notes=notes,
                )
            )

        if gpu_info:
            total_used = sum(entry["memory_used_mb"] for entry in gpu_info)
            total_capacity = sum(entry["memory_total_mb"] for entry in gpu_info)
            peak_util = max(entry["gpu_util_percent"] for entry in gpu_info)
        else:
            total_used = total_capacity = peak_util = 0.0

        overview = GPUOverview(
            detected=bool(gpu_info),
            per_gpu=gpu_info,
            total_used_mb=total_used,
            total_capacity_mb=total_capacity,
            peak_util_percent=peak_util,
        )

        return metrics, overview

    def systemctl(self, key: str, action: str) -> bool:
        """Execute systemctl command with security validation.

        SECURITY: Validates service name and action against allowlists to prevent
        command injection attacks. Only allows whitelisted services and actions.

        Args:
            key: Provider key (must be in ALLOWED_SERVICES)
            action: Action to execute (must be in ALLOWED_ACTIONS)

        Returns:
            True if command succeeded, False otherwise
        """
        # SECURITY: Validate service key
        if key not in ALLOWED_SERVICES:
            logger.warning(f"Rejected invalid service key: {key}")
            return False

        # SECURITY: Validate action
        if action not in ALLOWED_ACTIONS:
            logger.warning(f"Rejected invalid action: {action}")
            return False

        service = self._service_name(key)
        if not service:
            return False

        try:
            subprocess.run(
                ["systemctl", "--user", action, service],
                check=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                timeout=8,
                # SECURITY: Minimal environment to prevent injection
                env={"PATH": "/usr/bin:/bin"},
            )
        except subprocess.SubprocessError as e:
            logger.debug(f"systemctl {action} {key} failed: {type(e).__name__}")
            return False

        return True


# ---------------------------------------------------------------------------
# UI components


class OverviewPanel(Static):
    """Displays condensed summary of all services.

    Shows count of active/inactive services, average CPU/memory usage,
    and lists any services requiring attention (degraded or inactive).
    """

    def update_overview(self, metrics: Iterable[ServiceMetrics]) -> None:
        """Update overview with current metrics.

        Args:
            metrics: Iterable of ServiceMetrics to summarize
        """
        metrics = list(metrics)
        total = len(metrics)
        active = sum(1 for m in metrics if m.status == "active")
        degraded = [m for m in metrics if m.status != "active"]
        avg_cpu = sum(m.cpu_percent for m in metrics) / total if total else 0.0
        avg_mem = sum(m.memory_percent for m in metrics) / total if total else 0.0

        warn_lines = ", ".join(f"{m.display}" for m in degraded) if degraded else "None"

        self.update(
            f"[b]Services:[/b] {active}/{total} active\n"
            f"[b]Average CPU:[/b] {avg_cpu:.1f}%   [b]Average Memory:[/b] {avg_mem:.1f}%\n"
            f"[b]Attention:[/b] {warn_lines}"
        )


class ServiceTable(DataTable):
    """Tabular view displaying all provider services.

    Shows provider name, status, CPU/memory usage, response time, and model count.
    Supports row selection for viewing detailed metrics in the detail panel.
    """

    def __init__(self) -> None:
        """Initialize service table with columns."""
        super().__init__(zebra_stripes=True)
        self.cursor_type = "row"
        self.show_header = True
        self.add_columns(
            "Provider",
            "Status",
            "CPU",
            "Mem",
            "VRAM",
            "Resp",
            "Models",
        )

    def populate(self, metrics: Iterable[ServiceMetrics], selected: str | None) -> None:
        """Populate table with service metrics and restore selection.

        Args:
            metrics: Iterable of ServiceMetrics to display
            selected: Provider key to select, if available
        """
        self.clear()
        metrics = list(metrics)
        selected_index: int | None = None

        for index, metric in enumerate(metrics):
            status_style = {
                "active": "green",
                "degraded": "yellow",
                "inactive": "red",
            }.get(metric.status, "white")

            cpu_text = f"{metric.cpu_percent:.1f}%"
            mem_text = f"{metric.memory_mb:.0f}MB"
            vram_text = "-" if not metric.vram_mb else f"{metric.vram_mb:.0f}MB"
            resp_text = f"{metric.response_ms:.0f}ms"

            self.add_row(
                metric.display,
                f"[{status_style}]{metric.status.title()}[/]",
                cpu_text,
                mem_text,
                vram_text,
                resp_text,
                str(metric.models),
                key=metric.key,
            )

            if selected and metric.key == selected:
                selected_index = index

        if selected_index is not None:
            self.cursor_coordinate = Coordinate(row=selected_index, column=0)
        elif self.row_count:
            self.cursor_coordinate = Coordinate(row=0, column=0)


class GPUCard(Static):
    """Displays GPU utilization summary.

    Shows total VRAM usage, peak utilization percentage, and per-GPU breakdown.
    Gracefully handles systems without NVIDIA GPUs.
    """

    def update_overview(self, overview: GPUOverview) -> None:
        """Update GPU card with latest GPU metrics.

        Args:
            overview: GPUOverview with aggregated GPU statistics
        """
        if not overview.detected:
            self.update("No NVIDIA GPU detected or NVML unavailable.")
            return

        lines = [
            f"Total VRAM: {overview.total_used_mb:.0f}/{overview.total_capacity_mb:.0f}MB ("
            f"{(overview.total_used_mb / overview.total_capacity_mb * 100) if overview.total_capacity_mb else 0:.1f}% )",
            f"Peak utilisation: {overview.peak_util_percent:.1f}%",
        ]

        for entry in overview.per_gpu:
            lines.append(
                f"GPU {int(entry['id'])}: {entry['memory_used_mb']:.0f}/"
                f"{entry['memory_total_mb']:.0f}MB ({entry['memory_util_percent']:.1f}%)"
            )

        self.update("\n".join(lines))


class DetailPanel(Vertical):
    """Shows focused service details and exposes control buttons.

    Displays comprehensive metrics for selected service including status,
    CPU/memory/VRAM usage, port, model count, and PID. Provides buttons
    to start/stop/restart/enable/disable the service.
    """

    class ServiceAction(Message):
        """Message posted when user clicks a service control button.

        Attributes:
            action: Control action (start, stop, restart, enable, disable)
            service_key: Provider key targeted by the action
        """

        def __init__(self, action: str, service_key: str) -> None:
            super().__init__()
            self.action = action
            self.service_key = service_key

    def __init__(self) -> None:
        super().__init__(id="detail-panel")
        self._current: ServiceMetrics | None = None

    def compose(self) -> ComposeResult:
        yield Label("Select a provider to inspect", id="detail-title")
        yield Label(id="detail-status")
        yield Label(id="detail-resources")
        yield Label(id="detail-metadata")
        with Horizontal(id="detail-actions"):
            yield Button("Start", id="action-start", variant="success")
            yield Button("Stop", id="action-stop", variant="error")
            yield Button("Restart", id="action-restart", variant="warning")
            yield Button("Enable", id="action-enable", variant="primary")
            yield Button("Disable", id="action-disable", variant="primary")
        yield Log(id="detail-notes", highlight=True)

    def update_details(self, metrics: ServiceMetrics | None) -> None:
        """Update detail panel with metrics for selected service.

        Args:
            metrics: ServiceMetrics for the selected provider, or None to clear.
        """
        self._current = metrics

        try:
            title = self.query_one("#detail-title", Label)
            status_label = self.query_one("#detail-status", Label)
            resources_label = self.query_one("#detail-resources", Label)
            metadata_label = self.query_one("#detail-metadata", Label)
            notes_log = self.query_one(Log)
        except Exception as e:
            logger.warning(f"Failed to query detail panel widgets: {type(e).__name__}: {e}")
            return

        if not metrics:
            try:
                title.update("Select a provider to inspect")
                status_label.update("")
                resources_label.update("")
                metadata_label.update("")
                notes_log.clear()
            except Exception as e:
                logger.debug(f"Error clearing detail panel: {e}")
            return

        try:
            status_colour = {
                "active": "green",
                "degraded": "yellow",
                "inactive": "red",
            }.get(metrics.status, "white")

            title.update(f"[b]{metrics.display}[/b] ({'required' if metrics.required else 'optional'})")
            status_label.update(
                f"Status: [{status_colour}]{metrics.status.upper()}[/]  |  Response: {metrics.response_ms:.0f} ms"
            )
            resources_label.update(
                "CPU: {:.1f}%   MEM: {:.1f} MB ({:.1f}%)   VRAM: {}".format(
                    metrics.cpu_percent,
                    metrics.memory_mb,
                    metrics.memory_percent,
                    "-" if not metrics.vram_mb else f"{metrics.vram_mb:.1f} MB ({metrics.vram_percent:.1f}%)",
                )
            )
            metadata_label.update(
                "Port: {}   Models: {}   PID: {}".format(
                    metrics.port or "n/a",
                    metrics.models,
                    metrics.pid or "n/a",
                )
            )

            notes_log.clear()
            if metrics.notes:
                for line in metrics.notes:
                    notes_log.write(line)
            else:
                notes_log.write("No warnings recorded.")
        except Exception as e:
            logger.warning(f"Error updating detail panel: {type(e).__name__}: {e}")

    @on(Button.Pressed)
    def handle_button(self, event: Button.Pressed) -> None:
        if not self._current:
            return
        action = event.button.id or ""
        if not action.startswith("action-"):
            return
        self.post_message(self.ServiceAction(action.replace("action-", ""), self._current.key))


# ---------------------------------------------------------------------------
# Main dashboard application


class DashboardApp(App[None]):
    """Interactive command center for the AI backend.

    Provides real-time monitoring and management of LLM provider services
    including Ollama, vLLM, and llama.cpp. Displays system metrics, GPU
    usage, and provider status with controls for service lifecycle management.

    Attributes:
        monitor: ProviderMonitor instance for collecting diagnostics
        metrics: Current service metrics snapshot
        gpu_overview: Current GPU utilization snapshot
        selected_key: Currently selected provider for detailed view
    """

    CSS = """
    Screen {
        layout: vertical;
    }

    /* Body grid splits the view in two columns
       Left: overview + table, Right: GPU + detail + event log

       ┌───────────────┬────────────┐
       │ overview      │ gpu        │
       │ service table │ detail/log │
       └───────────────┴────────────┘

       Fine-tune spacing with padding and gaps so layout stays compact. */

    Container#body {
        layout: grid;
        grid-size: 2;
        grid-columns: 3fr 2fr;
        grid-rows: auto 1fr;
        height: 1fr;
        padding: 1 2;
    }

    Vertical#left-column,
    Vertical#right-column {
        layout: vertical;
    }

    OverviewPanel {
        border: solid #3a3a3a;
        padding: 1;
    }

    ServiceTable {
        height: 1fr;
        border: solid #3a3a3a;
    }

    GPUCard {
        border: solid #3a3a3a;
        padding: 1;
    }

    /* Detail panel and log share the right column vertically.
       Detail gets the remaining height with the log pinned at the bottom.
       Provide some contrast for the action row. */

    #detail-panel {
        border: solid #3a3a3a;
        padding: 1;
        height: 1fr;
    }

    #detail-actions {
        padding-top: 1;
    }

    Log {
        height: 12;
        border: solid #333333;
        padding: 1;
    }
    """

    BINDINGS = [
        Binding("r", "refresh", "Refresh"),
        Binding("q", "quit", "Quit"),
        Binding("a", "toggle_auto", "Toggle auto-refresh"),
        Binding("ctrl+l", "clear_log", "Clear log"),
    ]

    auto_refresh = reactive(True)

    def __init__(self) -> None:
        super().__init__()
        self.monitor = ProviderMonitor()
        self.metrics: list[ServiceMetrics] = []
        self.gpu_overview = GPUOverview(False, [], 0.0, 0.0, 0.0)
        self.selected_key: str | None = None

    # ------------------------------ layout ---------------------------------

    def compose(self) -> ComposeResult:
        yield Header(show_clock=True)
        with Container(id="body"):
            with Vertical(id="left-column"):
                yield OverviewPanel(id="overview")
                yield ServiceTable()
            with Vertical(id="right-column"):
                yield GPUCard(id="gpu")
                yield DetailPanel()
                yield Log(id="event-log", highlight=True)
        yield Footer()

    # ----------------------------- lifecycle --------------------------------

    def on_mount(self) -> None:
        """Initialize dashboard on app mount.

        Sets up auto-refresh timer and performs initial data collection.
        """
        self._refresh_table()
        self.refresh_timer = self.set_interval(
            DEFAULT_REFRESH_INTERVAL, self._refresh_table, pause=not self.auto_refresh
        )

    def action_toggle_auto(self) -> None:
        """Toggle auto-refresh on/off (binding: 'a')."""
        self.auto_refresh = not self.auto_refresh
        if self.auto_refresh:
            self.refresh_timer.resume()
            self.log_event(f"Auto-refresh enabled ({DEFAULT_REFRESH_INTERVAL}s cadence).")
        else:
            self.refresh_timer.pause()
            self.log_event("Auto-refresh paused.")

    def action_refresh(self) -> None:
        """Manually refresh all metrics (binding: 'r')."""
        self._refresh_table()
        self.log_event("Manual refresh completed.")

    def action_clear_log(self) -> None:
        """Clear event log (binding: 'ctrl+l')."""
        try:
            self.query_one("#event-log", Log).clear()
        except Exception as e:
            logger.debug(f"Error clearing log: {e}")

    # ---------------------------- refresh engine ----------------------------

    def _refresh_table(self) -> None:
        """Refresh all dashboard displays with latest provider metrics."""
        try:
            self.metrics, self.gpu_overview = self.monitor.collect_snapshot()
        except Exception as e:
            logger.warning(f"Error collecting provider snapshot: {type(e).__name__}: {e}")
            self.log_event(f"[red]Snapshot failed:[/] {type(e).__name__}")
            return

        try:
            table = self.query_one(ServiceTable)
            table.populate(self.metrics, self.selected_key)

            self.query_one(OverviewPanel).update_overview(self.metrics)
            self.query_one(GPUCard).update_overview(self.gpu_overview)

            detail = self.query_one(DetailPanel)
            detail.update_details(self._find_metric(self.selected_key))
        except Exception as e:
            logger.warning(f"Error updating dashboard displays: {type(e).__name__}: {e}")

    def _find_metric(self, key: str | None) -> ServiceMetrics | None:
        """Find ServiceMetrics by provider key.

        Args:
            key: Provider key to search for

        Returns:
            ServiceMetrics if found, None otherwise
        """
        if not key:
            return None
        for metric in self.metrics:
            if metric.key == key:
                return metric
        return None

    # ------------------------------- helpers --------------------------------

    def log_event(self, message: str) -> None:
        """Log an event message to the event log widget.

        Args:
            message: Message to display (supports Rich markup)
        """
        try:
            self.query_one("#event-log", Log).write(message)
        except Exception as e:
            logger.debug(f"Error logging event: {e}")

    # --------------------------- event handlers -----------------------------

    @on(DataTable.RowSelected)
    def handle_row_selected(self, event: DataTable.RowSelected) -> None:
        """Handle service table row selection.

        Updates detail panel to show metrics for selected provider.

        Args:
            event: DataTable row selection event
        """
        row_key = getattr(event.row_key, "value", event.row_key)
        key = str(row_key)
        self.selected_key = key
        self.query_one(DetailPanel).update_details(self._find_metric(key))

    @on(DetailPanel.ServiceAction)
    def handle_service_action(self, event: DetailPanel.ServiceAction) -> None:
        """Handle service control action (start/stop/restart/enable/disable).

        Executes the requested systemctl action and refreshes metrics.

        Args:
            event: ServiceAction event with action and service key
        """
        success = self.monitor.systemctl(event.service_key, event.action)
        if success:
            self.log_event(f"{event.action.title()} command sent to {event.service_key}.")
            # Refresh immediately to reflect new status
            self._refresh_table()
        else:
            self.log_event(f"[red]Failed[/] to {event.action} {event.service_key}.")


if __name__ == "__main__":
    DashboardApp().run()
