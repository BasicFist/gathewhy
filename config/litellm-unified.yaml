# LiteLLM Unified Configuration
# Extended configuration integrating all providers: Ollama + llama.cpp + vLLM
#
# This file extends the existing ../openwebui/config/litellm.yaml
# To use: Copy/merge this with openwebui/config/litellm.yaml

model_list:
  # ============================================================================
  # OLLAMA MODELS (existing)
  # ============================================================================

  - model_name: llama3.1-ollama
    litellm_params:
      model: ollama/llama3.1:8b
      api_base: http://127.0.0.1:11434
    model_info:
      tags: ["general", "chat", "8b"]
      provider: ollama

  - model_name: qwen-coder
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://127.0.0.1:11434
    model_info:
      tags: ["code", "specialized", "7b"]
      provider: ollama

  # Specialized models (if pulled)
  - model_name: deepseek-coder
    litellm_params:
      model: ollama/deepseek-coder-v2:16b
      api_base: http://127.0.0.1:11434
    model_info:
      tags: ["code", "large", "16b"]
      provider: ollama

  - model_name: qwen-math
    litellm_params:
      model: ollama/qwen2-math:7b
      api_base: http://127.0.0.1:11434
    model_info:
      tags: ["math", "specialized", "7b"]
      provider: ollama

  - model_name: llava-vision
    litellm_params:
      model: ollama/llava:13b
      api_base: http://127.0.0.1:11434
    model_info:
      tags: ["vision", "multimodal", "13b"]
      provider: ollama

  # ============================================================================
  # LLAMA.CPP MODELS (new)
  # ============================================================================

  - model_name: llama-cpp-python
    litellm_params:
      model: openai/local-model  # llama.cpp uses OpenAI-compatible API
      api_base: http://127.0.0.1:8000
      api_key: "not-needed"
      stream: true
    model_info:
      tags: ["gguf", "cuda", "high-performance"]
      provider: llama_cpp_python
      notes: "Python bindings, n_parallel=4, n_ctx=8192"

  - model_name: llama-cpp-native
    litellm_params:
      model: openai/local-model
      api_base: http://127.0.0.1:8080
      api_key: "not-needed"
      stream: true
    model_info:
      tags: ["gguf", "cuda", "maximum-performance"]
      provider: llama_cpp_native
      notes: "Pure C++ server, no Python overhead"

  # ============================================================================
  # vLLM MODELS (NEW - integrated from CrushVLLM)
  # ============================================================================

  - model_name: llama2-13b-vllm
    litellm_params:
      model: vllm/meta-llama/Llama-2-13b-chat-hf
      api_base: http://127.0.0.1:8001
      stream: true
    model_info:
      tags: ["chat", "high-throughput", "13b", "vllm"]
      provider: vllm
      context_length: 4096
      notes: "High-throughput inference via vLLM, continuous batching"

  # Template for adding more vLLM models
  # - model_name: mistral-7b-vllm
  #   litellm_params:
  #     model: vllm/mistralai/Mistral-7B-Instruct-v0.2
  #     api_base: http://127.0.0.1:8001
  #   model_info:
  #     tags: ["chat", "7b", "vllm"]
  #     provider: vllm

  # ============================================================================
  # OPENWEBUI PIPELINES (INTEGRATED)
  # ============================================================================

  - model_name: academic_search
    litellm_params:
      model: openai/academic_search  # OpenAI-compatible API format
      api_base: http://127.0.0.1:9099/v1
      api_key: "not-needed"
      custom_llm_provider: openai
      stream: true
    model_info:
      tags: ["pipeline", "academic", "arxiv", "pubmed"]
      provider: openwebui_pipelines
      description: "Search academic papers from arXiv and PubMed"
      notes: "Uses toolsrv for arXiv/PubMed APIs"

  - model_name: market_snapshot
    litellm_params:
      model: openai/market_snapshot
      api_base: http://127.0.0.1:9099/v1
      api_key: "not-needed"
      custom_llm_provider: openai
      stream: true
    model_info:
      tags: ["pipeline", "market", "data"]
      provider: openwebui_pipelines
      description: "Market data snapshots with ticker extraction"

  - model_name: smart_router
    litellm_params:
      model: openai/smart_router
      api_base: http://127.0.0.1:9099/v1
      api_key: "not-needed"
      custom_llm_provider: openai
      stream: true
    model_info:
      tags: ["pipeline", "router", "query-classification"]
      provider: openwebui_pipelines
      description: "Query classification routing (code→deepseek, math→qwen, vision→llava)"
      notes: "Depends on Ollama models being available"

  - model_name: hybrid_search
    litellm_params:
      model: openai/hybrid_search
      api_base: http://127.0.0.1:9099/v1
      api_key: "not-needed"
      custom_llm_provider: openai
      stream: true
    model_info:
      tags: ["pipeline", "search", "hybrid", "rag"]
      provider: openwebui_pipelines
      description: "Hybrid keyword + semantic search"

  - model_name: consensus
    litellm_params:
      model: openai/consensus
      api_base: http://127.0.0.1:9099/v1
      api_key: "not-needed"
      custom_llm_provider: openai
      stream: true
    model_info:
      tags: ["pipeline", "consensus", "multi-model"]
      provider: openwebui_pipelines
      description: "Multi-model consensus generation"

  - model_name: code_analyzer
    litellm_params:
      model: openai/code_analyzer
      api_base: http://127.0.0.1:9099/v1
      api_key: "not-needed"
      custom_llm_provider: openai
      stream: true
    model_info:
      tags: ["pipeline", "code", "analysis"]
      provider: openwebui_pipelines
      description: "Code analysis and review pipeline"

  # ============================================================================
  # ROUTER CONFIGURATIONS
  # ============================================================================

  # Redundant routing for high availability
  - model_name: llama3.1:8b  # Canonical name
    litellm_params:
      model: ollama/llama3.1:8b
      api_base: http://127.0.0.1:11434
    model_info:
      tags: ["primary"]
      provider: ollama

  # Same model via fallback provider
  - model_name: llama3.1-backup
    litellm_params:
      model: openai/llama3.1-gguf
      api_base: http://127.0.0.1:8000
    model_info:
      tags: ["fallback"]
      provider: llama_cpp_python

# ============================================================================
# LITELLM SETTINGS
# ============================================================================

litellm_settings:
  # Request handling
  request_timeout: 60  # Increased for vLLM larger models
  stream_timeout: 0    # No timeout for streaming
  num_retries: 3
  timeout: 300

  # Caching (Redis) with Provider Namespacing
  # Addresses Codex risk: "Redis cache implications - vLLM's high throughput could surge cache keys"
  cache: true
  cache_params:
    type: redis
    host: 127.0.0.1
    port: 6379
    ttl: 3600  # 1 hour cache
    namespace: "litellm"  # Global namespace prefix

    # Provider-specific cache isolation strategy:
    # - Uses model_name as part of cache key (automatic in LiteLLM)
    # - Global namespace prevents conflicts with other Redis users
    # - Per-provider TTLs can be configured via router_settings below
    # - Monitor cache hit rates: redis-cli --scan --pattern "litellm:*"

  # Logging
  set_verbose: false
  json_logs: true

  # Success/Failure tracking (disabled - langfuse not installed)
  # success_callback: ["langfuse"]  # Optional: if using Langfuse
  # failure_callback: ["langfuse"]

# ============================================================================
# SECURITY SETTINGS
# ============================================================================

general_settings:
  # Master Key Authentication
  # SECURITY: Set via environment variable for production
  # Format: Must start with "sk-"
  # Setup:
  #   1. Generate: LITELLM_MASTER_KEY="sk-$(openssl rand -hex 16)"
  #   2. Add to systemd service:
  #      systemctl --user edit litellm.service
  #      [Service]
  #      Environment="LITELLM_MASTER_KEY=sk-..."
  #   3. Restart: systemctl --user daemon-reload && systemctl --user restart litellm.service
  #
  # When enabled, all requests must include header:
  #   Authorization: Bearer sk-your-master-key
  #
  # Uncomment to enable:
  # master_key: ${LITELLM_MASTER_KEY}

  # Salt Key for DB encryption
  # IMPORTANT: Do not change after adding models to DB
  # salt_key: ${LITELLM_SALT_KEY}

  # Database connection (if using DB for model management)
  # database_url: ${DATABASE_URL}

# ============================================================================
# ROUTER SETTINGS
# ============================================================================

router_settings:
  # Routing strategy
  routing_strategy: usage-based-routing-v2
  # Options: simple-shuffle, least-busy, usage-based-routing, latency-based-routing

  # Model fallback
  model_group_alias:
    general_chat:
      - llama3.1-ollama
      - llama2-13b-vllm
      - llama-cpp-python
    code_generation:
      - qwen-coder
      - deepseek-coder
    high_throughput:
      - llama2-13b-vllm  # vLLM excels here

  # Reliability
  allowed_fails: 3
  num_retries: 2
  timeout: 30  # seconds
  cooldown_time: 60  # seconds before retrying failed provider

  # Load balancing
  enable_pre_call_checks: true  # Check provider health before routing
  redis_host: 127.0.0.1
  redis_port: 6379

  # Fallback chains (simple list format for LiteLLM v1.x)
  fallbacks:
    - llama3.1:8b:
        - llama3.1-backup
        - llama-cpp-python
    - llama2-13b-vllm:
        - llama3.1-ollama  # Smaller model if vLLM fails

# ============================================================================
# SERVER SETTINGS
# ============================================================================

server_settings:
  port: 4000
  host: 0.0.0.0  # Listen on all interfaces

  # CORS (for web access)
  # SECURITY: Restricted to localhost for local network deployment
  cors:
    enabled: true
    allowed_origins:
      - "http://localhost:*"
      - "http://127.0.0.1:*"
      - "http://[::1]:*"
    # For production with external access, add specific domains:
    # - "https://yourdomain.com"

  # Health checks
  health_check_endpoint: /health

  # Monitoring
  prometheus:
    enabled: true
    port: 9090

# ============================================================================
# ADVANCED FEATURES
# ============================================================================

# Rate limiting - SECURITY: Enabled with sensible defaults
rate_limit_settings:
  enabled: true
  limits:
    # General models - moderate limits
    llama3.1:8b:
      rpm: 100  # requests per minute
      tpm: 50000  # tokens per minute

    qwen-coder:
      rpm: 80
      tpm: 40000

    # High-throughput models - higher limits
    llama2-13b-vllm:
      rpm: 50  # Lower RPM due to higher per-request cost
      tpm: 100000

    # High-performance models - balanced limits
    llama-cpp-python:
      rpm: 120
      tpm: 60000

    llama-cpp-native:
      rpm: 150  # Fastest provider
      tpm: 75000

# ============================================================================
# OBSERVABILITY & MONITORING (Phase 2)
# ============================================================================

litellm_settings:
  # Prometheus metrics integration
  callbacks: ["prometheus"]
  service_callback: ["prometheus_system"]  # System health (Redis, latency)

  # Custom Prometheus configuration
  prometheus_initialize_budget_metrics: true  # Initialize budget metrics on startup
  enable_end_user_cost_tracking_prometheus_only: false  # Reduce cardinality (solo dev)

  # Metrics endpoint authentication (optional - disabled for dev)
  require_auth_for_metrics_endpoint: false

  # Custom metadata labels (extract from request metadata)
  custom_prometheus_metadata_labels:
    - "metadata.project"      # Track by project
    - "metadata.environment"  # dev/staging/prod
    - "metadata.user_type"    # human/agent/api

  # Custom tags (track specific request types)
  custom_prometheus_tags:
    - "production"
    - "development"
    - "batch-job"
    - "interactive"
    - "User-Agent: curl/*"       # Track CLI usage
    - "User-Agent: python/*"     # Track Python SDK usage
    - "User-Agent: openai/*"     # Track OpenAI SDK usage

  # Prometheus metrics configuration (optimize cardinality)
  prometheus_metrics_config:
    # Token consumption metrics
    - group: "token_usage"
      metrics:
        - "litellm_input_tokens_metric"
        - "litellm_output_tokens_metric"
        - "litellm_total_tokens_metric"
      include_labels:
        - "model"
        - "requested_model"
        - "api_provider"

    # Request tracking metrics
    - group: "requests"
      metrics:
        - "litellm_proxy_total_requests_metric"
        - "litellm_proxy_failed_requests_metric"
      include_labels:
        - "status_code"
        - "requested_model"
        - "model_group"

    # Latency metrics
    - group: "performance"
      metrics:
        - "litellm_request_total_latency_metric"
        - "litellm_llm_api_latency_metric"
      include_labels:
        - "model"
        - "api_provider"
        - "requested_model"

    # Deployment health metrics
    - group: "health"
      metrics:
        - "litellm_deployment_success_responses"
        - "litellm_deployment_failure_responses"
      include_labels:
        - "api_provider"
        - "requested_model"
        - "model"

    # System health metrics (Redis, self)
    - group: "system"
      metrics:
        - "litellm_redis_latency"
        - "litellm_redis_fails"
        - "litellm_self_latency"
      include_labels:
        - "operation"

  # ============================================================================
  # REQUEST TRACING & DEBUG LOGGING
  # ============================================================================

  # Success/failure callbacks for detailed tracing
  success_callback: ["langfuse"]  # Track successful requests (optional: langfuse, langsmith, etc.)
  failure_callback: ["langfuse"]  # Track failed requests for debugging

  # Detailed request/response logging
  log_raw_request_response: true  # Log full request/response payloads (dev only)
  detailed_debug: true             # Enable detailed debug information

  # Request metadata preservation
  metadata_persistence: true       # Preserve metadata across request lifecycle

  # Logging configuration
  logging:
    # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
    level: "INFO"  # Change to DEBUG for troubleshooting

    # Log file settings
    log_file: "/var/log/litellm/requests.log"
    log_file_max_size_mb: 100
    log_file_backup_count: 5

    # Request logging format
    log_format: "json"  # json or text

    # What to log
    log_requests: true
    log_responses: true
    log_errors: true
    log_latency: true
    log_token_usage: true

    # Privacy & security
    redact_user_api_keys: true      # Don't log API keys
    redact_messages_in_exceptions: false  # Log full messages for debugging

    # Performance logging
    log_slow_requests: true
    slow_request_threshold_ms: 5000  # Log requests slower than 5s

  # Request tracing settings
  tracing:
    enabled: true

    # Generate unique request IDs
    generate_request_id: true

    # Include in response headers
    include_request_id_in_response: true

    # Trace sampling (1.0 = all requests, 0.1 = 10% of requests)
    sampling_rate: 1.0  # Full tracing for dev, reduce for production

    # Trace attributes to capture
    capture_attributes:
      - "model"
      - "api_provider"
      - "requested_model"
      - "user_id"
      - "metadata.project"
      - "metadata.environment"
      - "http.status_code"
      - "http.method"
      - "http.url"

  # Debug mode settings (environment-specific)
  debug_mode:
    enabled: false  # Enable for troubleshooting
    verbose_logging: false
    log_all_callbacks: false
    log_internal_state: false

# Budget management (optional)
budget_settings:
  enabled: false
  # max_budget: 100  # dollars per month
  # budget_duration: "1mo"

# ============================================================================
# DEBUGGING & DEVELOPMENT
# ============================================================================

# Enable debug mode for troubleshooting
debug: false
debug_router: false

# Test mode (dry run without actual API calls)
test_mode: false

# ============================================================================
# USAGE NOTES
# ============================================================================

# To apply this configuration:
#
# 1. Backup existing config:
#    cp ../openwebui/config/litellm.yaml ../openwebui/config/litellm.yaml.backup
#
# 2. Merge or replace:
#    cp config/litellm-unified.yaml ../openwebui/config/litellm.yaml
#
# 3. Restart LiteLLM:
#    systemctl --user restart litellm.service
#
# 4. Verify:
#    curl http://localhost:4000/v1/models | jq
#    ./scripts/validate-unified-backend.sh
#
# 5. Test vLLM integration:
#    curl -X POST http://localhost:4000/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -d '{
#        "model": "llama2-13b-vllm",
#        "messages": [{"role": "user", "content": "Hello!"}]
#      }'

# ============================================================================
# MODEL ADDITION TEMPLATE
# ============================================================================

# To add a new model:
#
# 1. Add entry to model_list:
#    - model_name: your-model-name
#      litellm_params:
#        model: provider/model-path
#        api_base: http://host:port
#      model_info:
#        tags: ["tag1", "tag2"]
#        provider: provider_name
#
# 2. Add routing rules if needed in router_settings
#
# 3. Update config/providers.yaml and config/model-mappings.yaml
#
# 4. Test with validation script

# ============================================================================
# PROVIDER-SPECIFIC NOTES
# ============================================================================

# Ollama:
#   - Use format: ollama/model:version
#   - API base: http://127.0.0.1:11434
#   - Pull models first: ollama pull model:version
#
# llama.cpp:
#   - Use format: openai/local-model (generic)
#   - API base: http://127.0.0.1:8000 (Python) or :8080 (native)
#   - Load GGUF model before starting server
#
# vLLM:
#   - Use format: vllm/huggingface/model-path
#   - API base: http://127.0.0.1:8001
#   - Requires GPU with sufficient VRAM
#   - Best for 13B+ models with high concurrency
