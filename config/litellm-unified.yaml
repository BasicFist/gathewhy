# ============================================================================
# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# ============================================================================
#
# Generated by: scripts/generate-litellm-config.py
# Source files: config/providers.yaml, config/model-mappings.yaml
# Generated at: 2025-10-27T16:48:57.544299
# Version: git-e751d27
#
# To modify this configuration:
#   1. Edit config/providers.yaml or config/model-mappings.yaml
#   2. Run: python3 scripts/generate-litellm-config.py
#   3. Validate: python3 scripts/validate-config-schema.py
#
# ============================================================================

model_list:
  - model_name: llama3.1:latest
    litellm_params:
      model: ollama/llama3.1:latest
      api_base: http://127.0.0.1:11434
    model_info:
      tags:
        - general_chat
        - 8b
        - q4_k_m
      provider: ollama
  - model_name: qwen2.5-coder:7b
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://127.0.0.1:11434
    model_info:
      tags:
        - code_generation
        - 7.6b
        - q4_k_m
      provider: ollama
  - model_name: mythomax-l2-13b-q5_k_m
    litellm_params:
      model: ollama/mythomax-l2-13b-q5_k_m
      api_base: http://127.0.0.1:11434
      extra_body:
        options:
          num_gpu_layers: -1
    model_info:
      tags:
        - creative_writing
        - 13b
        - q5_k_m
      provider: ollama
  - model_name: qwen-coder-vllm
    litellm_params:
      model: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
      api_base: http://127.0.0.1:8001/v1
      custom_llm_provider: openai
      stream: true
      api_key: not-needed
    model_info:
      tags:
        - code_generation
        - 7b
        - awq
      provider: vllm-qwen
      context_length: 4096
  - model_name: dolphin-uncensored-vllm
    litellm_params:
      model: solidrust/dolphin-2.8-mistral-7b-v02-AWQ
      api_base: http://127.0.0.1:8002/v1
      custom_llm_provider: openai
      stream: true
      api_key: not-needed
    model_info:
      tags:
        - conversational
        - 7b
        - awq
      provider: vllm-dolphin
      context_length: 4096
  - model_name: deepseek-v3.1:671b-cloud
    litellm_params:
      model: ollama_chat/deepseek-v3.1:671b-cloud
      api_base: https://ollama.com
    model_info:
      tags:
        - advanced_reasoning
        - 671b
      provider: ollama_cloud
  - model_name: qwen3-coder:480b-cloud
    litellm_params:
      model: ollama_chat/qwen3-coder:480b-cloud
      api_base: https://ollama.com
    model_info:
      tags:
        - code_generation
        - 480b
      provider: ollama_cloud
  - model_name: kimi-k2:1t-cloud
    litellm_params:
      model: ollama_chat/kimi-k2:1t-cloud
      api_base: https://ollama.com
    model_info:
      tags:
        - advanced_reasoning
        - 1t
      provider: ollama_cloud
  - model_name: gpt-oss:120b-cloud
    litellm_params:
      model: ollama_chat/gpt-oss:120b-cloud
      api_base: https://ollama.com
    model_info:
      tags:
        - general_chat
        - 120b
      provider: ollama_cloud
  - model_name: gpt-oss:20b-cloud
    litellm_params:
      model: ollama_chat/gpt-oss:20b-cloud
      api_base: https://ollama.com
    model_info:
      tags:
        - general_chat
        - 20b
      provider: ollama_cloud
  - model_name: glm-4.6:cloud
    litellm_params:
      model: ollama_chat/glm-4.6:cloud
      api_base: https://ollama.com
    model_info:
      tags:
        - general_chat
        - 4.6b
      provider: ollama_cloud
litellm_settings:
  request_timeout: 60
  stream_timeout: 120
  num_retries: 3
  timeout: 300
  cache: true
  cache_params:
    type: redis
    host: 127.0.0.1
    port: 6379
    ttl: 3600
  set_verbose: true
  json_logs: true
router_settings:
  routing_strategy: simple-shuffle
  model_group_alias:
    code_generation:
      - qwen2.5-coder:7b
      - qwen3-coder:480b-cloud
    analysis:
      - qwen2.5-coder:7b
      - llama3.1:latest
    reasoning:
      - llama3.1:latest
      - deepseek-v3.1:671b-cloud
      - qwen-coder-vllm
    creative_writing:
      - mythomax-l2-13b-q5_k_m
      - llama3.1:latest
    uncensored:
      - dolphin-uncensored-vllm
    conversational:
      - dolphin-uncensored-vllm
      - llama3.1:latest
      - mythomax-l2-13b-q5_k_m
    general_chat:
      - llama3.1:latest
      - mythomax-l2-13b-q5_k_m
  allowed_fails: 3
  num_retries: 2
  timeout: 30
  cooldown_time: 60
  enable_pre_call_checks: true
  redis_host: 127.0.0.1
  redis_port: 6379
  fallbacks:
    - llama3.1:latest:
        - qwen2.5-coder:7b
        - qwen-coder-vllm
        - dolphin-uncensored-vllm
        - gpt-oss:120b-cloud
    - deepseek-v3.1:671b-cloud:
        - llama3.1:latest
        - qwen-coder-vllm
    - qwen2.5-coder:7b:
        - qwen-coder-vllm
        - qwen3-coder:480b-cloud
        - dolphin-uncensored-vllm
    - qwen3-coder:480b-cloud:
        - qwen2.5-coder:7b
        - qwen-coder-vllm
    - mythomax-l2-13b-q5_k_m:
        - llama3.1:latest
        - qwen-coder-vllm
        - dolphin-uncensored-vllm
        - gpt-oss:120b-cloud
    - qwen-coder-vllm:
        - dolphin-uncensored-vllm
        - qwen3-coder:480b-cloud
    - default:
        - llama3.1:latest
        - qwen2.5-coder:7b
        - qwen-coder-vllm
        - dolphin-uncensored-vllm
        - gpt-oss:120b-cloud
server_settings:
  port: 4000
  host: 0.0.0.0
  cors:
    enabled: true
    allowed_origins:
      - http://localhost:*
      - http://127.0.0.1:*
      - http://[::1]:*
  health_check_endpoint: /health
rate_limit_settings:
  enabled: true
  limits:
    llama3.1:latest:
      rpm: 100
      tpm: 50000
    qwen2.5-coder:7b:
      rpm: 100
      tpm: 50000
    mythomax-l2-13b-q5_k_m:
      rpm: 100
      tpm: 50000
    qwen-coder-vllm:
      rpm: 50
      tpm: 100000
    dolphin-uncensored-vllm:
      rpm: 50
      tpm: 100000
    deepseek-v3.1:671b-cloud:
      rpm: 100
      tpm: 50000
    qwen3-coder:480b-cloud:
      rpm: 100
      tpm: 50000
    kimi-k2:1t-cloud:
      rpm: 100
      tpm: 50000
    gpt-oss:120b-cloud:
      rpm: 100
      tpm: 50000
    gpt-oss:20b-cloud:
      rpm: 100
      tpm: 50000
    glm-4.6:cloud:
      rpm: 100
      tpm: 50000
general_settings:
  background_health_checks: true
  health_check_interval: 300
  health_check_details: false
  '# Master Key Authentication':
  '# Uncomment to enable':
  '# master_key': ${LITELLM_MASTER_KEY}
  '# Salt Key for DB encryption':
  '# salt_key': ${LITELLM_SALT_KEY}
debug: false
debug_router: false
test_mode: false
