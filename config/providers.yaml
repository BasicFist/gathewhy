# AI Backend Provider Registry
# Master catalog of all LLM inference providers in LAB ecosystem

providers:
  # ============================================================================
  # ACTIVE PROVIDERS
  # ============================================================================

  ollama:
    type: ollama
    base_url: http://127.0.0.1:11434
    status: active
    description: Local Ollama server for general-purpose models
    features:
      - Easy model management (ollama pull/list)
      - CPU and GPU support
      - QuantizedGGUF format
      - Fast model switching
    models:
      - name: llama3.1:latest
        size: "8B"
        quantization: Q4_K_M
        pulled_at: "2025-10-19"
        specialty: general_chat
      - name: qwen2.5-coder:7b
        size: "7.6B"
        quantization: Q4_K_M
        pulled_at: "2025-10-19"
        specialty: code_generation
      - name: mythomax-l2-13b-q5_k_m
        size: "13B"
        quantization: Q5_K_M
        pulled_at: "2025-10-24"
        specialty: creative_writing
        options:
          num_gpu_layers: -1
    health_endpoint: /api/tags
    docs: https://ollama.ai/docs
    location: ../openwebui (integrated)

  llama_cpp_python:
    type: llama_cpp
    base_url: http://127.0.0.1:8000
    status: active
    description: llama.cpp Python bindings server with CUDA support
    features:
      - GPU offloading (full or partial)
      - Context window up to 8192 tokens
      - Parallel requests (4 concurrent)
      - OpenAI-compatible API
    models:
      - name: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
        size: "8B"
        quantization: Q4_K_M
        context_length: 131072
        specialty: general_chat
        model_path: /home/miko/LAB/models/gguf/library/Meta-Llama-3.1-8B-Instruct-Q4_K_M/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
        description: "Default Meta Llama 3.1 8B Instruct model served via llama.cpp Python"
      - name: Mistral-Nemo-Instruct-2407.Q4_K_M.gguf
        size: "12B"
        quantization: Q4_K_M
        context_length: 131072
        specialty: reasoning
        model_path: /home/miko/LAB/models/gguf/library/Mistral-Nemo-Instruct-2407-Q4_K_M/Mistral-Nemo-Instruct-2407.Q4_K_M.gguf
        description: "Mistral Nemo Instruct 12B (2407) tuned for analytical reasoning with extended context"
        recommended_gpu_layers: 60
      - name: DeepSeek-Coder-V2-Lite-Instruct.Q4_K_M.gguf
        size: "16B"
        quantization: Q4_K_M
        context_length: 16384
        specialty: code_generation
        model_path: /home/miko/LAB/models/gguf/library/DeepSeek-Coder-V2-Lite-Q4_K_M/DeepSeek-Coder-V2-Lite-Instruct.Q4_K_M.gguf
        description: "DeepSeek Coder v2 Lite for deterministic code completion"
        recommended_gpu_layers: 60
      - name: openhermes-2.5-mistral-7b.Q5_K_M.gguf
        size: "7B"
        quantization: Q5_K_M
        context_length: 8192
        specialty: creative_writing
        model_path: /home/miko/LAB/models/gguf/library/OpenHermes-2.5-Mistral-Q5_K_M/openhermes-2.5-mistral-7b.Q5_K_M.gguf
        description: "OpenHermes 2.5 (Mistral) for rich conversational and creative responses"
      - name: phi-3-medium-4k-instruct.Q4_K_M.gguf
        size: "14B"
        quantization: Q4_K_M
        context_length: 4096
        specialty: knowledge_retrieval
        model_path: /home/miko/LAB/models/gguf/library/Phi-3-medium-4k-instruct-Q4_K_M/phi-3-medium-4k-instruct.Q4_K_M.gguf
        description: "Phi-3 Medium 4K Instruct for factual answering with low latency"
        recommended_gpu_layers: 48
      - name: mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf
        size: "46B"
        quantization: Q4_K_M
        context_length: 32768
        specialty: advanced_reasoning
        model_path: /home/miko/LAB/models/gguf/library/Mixtral-8x7B-Instruct-v0.1-Q4_K_M/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf
        description: "Mixtral 8x7B MoE for heavier reasoning workloads (requires partial GPU offload)"
        recommended_gpu_layers: 40
    configuration:
      n_gpu_layers: -1  # Full GPU offload
      n_ctx: 8192
      n_parallel: 4
      threads: 6
    health_endpoint: /v1/models
    service_file: ~/.config/systemd/user/llamacpp-python.service
    location: ../openwebui/backends/llama.cpp

  llama_cpp_native:
    type: llama_cpp
    base_url: http://127.0.0.1:8080
    status: disabled
    description: llama.cpp native C++ server (maximum performance) - NOT CURRENTLY DEPLOYED
    features:
      - Pure C/C++ implementation
      - No Python overhead
      - Maximum throughput
      - CUDA-optimized
    health_endpoint: /v1/models
    location: ../openwebui/backends/llama.cpp

  vllm-qwen:
    type: vllm
    base_url: http://127.0.0.1:8001
    status: active
    description: vLLM high-throughput inference with AWQ quantization (Qwen model)
    features:
      - Continuous batching
      - PagedAttention for KV cache
      - AWQ quantization (4-bit)
      - Optimized for code generation
      - Low memory footprint
    models:
      - name: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
        size: "7B"
        quantization: AWQ
        context_length: 4096
        specialty: code_generation
        model_size_gb: 5.2
        kv_cache_available_gb: 7.36
    health_endpoint: /v1/models
    api_format: openai_compatible
    gpu_memory_utilization: 0.45
    max_model_len: 4096
    location: ~/venvs/vllm
    hardware_requirements:
      gpu_vram_gb: 16
      gpu_model: "Quadro RTX 5000"
      compute_capability: "7.5"

  vllm-dolphin:
    type: vllm
    base_url: http://127.0.0.1:8002
    status: active
    description: vLLM high-throughput inference with AWQ quantization (Dolphin model)
    features:
      - Continuous batching
      - PagedAttention for KV cache
      - AWQ quantization (4-bit)
      - Optimized for conversational responses
      - Low memory footprint
    models:
      - name: solidrust/dolphin-2.8-mistral-7b-v02-AWQ
        size: "7B"
        quantization: AWQ
        context_length: 4096
        specialty: conversational
    health_endpoint: /v1/models
    api_format: openai_compatible
    gpu_memory_utilization: 0.45
    max_model_len: 4096
    location: ~/venvs/vllm
    hardware_requirements:
      gpu_vram_gb: 16
      gpu_model: "Quadro RTX 5000"
      compute_capability: "7.5"

  ollama_cloud:
    type: ollama
    base_url: https://ollama.com/v1
    status: active
    description: Ollama Cloud API for large models without local GPU
    requires_api_key: true
    env_var: OLLAMA_CLOUD_API_KEY
    features:
      - Cloud-hosted large models (no local GPU required)
      - Data-center-grade hardware
      - Same API as local Ollama
      - Automatic offloading for cloud models
    models:
      - name: deepseek-v3.1:671b-cloud
        size: "671B"
        specialty: advanced_reasoning
      - name: qwen3-coder:480b-cloud
        size: "480B"
        specialty: code_generation
      - name: kimi-k2:1t-cloud
        size: "1T"
        specialty: advanced_reasoning
      - name: gpt-oss:120b-cloud
        size: "120B"
        specialty: general_chat
      - name: gpt-oss:20b-cloud
        size: "20B"
        specialty: general_chat
      - name: glm-4.6:cloud
        size: "4.6B"
        specialty: general_chat
    health_endpoint: /api/tags
    docs: https://docs.ollama.com/cloud

  # ============================================================================
  # TEMPLATE PROVIDERS (Disabled, for reference)
  # ============================================================================

  openai:
    type: openai
    base_url: https://api.openai.com/v1
    status: disabled
    description: OpenAI cloud API (requires API key)
    requires_api_key: true
    env_var: OPENAI_API_KEY
    models:
      - gpt-4
      - gpt-3.5-turbo
    rate_limits:
      requests_per_minute: 3500
      tokens_per_minute: 90000
    cost_per_1k_tokens:
      gpt-4: 0.03
      gpt-3.5-turbo: 0.0015

  anthropic:
    type: anthropic
    base_url: https://api.anthropic.com/v1
    status: disabled
    description: Anthropic Claude API (requires API key)
    requires_api_key: true
    env_var: ANTHROPIC_API_KEY
    models:
      - claude-3-opus-20240229
      - claude-3-sonnet-20240229

  custom_openai_compatible:
    type: openai_compatible
    base_url: http://localhost:CUSTOM_PORT
    status: template
    description: Template for any OpenAI-compatible API server
    notes: |
      Use this template to add:
      - LocalAI servers
      - Text Generation WebUI
      - FastChat servers
      - Any server implementing OpenAI API spec

# ============================================================================
# PROVIDER METADATA
# ============================================================================

metadata:
  version: "1.2"
  last_updated: "2025-11-04"
  total_providers: 5  # ollama, llama_cpp_python, llama_cpp_native, vllm-qwen, vllm-dolphin
  total_models_available: 17  # 3 Ollama + 6 llama.cpp + 2 vLLM + 6 Ollama Cloud

  provider_types:
    - ollama: Simple local server
    - llama_cpp: High-performance C++ inference
    - vllm: Production-grade batched inference
    - openai: Cloud API providers
    - openai_compatible: Generic OpenAI-compatible servers

  selection_criteria:
    ollama:
      best_for: Quick prototyping, easy model management
      performance: Medium
      resource_usage: Low to Medium

    llama_cpp:
      best_for: Maximum single-request performance
      performance: High
      resource_usage: Medium

    vllm:
      best_for: High concurrency, production workloads
      performance: Very High (batched)
      resource_usage: High (requires GPU)

# ============================================================================
# HEALTH CHECK CONFIGURATION
# ============================================================================

health_checks:
  enabled: true
  interval_seconds: 60
  timeout_seconds: 5
  retry_attempts: 3

  endpoints:
    ollama: http://127.0.0.1:11434/api/tags
    llama_cpp_python: http://127.0.0.1:8000/v1/models
    llama_cpp_native: http://127.0.0.1:8080/v1/models
    vllm-qwen: http://127.0.0.1:8001/v1/models
    vllm-dolphin: http://127.0.0.1:8002/v1/models
