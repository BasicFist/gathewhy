# ============================================================================
# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# ============================================================================
#
# Generated by: scripts/generate-litellm-config.py
# Source files: config/providers.yaml, config/model-mappings.yaml
# Generated at: 2025-11-05T16:31:49.704263
# Version: git-8577569
#
# To modify this configuration:
#   1. Edit config/providers.yaml or config/model-mappings.yaml
#   2. Run: python3 scripts/generate-litellm-config.py
#   3. Validate: python3 scripts/validate-config-schema.py
#
# ============================================================================

model_list:
  - model_name: llama3.1:latest
    litellm_params:
      model: ollama/llama3.1:latest
      api_base: http://127.0.0.1:11434
    model_info:
      tags:
        - general_chat
        - 8b
        - q4_k_m
      provider: ollama
  - model_name: qwen2.5-coder:7b
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://127.0.0.1:11434
    model_info:
      tags:
        - code_generation
        - 7.6b
        - q4_k_m
      provider: ollama
  - model_name: mythomax-l2-13b-q5_k_m
    litellm_params:
      model: ollama/mythomax-l2-13b-q5_k_m
      api_base: http://127.0.0.1:11434
      extra_body:
        options:
          num_gpu_layers: -1
    model_info:
      tags:
        - creative_writing
        - 13b
        - q5_k_m
      provider: ollama
  - model_name: llama-cpp-llama3.1
    litellm_params:
      model: openai/local-model
      api_base: http://127.0.0.1:8000
      stream: true
    model_info:
      tags:
        - general_chat
        - 8b
        - q4_k_m
      provider: llama_cpp_python
      context_length: 131072
      notes: Default Meta Llama 3.1 8B Instruct model served via llama.cpp Python
  - model_name: llama-cpp-mistral-nemo
    litellm_params:
      model: openai/local-model
      api_base: http://127.0.0.1:8000
      stream: true
    model_info:
      tags:
        - reasoning
        - 12b
        - q4_k_m
      provider: llama_cpp_python
      context_length: 131072
      notes: Mistral Nemo Instruct 12B (2407) tuned for analytical reasoning with extended context
  - model_name: llama-cpp-deepseek-coder-lite
    litellm_params:
      model: openai/local-model
      api_base: http://127.0.0.1:8000
      stream: true
    model_info:
      tags:
        - code_generation
        - 16b
        - q4_k_m
      provider: llama_cpp_python
      context_length: 16384
      notes: DeepSeek Coder v2 Lite for deterministic code completion
  - model_name: llama-cpp-openhermes-2.5
    litellm_params:
      model: openai/local-model
      api_base: http://127.0.0.1:8000
      stream: true
    model_info:
      tags:
        - creative_writing
        - 7b
        - q5_k_m
      provider: llama_cpp_python
      context_length: 8192
      notes: OpenHermes 2.5 (Mistral) for rich conversational and creative responses
  - model_name: llama-cpp-phi3-medium
    litellm_params:
      model: openai/local-model
      api_base: http://127.0.0.1:8000
      stream: true
    model_info:
      tags:
        - knowledge_retrieval
        - 14b
        - q4_k_m
      provider: llama_cpp_python
      context_length: 4096
      notes: Phi-3 Medium 4K Instruct for factual answering with low latency
  - model_name: llama-cpp-mixtral-8x7b
    litellm_params:
      model: openai/local-model
      api_base: http://127.0.0.1:8000
      stream: true
    model_info:
      tags:
        - advanced_reasoning
        - 46b
        - q4_k_m
      provider: llama_cpp_python
      context_length: 32768
      notes: Mixtral 8x7B MoE for heavier reasoning workloads (requires partial GPU offload)
  - model_name: qwen-coder-vllm
    litellm_params:
      model: Qwen/Qwen2.5-Coder-7B-Instruct-AWQ
      api_base: http://127.0.0.1:8001/v1
      custom_llm_provider: openai
      stream: true
      api_key: not-needed
    model_info:
      tags:
        - code_generation
        - 7b
        - awq
      provider: vllm-qwen
      context_length: 4096
  - model_name: dolphin-uncensored-vllm
    litellm_params:
      model: solidrust/dolphin-2.8-mistral-7b-v02-AWQ
      api_base: http://127.0.0.1:8002/v1
      custom_llm_provider: openai
      stream: true
      api_key: not-needed
    model_info:
      tags:
        - conversational
        - 7b
        - awq
      provider: vllm-dolphin
      context_length: 4096
  - model_name: deepseek-v3.1:671b-cloud
    litellm_params:
      model: ollama/deepseek-v3.1:671b-cloud
      api_base: https://api.ollama.com/v1
    model_info:
      tags:
        - advanced_reasoning
        - 671b
      provider: ollama_cloud
  - model_name: qwen3-coder:480b-cloud
    litellm_params:
      model: ollama/qwen3-coder:480b-cloud
      api_base: https://api.ollama.com/v1
    model_info:
      tags:
        - code_generation
        - 480b
      provider: ollama_cloud
  - model_name: kimi-k2:1t-cloud
    litellm_params:
      model: ollama/kimi-k2:1t-cloud
      api_base: https://api.ollama.com/v1
    model_info:
      tags:
        - advanced_reasoning
        - 1t
      provider: ollama_cloud
  - model_name: gpt-oss:120b-cloud
    litellm_params:
      model: ollama/gpt-oss:120b-cloud
      api_base: https://api.ollama.com/v1
    model_info:
      tags:
        - general_chat
        - 120b
      provider: ollama_cloud
  - model_name: gpt-oss:20b-cloud
    litellm_params:
      model: ollama/gpt-oss:20b-cloud
      api_base: https://api.ollama.com/v1
    model_info:
      tags:
        - general_chat
        - 20b
      provider: ollama_cloud
  - model_name: glm-4.6:cloud
    litellm_params:
      model: ollama/glm-4.6:cloud
      api_base: https://api.ollama.com/v1
    model_info:
      tags:
        - general_chat
        - 4.6b
      provider: ollama_cloud
litellm_settings:
  request_timeout: 60
  stream_timeout: 0
  num_retries: 3
  timeout: 300
  cache: true
  cache_params:
    type: redis
    host: 127.0.0.1
    port: 6379
    ttl: 3600
  set_verbose: false
  json_logs: true
  fallback_to_no_cache_on_redis_error: true
router_settings:
  routing_strategy: usage-based-routing-v2
  model_group_alias:
    code_generation:
      - qwen2.5-coder:7b
      - qwen-coder-vllm
      - llama-cpp-deepseek-coder-lite
      - qwen3-coder:480b-cloud
    analysis:
      - qwen2.5-coder:7b
      - llama3.1:latest
      - llama-cpp-llama3.1
      - llama-cpp-mistral-nemo
      - llama-cpp-phi3-medium
      - llama-cpp-mixtral-8x7b
      - deepseek-v3.1:671b-cloud
    reasoning:
      - llama3.1:latest
      - llama-cpp-llama3.1
      - llama-cpp-mistral-nemo
      - llama-cpp-mixtral-8x7b
      - qwen-coder-vllm
      - deepseek-v3.1:671b-cloud
    creative_writing:
      - mythomax-l2-13b-q5_k_m
      - llama3.1:latest
      - llama-cpp-llama3.1
      - llama-cpp-openhermes-2.5
      - kimi-k2:1t-cloud
    uncensored:
      - dolphin-uncensored-vllm
    conversational:
      - dolphin-uncensored-vllm
      - llama3.1:latest
      - llama-cpp-llama3.1
      - llama-cpp-mistral-nemo
      - llama-cpp-openhermes-2.5
      - mythomax-l2-13b-q5_k_m
      - gpt-oss:20b-cloud
      - gpt-oss:120b-cloud
      - glm-4.6:cloud
    general_chat:
      - llama3.1:latest
      - llama-cpp-llama3.1
      - llama-cpp-mistral-nemo
      - llama-cpp-openhermes-2.5
      - mythomax-l2-13b-q5_k_m
      - llama-cpp-phi3-medium
      - llama-cpp-mixtral-8x7b
      - gpt-oss:20b-cloud
      - gpt-oss:120b-cloud
      - glm-4.6:cloud
    large_context:
      - llama-cpp-llama3.1
      - llama-cpp-mistral-nemo
      - llama-cpp-mixtral-8x7b
  allowed_fails: 3
  num_retries: 2
  timeout: 30
  cooldown_time: 60
  enable_pre_call_checks: true
  redis_host: 127.0.0.1
  redis_port: 6379
  fallbacks:
    - llama3.1:latest:
        - qwen2.5-coder:7b
        - qwen-coder-vllm
        - dolphin-uncensored-vllm
    - qwen2.5-coder:7b:
        - qwen-coder-vllm
        - dolphin-uncensored-vllm
    - mythomax-l2-13b-q5_k_m:
        - llama3.1:latest
        - qwen-coder-vllm
        - dolphin-uncensored-vllm
    - llama-cpp-llama3.1:
        - llama3.1:latest
        - qwen2.5-coder:7b
        - dolphin-uncensored-vllm
    - llama-cpp-mistral-nemo:
        - llama3.1:latest
        - llama-cpp-llama3.1
        - dolphin-uncensored-vllm
    - llama-cpp-deepseek-coder-lite:
        - qwen-coder-vllm
        - qwen2.5-coder:7b
        - dolphin-uncensored-vllm
    - llama-cpp-openhermes-2.5:
        - llama3.1:latest
        - mythomax-l2-13b-q5_k_m
        - llama-cpp-llama3.1
    - llama-cpp-phi3-medium:
        - llama3.1:latest
        - llama-cpp-mistral-nemo
        - llama-cpp-llama3.1
    - llama-cpp-mixtral-8x7b:
        - llama-cpp-mistral-nemo
        - llama-cpp-llama3.1
        - deepseek-v3.1:671b-cloud
    - qwen-coder-vllm:
        - dolphin-uncensored-vllm
    - default:
        - llama3.1:latest
        - qwen2.5-coder:7b
        - qwen-coder-vllm
        - dolphin-uncensored-vllm
server_settings:
  port: 4000
  host: 0.0.0.0
  cors:
    enabled: true
    allowed_origins:
      - http://localhost:*
      - http://127.0.0.1:*
      - http://[::1]:*
  health_check_endpoint: /health
  prometheus:
    enabled: true
    port: 9090
rate_limit_settings:
  enabled: true
  limits:
    llama3.1:latest:
      rpm: 100
      tpm: 50000
    qwen2.5-coder:7b:
      rpm: 100
      tpm: 50000
    mythomax-l2-13b-q5_k_m:
      rpm: 100
      tpm: 50000
    llama-cpp-llama3.1:
      rpm: 120
      tpm: 60000
    llama-cpp-mistral-nemo:
      rpm: 120
      tpm: 60000
    llama-cpp-deepseek-coder-lite:
      rpm: 120
      tpm: 60000
    llama-cpp-openhermes-2.5:
      rpm: 120
      tpm: 60000
    llama-cpp-phi3-medium:
      rpm: 120
      tpm: 60000
    llama-cpp-mixtral-8x7b:
      rpm: 120
      tpm: 60000
    qwen-coder-vllm:
      rpm: 50
      tpm: 100000
    dolphin-uncensored-vllm:
      rpm: 50
      tpm: 100000
    deepseek-v3.1:671b-cloud:
      rpm: 100
      tpm: 50000
    qwen3-coder:480b-cloud:
      rpm: 100
      tpm: 50000
    kimi-k2:1t-cloud:
      rpm: 100
      tpm: 50000
    gpt-oss:120b-cloud:
      rpm: 100
      tpm: 50000
    gpt-oss:20b-cloud:
      rpm: 100
      tpm: 50000
    glm-4.6:cloud:
      rpm: 100
      tpm: 50000
general_settings: {}
debug: false
debug_router: false
test_mode: false
