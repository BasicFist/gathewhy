# Model Routing Configuration
# Defines how LiteLLM routes model requests to providers

# ============================================================================
# EXACT MODEL NAME ROUTING
# ============================================================================

exact_matches:
  # Ollama models (exact name)
  "llama3.1:latest":
    provider: ollama
    priority: primary
    fallback: null
    description: "General-purpose chat model"

  "qwen2.5-coder:7b":
    provider: ollama
    priority: primary
    fallback: null
    description: "Code generation specialist"
  "mythomax-l2-13b-q5_k_m":
    provider: ollama
    priority: primary
    fallback: llama_cpp_python
    description: "Creative writing and roleplay tuned MythoMax L2"
  # vLLM models (AWQ quantized)
  "qwen-coder-vllm":
    provider: vllm-qwen
    priority: primary
    fallback: ollama
    description: "AWQ-quantized 7B code generation model via vLLM"
    backend_model: "Qwen/Qwen2.5-Coder-7B-Instruct-AWQ"

  # Dolphin Uncensored (Mistral-based)
  "dolphin-uncensored-vllm":
    provider: vllm-dolphin
    priority: primary
    fallback: ollama
    description: "7B uncensored Mistral-based model via vLLM (AWQ quantized)"
    backend_model: "solidrust/dolphin-2.8-mistral-7b-v02-AWQ"

  # llama.cpp models
  "llama-cpp-default":
    provider: llama_cpp_python
    priority: primary
    fallback: llama_cpp_native
    description: "GGUF model via llama.cpp Python bindings"

  # Ollama Cloud models (large models not available locally)
  "deepseek-v3.1:671b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: null
    description: "671B advanced reasoning model via Ollama Cloud"

  "qwen3-coder:480b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "480B code generation model via Ollama Cloud"

  "kimi-k2:1t-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: null
    description: "1 trillion parameter model via Ollama Cloud"

  "gpt-oss:120b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "120B open-source GPT via Ollama Cloud"

  "gpt-oss:20b-cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "20B general chat model via Ollama Cloud"

  "glm-4.6:cloud":
    provider: ollama_cloud
    priority: primary
    fallback: ollama
    description: "4.6B general chat model via Ollama Cloud"

# ============================================================================
# PATTERN-BASED ROUTING
# ============================================================================

patterns:
  # HuggingFace model paths → vLLM
  - pattern: "^Qwen/Qwen2\\.5-Coder.*"
    provider: vllm-qwen
    fallback: ollama
    description: "Qwen Coder models via vLLM (AWQ quantized)"

  - pattern: "^solidrust/dolphin.*AWQ$"
    provider: vllm-dolphin
    fallback: ollama
    description: "Dolphin uncensored models via vLLM (AWQ quantized)"

  - pattern: "^meta-llama/.*"
    provider: vllm-qwen
    fallback: ollama
    description: "HuggingFace Llama models prefer vLLM for performance"

  - pattern: "^mistralai/.*"
    provider: vllm-dolphin
    fallback: ollama
    description: "Mistral models via vLLM"

  # Ollama-style names → Ollama
  - pattern: ".*:\\d+[bB]$"  # Matches "model:7b", "model:13B", etc.
    provider: ollama
    description: "Ollama naming convention (model:size)"

  # GGUF files → llama.cpp
  - pattern: ".*\\.gguf$"
    provider: llama_cpp_python
    fallback: llama_cpp_native
    description: "GGUF quantized models"

# ============================================================================
# CAPABILITY-BASED ROUTING
# ============================================================================

capabilities:
  code_generation:
    description: "Models specialized for code"
    preferred_models:
      - qwen2.5-coder:7b
      - qwen3-coder:480b-cloud  # Ollama Cloud
    provider: ollama
    routing_strategy: load_balance

  analysis:
    description: "Models tuned for structured analysis and planning"
    preferred_models:
      - qwen2.5-coder:7b
      - llama3.1:latest
    provider: ollama
    routing_strategy: usage_based

  reasoning:
    description: "Models optimized for multi-step reasoning and problem solving"
    preferred_models:
      - llama3.1:latest
      - deepseek-v3.1:671b-cloud  # Ollama Cloud for advanced reasoning
      - qwen-coder-vllm
    provider: ollama
    routing_strategy: direct

  creative_writing:
    description: "Storytelling and roleplay optimized models"
    preferred_models:
      - mythomax-l2-13b-q5_k_m
      - llama3.1:latest
    provider: ollama
    routing_strategy: usage_based

  uncensored:
    description: "Uncensored models without content filters"
    preferred_models:
      - dolphin-uncensored-vllm
    provider: vllm-dolphin
    routing_strategy: direct

  conversational:
    description: "Natural conversation and general assistance"
    preferred_models:
      - dolphin-uncensored-vllm
      - llama3.1:latest
      - mythomax-l2-13b-q5_k_m
    routing_strategy: usage_based

  high_throughput:
    description: "When you need to handle many concurrent requests"
    min_model_size: "13B"
    provider: vllm-qwen
    routing_strategy: least_loaded

  low_latency:
    description: "Single-request speed priority"
    provider: llama_cpp_native
    fallback: llama_cpp_python
    routing_strategy: fastest_response

  general_chat:
    description: "General conversational AI"
    preferred_models:
      - llama3.1:latest
      - mythomax-l2-13b-q5_k_m
    routing_strategy: usage_based

  large_context:
    description: "Models that can handle large context windows"
    min_context: 8192
    providers:
      - llama_cpp_python  # Configured for 8192 context
      - vllm  # Supports up to 4096 by default
    routing_strategy: most_capacity

# ============================================================================
# LOAD BALANCING CONFIGURATION
# ============================================================================
# Same model available on multiple providers with weighted distribution

load_balancing:
  "llama3.1:latest":
    providers:
      - provider: ollama
        weight: 0.7  # Primary
      - provider: llama_cpp_python
        weight: 0.3  # Backup
    strategy: weighted_round_robin
    description: "Distribute requests with 70% to Ollama, 30% to llama.cpp"

  "general-chat":
    providers:
      - provider: ollama
        weight: 0.6
      - provider: vllm-qwen
        weight: 0.4
    strategy: least_loaded
    description: "Load balance general chat across multiple providers"

# ============================================================================
# FALLBACK CHAINS
# ============================================================================
# Defines fallback order when primary provider fails

fallback_chains:
  "llama3.1:latest":
    # Prefer other capable models if llama3.1 is unavailable
    chain:
      - qwen2.5-coder:7b
      - qwen-coder-vllm
      - dolphin-uncensored-vllm

  "qwen2.5-coder:7b":
    # Ollama code model can fall back to higher throughput or creative options
    chain:
      - qwen-coder-vllm
      - dolphin-uncensored-vllm

  "qwen3-coder:480b-cloud":
    # Ollama Cloud code model fallback
    chain:
      - qwen2.5-coder:7b
      - qwen-coder-vllm

  "deepseek-v3.1:671b-cloud":
    # Cloud model fallback to local alternatives
    chain:
      - llama3.1:latest
      - qwen-coder-vllm

  "gpt-oss:120b-cloud":
    chain:
      - qwen2.5-coder:7b
      - qwen-coder-vllm

  "gpt-oss:20b-cloud":
    chain:
      - qwen2.5-coder:7b
      - qwen-coder-vllm

  "glm-4.6:cloud":
    chain:
      - llama3.1:latest

  "mythomax-l2-13b-q5_k_m":
    chain:
      - llama3.1:latest
      - qwen-coder-vllm
      - dolphin-uncensored-vllm

  "qwen-coder-vllm":
    chain:
      - dolphin-uncensored-vllm

  default:
    # Default fallback chain for all models
    chain:
      - llama3.1:latest
      - qwen2.5-coder:7b
      - qwen-coder-vllm
      - dolphin-uncensored-vllm
      - gpt-oss:120b-cloud  # Ollama Cloud as final fallback
      - gpt-oss:20b-cloud

# ============================================================================
# ROUTING RULES
# ============================================================================

routing_rules:
  # Rule precedence (highest to lowest):
  # 1. Exact model name match
  # 2. Capability-based routing
  # 3. Pattern matching
  # 4. Default provider

  default_provider: ollama
  default_fallback: llama_cpp_python

  request_metadata_routing:
    # Route based on request headers/metadata
    high_priority_requests:
      provider: vllm-qwen
      condition: header.x-priority == "high"

    batch_requests:
      provider: vllm-qwen
      condition: batch_size > 1

    streaming_requests:
      provider: ollama  # Best streaming support
      condition: stream == true

  model_size_routing:
    # Automatic provider selection based on model size
    - size: "< 8B"
      provider: ollama
      reason: "Small models work well with Ollama"

    - size: "8B - 13B"
      provider: ollama
      fallback: llama_cpp_python
      reason: "Medium models, Ollama preferred"

    - size: "> 13B"
      provider: vllm-qwen
      fallback: ollama
      reason: "Large models benefit from vLLM batching"

# ============================================================================
# SPECIAL ROUTING CASES
# ============================================================================

special_cases:
  first_request_routing:
    description: "Cold start optimization"
    strategy: prefer_warm_providers
    warm_check_timeout_ms: 100

  rate_limited_fallback:
    description: "Automatic fallback when provider hits rate limit"
    enabled: true
    fallback_duration_seconds: 60

  error_based_routing:
    description: "Avoid providers with recent errors"
    enabled: true
    error_threshold: 3
    cooldown_seconds: 300

  geographic_routing:
    description: "Route based on provider location (future)"
    enabled: false
    prefer_local: true

# ============================================================================
# METADATA
# ============================================================================

metadata:
  version: "1.6"
  last_updated: "2025-10-29"
  total_routing_rules: 24+
  supported_routing_strategies:
    - exact_match
    - pattern_match
    - capability_based
    - load_balanced
    - fallback_chain
  changes:
    - "Added Ollama Cloud provider with DeepSeek, Qwen3, Kimi K2, GPT-OSS, and GLM models"
    - "Updated capability routing and fallback chains to leverage cloud capacity"
    - "Documented local vs cloud routing strategy for CrushVLLM integration"

notes: |
  This configuration is consumed by LiteLLM's router.

  When adding new models:
  1. Add exact match entry if it has a canonical name
  2. Consider which capability category it belongs to
  3. Define fallback chain for reliability
  4. Test routing with validation script

  When adding new providers:
  1. Update providers.yaml first
  2. Add routing rules here
  3. Update litellm-unified.yaml
  4. Test end-to-end
